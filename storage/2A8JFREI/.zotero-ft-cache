
Skip to main content
Cornell University
We gratefully acknowledge support from the Simons Foundation, member institutions , and all contributors. Donate
arxiv logo > cs > arXiv:2307.11661

Help | Advanced Search
Search
Computer Science > Computer Vision and Pattern Recognition
(cs)
[Submitted on 21 Jul 2023 ( v1 ), last revised 8 Aug 2023 (this version, v2)]
Title: Enhancing CLIP with GPT-4: Harnessing Visual Descriptions as Prompts
Authors: Mayug Maniparambil , Chris Vorster , Derek Molloy , Noel Murphy , Kevin McGuinness , Noel E. O'Connor
Download a PDF of the paper titled Enhancing CLIP with GPT-4: Harnessing Visual Descriptions as Prompts, by Mayug Maniparambil and 5 other authors
Download PDF

    Abstract: Contrastive pretrained large Vision-Language Models (VLMs) like CLIP have revolutionized visual representation learning by providing good performance on downstream datasets. VLMs are 0-shot adapted to a downstream dataset by designing prompts that are relevant to the dataset. Such prompt engineering makes use of domain expertise and a validation dataset. Meanwhile, recent developments in generative pretrained models like GPT-4 mean they can be used as advanced internet search tools. They can also be manipulated to provide visual information in any structure. In this work, we show that GPT-4 can be used to generate text that is visually descriptive and how this can be used to adapt CLIP to downstream tasks. We show considerable improvements in 0-shot transfer accuracy on specialized fine-grained datasets like EuroSAT (~7%), DTD (~7%), SUN397 (~4.6%), and CUB (~3.3%) when compared to CLIP's default prompt. We also design a simple few-shot adapter that learns to choose the best possible sentences to construct generalizable classifiers that outperform the recently proposed CoCoOP by ~2% on average and by over 4% on 4 specialized fine-grained datasets. The code, prompts, and auxiliary text dataset is available at this https URL . 

Comments: 	Paper accepted at ICCV-W 2023. V2 contains additional comparisons with concurrent works
Subjects: 	Computer Vision and Pattern Recognition (cs.CV) ; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG)
Cite as: 	arXiv:2307.11661 [cs.CV]
  	(or arXiv:2307.11661v2 [cs.CV] for this version)
  	https://doi.org/10.48550/arXiv.2307.11661
Focus to learn more
arXiv-issued DOI via DataCite
Submission history
From: Mayug Maniparambil [ view email ]
[v1] Fri, 21 Jul 2023 15:49:59 UTC (3,034 KB)
[v2] Tue, 8 Aug 2023 13:44:12 UTC (4,122 KB)
Full-text links:
Download:

    Download a PDF of the paper titled Enhancing CLIP with GPT-4: Harnessing Visual Descriptions as Prompts, by Mayug Maniparambil and 5 other authors
    PDF
    Other formats 

Current browse context:
cs.CV
< prev   |   next >
new | recent | 2307
Change to browse by:
cs
cs.AI
cs.CL
cs.LG
References & Citations

    NASA ADS
    Google Scholar
    Semantic Scholar

a export BibTeX citation Loading...
Bookmark
BibSonomy logo Reddit logo
Bibliographic Tools
Bibliographic and Citation Tools
Bibliographic Explorer Toggle
Bibliographic Explorer ( What is the Explorer? )
Litmaps Toggle
Litmaps ( What is Litmaps? )
scite.ai Toggle
scite Smart Citations ( What are Smart Citations? )
Code, Data, Media
Demos
Related Papers
About arXivLabs
Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? )

    About
    Help

    contact arXiv Click here to contact arXiv Contact
    subscribe to arXiv mailings Click here to subscribe Subscribe

    Copyright
    Privacy Policy

    Web Accessibility Assistance

    arXiv Operational Status
    Get status notifications via email or slack

