
Skip to main content
Cornell University
We gratefully acknowledge support from the Simons Foundation, member institutions , and all contributors. Donate
arxiv logo > cs > arXiv:2308.12950

Help | Advanced Search
Search
Computer Science > Computation and Language
(cs)
[Submitted on 24 Aug 2023 ( v1 ), last revised 25 Aug 2023 (this version, v2)]
Title: Code Llama: Open Foundation Models for Code
Authors: Baptiste Rozière , Jonas Gehring , Fabian Gloeckle , Sten Sootla , Itai Gat , Xiaoqing Ellen Tan , Yossi Adi , Jingyu Liu , Tal Remez , Jérémy Rapin , Artyom Kozhevnikov , Ivan Evtimov , Joanna Bitton , Manish Bhatt , Cristian Canton Ferrer , Aaron Grattafiori , Wenhan Xiong , Alexandre Défossez , Jade Copet , Faisal Azhar , Hugo Touvron , Louis Martin , Nicolas Usunier , Thomas Scialom , Gabriel Synnaeve
Download a PDF of the paper titled Code Llama: Open Foundation Models for Code, by Baptiste Rozi\`ere and 24 other authors
Download PDF

    Abstract: We release Code Llama, a family of large language models for code based on Llama 2 providing state-of-the-art performance among open models, infilling capabilities, support for large input contexts, and zero-shot instruction following ability for programming tasks. We provide multiple flavors to cover a wide range of applications: foundation models (Code Llama), Python specializations (Code Llama - Python), and instruction-following models (Code Llama - Instruct) with 7B, 13B and 34B parameters each. All models are trained on sequences of 16k tokens and show improvements on inputs with up to 100k tokens. 7B and 13B Code Llama and Code Llama - Instruct variants support infilling based on surrounding content. Code Llama reaches state-of-the-art performance among open models on several code benchmarks, with scores of up to 53% and 55% on HumanEval and MBPP, respectively. Notably, Code Llama - Python 7B outperforms Llama 2 70B on HumanEval and MBPP, and all our models outperform every other publicly available model on MultiPL-E. We release Code Llama under a permissive license that allows for both research and commercial use. 

Subjects: 	Computation and Language (cs.CL)
Cite as: 	arXiv:2308.12950 [cs.CL]
  	(or arXiv:2308.12950v2 [cs.CL] for this version)
  	https://doi.org/10.48550/arXiv.2308.12950
Focus to learn more
arXiv-issued DOI via DataCite
Submission history
From: Baptiste Roziere [ view email ]
[v1] Thu, 24 Aug 2023 17:39:13 UTC (1,371 KB)
[v2] Fri, 25 Aug 2023 08:51:22 UTC (1,371 KB)
Full-text links:
Download:

    Download a PDF of the paper titled Code Llama: Open Foundation Models for Code, by Baptiste Rozi\`ere and 24 other authors
    PDF
    Other formats 

Current browse context:
cs.CL
< prev   |   next >
new | recent | 2308
Change to browse by:
cs
References & Citations

    NASA ADS
    Google Scholar
    Semantic Scholar

a export BibTeX citation Loading...
Bookmark
BibSonomy logo Reddit logo
Bibliographic Tools
Bibliographic and Citation Tools
Bibliographic Explorer Toggle
Bibliographic Explorer ( What is the Explorer? )
Litmaps Toggle
Litmaps ( What is Litmaps? )
scite.ai Toggle
scite Smart Citations ( What are Smart Citations? )
Code, Data, Media
Demos
Related Papers
About arXivLabs
Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? )

    About
    Help

    contact arXiv Click here to contact arXiv Contact
    subscribe to arXiv mailings Click here to subscribe Subscribe

    Copyright
    Privacy Policy

    Web Accessibility Assistance

    arXiv Operational Status
    Get status notifications via email or slack

