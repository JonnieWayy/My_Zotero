error_outline
JavaScript disabled

You have to enable JavaScript in your browser's settings in order to use the eReader.

Or try downloading the content offline
DOWNLOAD
info list link

    navigate_before BACK info Details
    navigate_before BACK list Outline
    navigate_before BACK perm_media Materials
    navigate_before BACK link Links

home
PDF
Page 1 / 8
format_size
fullscreen remove_circle_outline add_circle_outline
search
group_add
more_horiz
get_app
RESEARCHARTICLE COMPUTERSCIENCES OPEN ACCESS Socially situated artificial intelligence enables learning from human interaction RanjayKrishna a,1 ,DonsukLee a ,LiFei-Fei a,2 ,andMichaelS.Bernstein a,2 EditedbyTerrenceSejnowski,SalkInstituteforBiologicalStudies,LaJolla,CA;receivedSeptember13,2021;acceptedJune14,2022 Regardless of how much data artificial intelligence agents have available, agents will inevitably encounter previously unseen situations in real-world deployments. Reacting to novel situations by acquiring new information from other people—socially situated learning—is a core faculty of human development. Unfortunately, socially situated learning remains an open challenge for artificial intelligence agents because they must learn how to interact with people to seek out the information that they lack. In this article, we formalize the task of socially situated artificial intelligence—agents that seek out new information through social interactions with people—as a reinforcement learn- ing problem where the agent learns to identify meaningful and informative questions via rewards observed through social interaction. We manifest our framework as an interactive agent that learns how to ask natural language questions about photos as it broadens its visual intelligence on a large photo-sharing social network. Unlike active- learning methods, which implicitly assume that humans are oracles willing to answer any question, our agent adapts its behavior based on observed norms of which questions people are or are not interested to answer. Through an 8-mo deployment where our agent interacted with 236,000 social media users, our agent improved its performance at recognizing new visual information by 112%. A controlled field experiment confirmed that our agent outperformed an active-learning baseline by 25.6%. This work advances opportunities for continuously improving artificial intelligence (AI) agents that better respect norms in open social environments. human-centeredAI | sociallysituatedlearning | computervision | human–computerinteraction Today’s methods for training artificial intelligence (AI) agents are akin to locking each agent alone in a room with a stack of books (1). Powered by large volumes of manually labeled training data (2, 3) or scraped web content (4, 5) for the agent to consume, machine learning has produced rapid progress in many tasks ranging from healthcare (6) to sustainability (7). But, when a concept is absent from the training data, the agent has no means to acquire it: Restricting an agent’s knowledge source to the books in the room prevents the agent from learning any concepts not present in the room. Worse, these methods ossify agents to ongoing changes to the world or to evolving human needs. So, while the resulting agents often demonstrate strong test set performance, they struggle when faced with novel situations or when deployed in the real world (8–10). We present a formalization that enables AI agents to break out of the metaphorical room by learning through ongoing interactions with people in real-world social environments. We term this approach socially situated artificial intelligence and present evidence through a field experiment that it enables AI agents to learn new concepts that never occurred in their initial training data by simultaneously learning how to interact with people. Our method is inspired by human development, which is a socially mediated process in which children acquire new concepts and cultural norms through inquisitive dialogues with more knowledgeable members of society (11, 12). Enabling socially situated AI is critical to realizing many beneficent applications, especially in which effective human interactions are critical to improve an AI agent’s ability to understand and act accordingly, including human–computer interaction (13), interactive robotics (14), personalized conversational agents (15), and accessible technology (16). To enable socially situated AI, the agent must not only gather data to learn new concepts, but also learn how to interact with people to gather the data. At any given moment, the agent must trade off between these twin goals of interacting to learn and learning to interact. The task is made more challenging because the space of possible interactions for the agent to traverse is vast, the space of useful social interactions is a sparse subset of these possible interactions, and the space of informative interactions constantly shifts as the agent learns. Reinforcement learning, which formalizes possible interactions as an action space and feedback as a reward, requires hundreds of millions of interactions to uncover this subspace of informative and prosocial interactions (17, 18); people will Significance Humanshavelongdemonstrated anabilitytolearnfrom interactionswithothers.However, artificialintelligence(AI)agents learninsocialisolation.Tocreate intelligentsystemsthat understandmorethanafixed sliceoftheworld,ourarticle formalizessociallysituatedAI—a frameworkthatenablesagentsto interactwithpeopleasthey simultaneouslylearnnew conceptsabouttheworldaround them.Usingourframework,we deployafieldexperimentona photo-sharingsocialnetwork whereouragentinteractswith hundredsofthousandsofpeople tolearnconceptsaboutthevisual world.Wecombineadvancesin deeplearning,computervision, naturallanguageprocessing,and human–computerInteractionto deliverahuman-centeredAIthat learnsfrominteractionswith peopleinsocialenvironments. Authoraffiliations: a ComputerScienceDepartment,Stan- fordUniversity,Stanford,CA94305 Author contributions: R.K., L.F.-F., and M.S.B. designed research; R.K. and D.L. performed research; R.K., D.L., L.F.-F., and M.S.B. analyzed data; and R.K., L.F.-F., and M.S.B.wrotethepaper. Theauthorsdeclarenocompetinginterest. ThisarticleisaPNASDirectSubmission. Copyright © 2022 the Author(s). Published by PNAS. This open access article is distributed under Creative CommonsAttributionLicense4.0(CCBY) . 1 To whom correspondence may be addressed. Email: ranjaykrishna@cs.stanford.edu. 2 L.F.-F.andM.S.B.contributedequallytothiswork. This article contains supporting information online at https://www.pnas.org/lookup/suppl/doi:10.1073/pnas. 2115730119/-/DCSupplemental . PublishedSeptember19,2022. PNAS 2022 Vol.119 No.39 e2115730119 https://doi.org/10.1073/pnas.2115730119 1of8
abandon such an agent long before it crosses such a threshold (19, 20). As a result of this limitation, methods that learn from human interaction have so far only seen success with manual human labels (21–25) or with small action spaces such as games and simulations with only a few dozen moves (26–28). To overcome these challenges, we introduce a formalization of socially situated artificial intelligence as an iterative reinforcement learning problem where an agent seeks to improve an underlying model by interacting with people in a social environment where people may or may not respond informatively. Responses are useful only if they contain new information that is useful to the agent. The agent must thus choose social interactions that elicit new concepts useful for the model. Our formulation adopts a knowledge reward to guide the agent to interactions that lead to the discovery of new concepts and an interaction reward to guide the agent to interactions that adhere to social norms in the environment. New concepts are gathered from these interactions, which are used as training data to update the model. As the model improves at these concepts, the agent updates its policy and begins learning how to ask questions about new concepts where people are interested to answer but the model’s performance is still poor. This process of uncovering social norms, improving the underlying model, and updating the interaction policy iterates throughout the agent’s lifetime. We explore the challenges associated with socially situated learning through a large-scale field experiment: We deploy an AI agent on a large photo-sharing social network to learn new visual concepts, a challenging task for which prior models have been criticized as being limited, brittle (29, 30), and prone to problematic behavior (31). Our agent interacts with people on social media by posting natural language questions as comments (Figs. 1 and 2). In a field experiment, we compare our agent to ablations that focus only on the knowledge reward (traditional active learning) or only on the interaction reward. From 236,000 interactions, the one agent using both is capable of learning and dramatically improves its visual intelligence while the control variants stop receiving feedback or quickly stop learning. Socially Situated AI Framework Active learning is the most common framework consulted when iteratively expanding a model’s capabilities. The goal of active learning is to optimize a sequence of labeling requests to acquire new data D ; the new data will be used to improve performance on the model V : X→Y with as few requests as possible. Al- though most active-learning methods design heuristic acquisition algorithms (32), recent work has formalized the process as a reinforcement-learning process (25). These attempts usually re- move real humans from the pipeline and assume the existence of an oracle that will provide labels for any request. Although a pure active-learning approach could gather new data through social interactions in social environments, recent work in human–computer interaction has concluded that users do not want to serve as simple oracles by repeatedly providing labels, breaking a fundamental assumption in active learning (33–35). Our work is a reaction to this observation: Traditional active learning is not ecologically valid in realistic social environments. In our field experiments, we empirically show that a baseline active-learning approach generates interactions that people are not interested in responding to (25). We formalize socially situated artificial intelligence as an itera- tive reinforcement-learning problem that generalizes conventional active learning. The agent is placed in a social environment E = ( S , A , P , P 0 ) . S is the state of environment; e.g., it could include the history of dialogues for a conversational agent or the current location of objects in a three-dimensional world for a robotic agent. A is the space of possible interactions with people that the agent can initiate; e.g., it can be the set of statements that a conversational agent can ask or the set of motions a robotics agent can perform. P : S×A→S is the transition dynamics; e.g., the transition function encodes how people react to the agent’s past actions and how the world changes as a result. Finally, P 0 is the probability measure on the initial state distribution. Similar to active learning, the agent’s goal is to gather data D to optimize V ’s performance with as few interactions as possible. We design this agent’s decision-making process as an infinite-horizon Markov decision process M =( ̄ S , A , ̄ P , ̄ P 0 , R ) . Intuitively, M jointly characterizes the evolution of the environment E , collected data history D ,andthemodel V as the agent makes interaction decisions to optimize the learning objective encoded by the reward R . ̄ S =( S×D×V ) is a metastate that now encodes the state of the environment S , the data history D , and the current capabilities of the underlying model V . So, at a given time step, ̄ s t ∈ ̄ S =( s t , D t , V t ) ,where D t = { s 0 , a 0 , ... , s t }∈D is the dataset of past interactions. D t is a raw form of the data collected so far and can be postprocessed to yield training data for V t . The metatransition dynamics are ̄ P : ̄ S×A→ ̄ S such that s t +1 ∼P ( ·| s t , a t ) , new interactions are added to the dataset D t +1 = D t ⋃ { a t , s t +1 } ,andanew model is trained from the accumulated data V t +1 = train ( D t +1 ) . The metainitial state distribution is ̄ P 0 ( ̄ s =( s , D 0 , V 0 )) = P ( s ) · 1 [ D 0 = {} ] · init ( V ) , where init ( V ) is the prior dis- tribution over V 0 initialized parameters and D 0 is an empty dataset. Parse responses to iden③fy social interac③ons Genera③ng social yet informa③ve ques③ons Visual intelligence through human interac③ons on a social network Meta state with new image from social network Agent : how many animals are in the photo? Curate new informa③on (re)train n Model Data Knowledge reward Interac③on reward Update interac③on reward from past interac③ons (Re-)train vision model with new image ques③on answers Current computer vision Q: how many animals are in the photo? A: 2 Gene r a③ng s informa ③ v e ques③ons Up d ate int e r ac ③ on rewar d from past i n te r ac ③ ons Deploy in a social network D Human: <no response> Human: Those are deer Human: I see 2 H uman : Those are Fig.1. Nomatterhowcomprehensivelywecuratedatasets,AImodeldeploymentswillinevitablyencountersituationstheyhavenotpreviouslyseen,limitin g theirutilityintherealworld.WeintroduceaframeworkforsociallysituatedAI,areinforcement-learningframeworkthatenablesagentstouncove rusefulsocial interactionswithpeoplethatresultinthediscoveryofnewinformation.Withthisformulation,wedesignaprototypetoshowcasethepossibilityof socially situatedlearningforavisualintelligencetask.Ourprototypeagentlearnsvisualconceptsbyaskingquestionsaboutpicturespeopleuploadonsoc ialmedia. Itparseshowpeoplerespondtoouragentintotworewards,whichguidetheagenttowardinteractionsthatarepreferableforpeopleandinformativefo rits underlyingmodel. 2of8 https://doi.org/10.1073/pnas.2115730119 pnas.org
help
