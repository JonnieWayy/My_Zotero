
Skip to main content
Cornell University
We gratefully acknowledge support from the Simons Foundation, member institutions , and all contributors. Donate
arxiv logo > cs > arXiv:2112.04478

Help | Advanced Search
Search
Computer Science > Computer Vision and Pattern Recognition
(cs)
[Submitted on 8 Dec 2021 ( v1 ), last revised 15 Jul 2022 (this version, v2)]
Title: Prompting Visual-Language Models for Efficient Video Understanding
Authors: Chen Ju , Tengda Han , Kunhao Zheng , Ya Zhang , Weidi Xie
Download a PDF of the paper titled Prompting Visual-Language Models for Efficient Video Understanding, by Chen Ju and 4 other authors
Download PDF

    Abstract: Image-based visual-language (I-VL) pre-training has shown great success for learning joint visual-textual representations from large-scale web data, revealing remarkable ability for zero-shot generalisation. This paper presents a simple but strong baseline to efficiently adapt the pre-trained I-VL model, and exploit its powerful ability for resource-hungry video understanding tasks, with minimal training. Specifically, we propose to optimise a few random vectors, termed as continuous prompt vectors, that convert video-related tasks into the same format as the pre-training objectives. In addition, to bridge the gap between static images and videos, temporal information is encoded with lightweight Transformers stacking on top of frame-wise visual features. Experimentally, we conduct extensive ablation studies to analyse the critical components. On 10 public benchmarks of action recognition, action localisation, and text-video retrieval, across closed-set, few-shot, and zero-shot scenarios, we achieve competitive or state-of-the-art performance to existing methods, despite optimising significantly fewer parameters. 

Comments: 	ECCV 2022. Project page: this https URL
Subjects: 	Computer Vision and Pattern Recognition (cs.CV) ; Computation and Language (cs.CL)
Cite as: 	arXiv:2112.04478 [cs.CV]
  	(or arXiv:2112.04478v2 [cs.CV] for this version)
  	https://doi.org/10.48550/arXiv.2112.04478
Focus to learn more
arXiv-issued DOI via DataCite
Submission history
From: Chen Ju [ view email ]
[v1] Wed, 8 Dec 2021 18:58:16 UTC (291 KB)
[v2] Fri, 15 Jul 2022 08:31:45 UTC (321 KB)
Full-text links:
Download:

    Download a PDF of the paper titled Prompting Visual-Language Models for Efficient Video Understanding, by Chen Ju and 4 other authors
    PDF
    Other formats 

( license )
Current browse context:
cs.CV
< prev   |   next >
new | recent | 2112
Change to browse by:
cs
cs.CL
References & Citations

    NASA ADS
    Google Scholar
    Semantic Scholar

DBLP - CS Bibliography
listing | bibtex
Tengda Han
Ya Zhang
Weidi Xie
a export BibTeX citation Loading...
Bookmark
BibSonomy logo Reddit logo
Bibliographic Tools
Bibliographic and Citation Tools
Bibliographic Explorer Toggle
Bibliographic Explorer ( What is the Explorer? )
Litmaps Toggle
Litmaps ( What is Litmaps? )
scite.ai Toggle
scite Smart Citations ( What are Smart Citations? )
Code, Data, Media
Demos
Related Papers
About arXivLabs
Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? )

    About
    Help

    contact arXiv Click here to contact arXiv Contact
    subscribe to arXiv mailings Click here to subscribe Subscribe

    Copyright
    Privacy Policy

    Web Accessibility Assistance

    arXiv Operational Status
    Get status notifications via email or slack

