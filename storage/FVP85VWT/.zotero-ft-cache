Lawyer LLaMA Technical Report
Quzhe Huangâˆ—, Mingxu Taoâˆ—, Zhenwei Anâˆ—, Chen Zhangâˆ—, Cong Jiang, Zhibin Chen, Zirui Wu, Yansong Feng Peking University
{huangquzhe,thomastao,anzhenwei,zhangch}@pku.edu.cn fengyansong@pku.edu.cn

arXiv:2305.15062v1 [cs.CL] 24 May 2023

Abstract
Large Language Models (LLMs), like LLaMA, have exhibited remarkable performances across various tasks. Nevertheless, when deployed to specific domains such as law or medicine, the models still confront the challenge of a deficiency in domain-specific knowledge and an inadequate capability to leverage that knowledge to resolve domain-related problems. In this paper, we focus on the legal domain and explore how to inject domain knowledge during the continual training stage and how to design proper supervised finetune tasks to help the model tackle practical issues. Moreover, to alleviate the hallucination problem during modelâ€™s generation, we add a retrieval module and extract relevant articles before the model answers any queries. Augmenting with the extracted evidence, our model could generate more reliable responses. We release our data and model at https://github.com/AndrewZhe/ lawyer-llama.
1 Introduction
Enhancing open-source large language models with instruction tuning, models such as Vicunna (Chiang et al., 2023) and ChatGLM (Zeng et al., 2023) have achieved remarkable performance in various general tasks. However, due to the lack of data and improper training paradigm, adapting large language models (LLMs) to specific domains like legal or medical domains remains a challenging task. On one hand, current LLMs are primarily trained on general corpora such as Common Crawl and Wikipedia, with limited exposure to domainspecific resources. Therefore, they lack the necessary knowledge required for specific domains. On the other hand, the strategy to analyze and solve domain-specific tasks might diverge significantly from what the model has required within general domains. Even if the model could access all the
âˆ— Equal Contribution. Work in Progress

Question A à£Ó¾à¢µÒ…á•®à§’áŒ±á€©à¨§à¬™á¼»à¸à¥šà©àª"

BELLE
à£Ó¾à¢µà¥Ÿá´­à£ˆâ€«Ò…Ü„â€¬ á•®à§’áŒ±á€©à¨§à¬™á¼»à¸ àªÌ¶

Lawyer LLaMA
×à´Ì½à¿†á€©ÙÌ¾á’«Óâ€«Ü‰â€¬áµ­à¢¥â€«Üˆâ€¬Ó à¹µá¥¢à¨§Ò…à£Ó¾à¢µÒ…áŠ« à¯”á•®à§’áŒ±á€©à¨§à¬™á¼»Ô…Ş®àªÒ…à¦€à¯”á•®à§’áŒ±á€©à¨§à¬™á¼»Ô… Ş®àªÌ¶Ôà©ªà¸á§”Ò…à£Ó¾à¢µÒ…à¦‡àºà°à¸áŠ«à¯”Ò…à® á¶³à¬™ áƒ¿Ş®àªà²áš†â€«İ³â€¬á€©à£ˆá•®à§’Ò…à¦‡àºà°à¸à¦€à¯”Ò…à® á¶³à¬™áƒ¿ Ş®àªà²áš†â€«İ³â€¬á€©à£ˆá•®à§’Ì¶

Question B
İ‡á˜á€©à¹µÒ“Ì½à¿†á€©ÙÌ¾á’«Óâ€«Ü‰â€¬áµ­â€«ÜˆÙƒâ€¬Ô«à¹µÒ…à¦€à·œà£à®½à¨€à¹—á³µÌµâ€«Ú“â€¬à§‡â€«İ¸â€¬Óà¬™Ù– à±²á˜á•£à¾Šà¦à§„â€«Ù…İ¸â€¬Ó»à¹ŒÙ–Ò…áŠ«à·œÓ§à®‘àµ‰â€«ÚŠâ€¬á¶à§’Ò…Ö•à¸Ò…à¦€à·œàµ‰â€«ÚŠâ€¬á¶à§’à±²á˜Õˆ à¿†á€©á´ºá¦ŠÔ…áŸà¹à® á¥â€«İ‘â€¬á‰˜áŠ«à·œá¶à§’á§—à¿¢áŒ±á´»à¥˜Ì¶

à±¯à¹ÓÓ»Ó»à¹ŒáŒ±à©œà¨Ò…à±¯á˜Œâ€«Ù„â€¬à°®áª™à±¯á¶à§’Ò…á§†à¯†Ôâ€«Ûâ€¬

BELLE
á¬¯à°ºâ€«Ş±â€¬á³Ö¦áŒ±Ó¢à¥¢ áŒ±á§—à¿¢â€«İ¢â€¬áš†áµ±á¥á¤© á€©á´ºá˜á¡¤à¸â€«Şˆâ€¬à® á¥ à´³à¶­Ì¶Ö¦â€«İ¢â€¬Õ¦ßá§ƒ ÓÖ–à®Œà«µà¹¶Ô§á¥´Ö¦ áŒ±à¹¦â€«Ş¾Ú¥â€¬Ôâ€«Û“â€¬Õ¦İŠ à¦‡Öœà¹‹à¦…à£ˆà¥’á‰˜á¬¯ á¿à°˜â€«Ì¶Ù­â€¬

Lawyer LLaMA
à°à¦…Ò…à»‘à´Ì½à¿†á€©ÙÌ¾áŒ±á¥¢à¨§Ò…à¦‡àºà°áˆ¿à£à¥’Ô­â€«Ú“â€¬à§‡ â€«İ¸â€¬áŒ±Óà¬™Ù–Ò…à°áŒ±Ó¢à¥¢à¸Ó§áš†â€«Ü”â€¬à·œá¶àµ‰â€«ÚŠâ€¬á¶à§’áŒ±Ì¶Ö• à¸Ò…à¦‡àºà°á›”à«©à°®á¥á¶à§’Ò…à±²á˜á€©á´ºá¦ŠÔ…à¹à® á¥â€«İ‘â€¬á‰˜ à°áŒ±Ó¢à¥¢áŒ±á¶à§’á§—à¿¢Ò…á®Ôà©ªâ€«İ¢â€¬Õ¦á¶à§’Ì¶à¦‡àºà°Ş¾à° áŒ±Ó¢à¥¢á®·à±„à°ºá¶à§’Ò…áµ±á¥á“‹á¦ˆÔ¡á¶á¶à§’â€«Ò…á¦“Üâ€¬à¬šÕ…á›” â€«Ú©â€¬à§’à¦´áŒ­á¦•à¹¢â€«Ù‰â€¬áŠ©á§—á¶à§’áŒ­á¦•Ì¶á¶à§’â€«á¦“Üâ€¬à¬«à­®á«¹à¸â€«İŒâ€¬ à·œá›”à±„á¶à§’áŒ±à°ºà¯á¤’áŞ¾à©’à§¼à¦€à²±ÙÌµá¨°Ô¾Õ¦İŠâ€«Û“×¿â€¬ à¥’á‰˜á’µÔªá¶±â€«à ŸÜâ€¬Óá›˜áŒ±à°ºá¥ Ì¶á§—á³¯à°à¸â€«Şˆâ€¬áµ±á¥á¬°Óà¾ Ô§á¥´â€«Ù‰â€¬Ô­à§¼à¦€à²±ÙÌµá¨°Ô¾â€«Û†Ú“â€¬á’µà·œá¶áŒ±á³¯á·ŒŞ«"

Figure 1: Comparison between LLaMA continually trained only on general Chinese Corpus (BELLE) and our Lawyer LLaMA which is specially designed for the legal domain. It can be seen that BELLE lacks domainspecific knowledge, like legal age of marriage. Whatâ€™s more, even if the necessary knowledge, like the specific law article is given, the original LLaMA still cannot use it to solve a practical problem. See English translations in Figure 4.

knowledge within a domain, applying them to solve practical problems remains a formidable undertaking. In this paper, we focus on the legal domain to explore how to adapt LLMs to specific domains. We will discuss the potential challenges and corresponding solutions to them.
Figure 1 shows the responses of LLaMA pretrained on the general corpus (BELLE) and our Lawyer LLaMA which is designed to solve legal issues. For Question A, the right answer is what Lawyer LLaMA provides, that the legal age of marriage is 22 for men and 20 for women. Obviously, BELLE lacks such domain-specific knowledge and

1

can only answer 18, which is the age with full civil capacity. What general LLMs lack is not only the knowledge but also the method to use them to solve domain-specific problems. For example, in Question B, even if the necessary law article is provided, BELLE cannot figure out the most important/relevant part to the userâ€™s query that the wife gave birth less than a year ago and thus the husband is not allowed to file for divorce at this time.
The poor performance of BELLE indicates the LLMs pre-trained in the general domain cannot be applied directly to solve legal problems. For large models to be applied in the field of law, they need to possess the following three capabilities: 1. Convey accurate/precise meaning without ambiguity. In the legal domain, just replacing a single word could influence the establishment of legal relationships and obligations, potentially leading to dramatically different consequences. For example, the words deposit (å®šé‡‘) and down payment (è®¢é‡‘) in Chinese are only different by one character, but their meanings and legal effects are completely different in contract law. 2. Understand and distinguish legal terminology. There are many concepts that only appear in the legal domain, like juridical person (æ³•äºº). Even for widely used words, their meanings could be slightly different in the legal domain. For example, the word wife does not necessarily imply a legally recognized and protected marital relationship in daily life, but in the specific field of law, when referring to wife, it definitely implies a lawful marital relationship. 3. Be able to analyze practical cases. Real-world scenarios are always complex and varied. Models need to have the ability to identify legal terminology and apply legal theories to analyze specific issues.
To build a model that can apply legal knowledge correctly and solve practical problems in the legal domain, we adopt the following four steps, based on the LLaMA model (Touvron et al., 2023):
1. Injecting Legal Knowledge: We collect a large amount of raw text in the legal domain, such as law articles, judicial interpretations, and judicial documents of the Peopleâ€™s Court of China. We then apply continue-training to help the model learn legal knowledge.
2. Training to Learn Domain-specific Skills: The ability to use domain knowledge could be learned when solving practical tasks, such as concept interpretation, case analysis and legal

consultation. Therefore, we collect a set of real queries of those tasks and generate corresponding answers using ChatGPT1. Then we do supervised finetuning using these data to teach the model how to solve domain-specific tasks with proper knowledge.
3. Augmenting with External Knowledge: To alleviate the hallucination problem in large language models and to enable the model to generate more accurate and faithful replies, we additionally introduce an information retrieval module. Before generating each response, we first utilize the userâ€™s query and context information to retrieve relevant law articles that can serve as evidence to the query, and then base our responses on these law articles.
2 Data Collection
2.1 Pre-training Corpus
To improve model performance on Chinese legal consultation, we employ two kinds of corpora to continually pre-train the LLaMA model (Touvron et al., 2023). We first train the model on a general multilingual corpus to enhance its ability to represent Chinese. We then use a Chinese legal corpus to augment the model with legal knowledge.
Multilingual General Corpus Since LLaMA has been pre-trained mainly on a corpus of English and other languages using either the Latin or Cyrillic scripts, it may have an imperfect performance to comprehend and to generate Chinese sentences. For example, we find that the original LLaMA always generates English responses to Chinese queries, and the responses might also be irrelevant. We, therefore, continually pre-train LLaMA on the Chinese general corpus. Following previous works (dâ€™Autume et al., 2019; Tao et al., 2023), we also collect English corpus for memory replay to avoid catastrophic forgetting.
To construct the Chinese general corpus, we sample articles from WuDaoCorpora (Yuan et al., 2021), CLUECorpus2020 (Xu et al., 2020b), and the Simplified Chinese edition of Wikipedia. For the English general corpus, we sample articles from C4 Corpus (Raffel et al., 2020).
Chinese Legal Corpus Previous work (Lee et al., 2019) reveals that language models can leverage
1https://chat.openai.com/

2

knowledge learned from a domain-specific corpus. And such models can handle tasks in the corresponding domain better than the models pre-trained with general corpus only. To augment our model with Chinese legal knowledge, we collect articles from the websites of China Courts, including judgment documents, law articles, judicial interpretations, court news, and various articles for law popularization.
2.2 National Judicial Examination
National Judicial Examination is a closed-book exam that applicants must pass to be certified as a lawyer or a judge. It is a comprehensive evaluation of legal knowledge and professional skills for legal practitioners, thus a model could learn legal skills through practicing on the questions from such exams. We use JEC-QA (Zhong et al., 2020a) to construct our data. JEC-QA includes 26,365 multiple-choice and multiple-answer questions but it does not contain any explanations for the answers. In order to provide more detailed evidence and reasoning procedures on how to solve such problems, we ask the ChatGPT to give an explanation for every answer. Below shows an example from our collected data. See the English translation in Table 4.
Question: å…³äºäº¤é€šè‚‡äº‹ç½ªä¸å…¶ä»–çŠ¯ç½ªå…³ç³»çš„è®º
è¿°ï¼Œä¸‹åˆ—é€‰é¡¹æ˜¯æ­£ç¡®çš„å—ï¼Ÿä¸é…’åé©¾é©¶äº¤é€šè‚‡äº‹ åï¼Œç»§ç»­é©¾è½¦åˆ°å¤„å†²æ’ï¼Œé€ æˆäº”æ­»ä¹ä¼¤ï¼Œåº”å®šäº¤ é€šè‚‡äº‹ç½ªã€‚
Response: æ ¹æ®ã€Šä¸­åäººæ°‘å…±å’Œå›½åˆ‘æ³•ã€‹è§„å®šï¼Œé†‰
é…’åé©¾é©¶æœºåŠ¨è½¦è¾†ï¼Œå‘ç”Ÿé‡å¤§äº¤é€šäº‹æ•…ï¼Œæ„æˆäº¤ é€šè‚‡äº‹ç½ªã€‚ä¸é…’åé©¾é©¶å‘ç”Ÿäº¤é€šäº‹æ•…ï¼Œå·²ç»æ„æˆ äº¤é€šè‚‡äº‹ç½ªã€‚ä½†æ˜¯ï¼Œä¸ç»§ç»­é©¾è½¦åˆ°å¤„å†²æ’ï¼Œé€ æˆ äº”æ­»ä¹ä¼¤ï¼Œå·²ç»è¶…å‡ºäº†äº¤é€šè‚‡äº‹ç½ªçš„èŒƒç•´ï¼Œåº”å½“ è®¤å®šä¸ºå±å®³å…¬å…±å®‰å…¨ç½ªã€‚å› æ­¤ï¼Œä¸åº”å½“è¢«å®šç½ªä¸º å±å®³å…¬å…±å®‰å…¨ç½ªï¼Œè€Œä¸æ˜¯äº¤é€šè‚‡äº‹ç½ªã€‚
2.3 Legal Consultation
We sample seed questions from an open-source dataset of legal consultation (Chen, 2018) and use ChatGPT to generate responses. We collect both one-turn responses and multi-turn dialogues with well-crafted prompts. To improve the faithfulness of generated responses, we adopt a law article retrieval component to search related law articles for a given question. The retrieved law articles are appended to the input prompt, helping ChatGPT to generate more faithful responses.

One-turn Responses To generate the response to a legal consultation question, we ask ChatGPT to act as a lawyer and respond to a client. In the input prompt, we list six requirements that the generated responses should meet: (1) properly citing law articles, (2) giving well-founded analyses based on the facts of the case and law articles, (3) responding comprehensively and analyzing the potential possibilities, (4) asking appropriate questions to dig out facts to assist in further answers, (5) using plain language, (6) giving preliminary legal opinions and consulting conclusions.
In the following example (see the English translation in Table 5), ChatGPT generates a response to a question regarding the legal age of marriage. It cites a law article in Marriage Law of the Peopleâ€™s Republic of China and indicates several special circumstances.
Question: æˆ‘å’Œæˆ‘å¯¹è±¡æƒ³ç»“å©šï¼Œæˆ‘æƒ³é—®ä¸€ä¸‹ï¼Œç»“å©š
çš„æ³•å®šå©šé¾„æ˜¯å‡ å²å•Š
Response: æ ¹æ®ã€Šä¸­åäººæ°‘å…±å’Œå›½å©šå§»æ³•ã€‹è§„å®šï¼Œ
ç”· å¥³ ç»“ å©š çš„ æ³• å®š å©š é¾„ ä¸º ç”· æ»¡22å‘¨ å² ï¼Œ å¥³ æ»¡20å‘¨ å²ã€‚æœªè¾¾åˆ°æ³•å®šå©šé¾„çš„äººä¸å¾—ç»“å©šã€‚ä½†æœ‰ç‰¹æ®Šæƒ… å†µçš„å¯ä»¥æå‰ç»“å©šï¼Œéœ€è¦å¾—åˆ°çˆ¶æ¯æˆ–è€…ç›‘æŠ¤äººçš„ ä¹¦é¢åŒæ„ï¼Œå¹¶ç»å½“åœ°å©šå§»ç™»è®°æœºå…³å®¡æŸ¥æ‰¹å‡†ã€‚
Multi-turn Dialogues To generate multi-turn dialogues for a legal consultation question, we design two different prompts, asking ChatGPT to act as a client and as a lawyer, respectively. We use these two prompts alternately, together with the dialogue history, as the input to ChatGPT.
In the following example (English translation in Table 6), the client gives a follow-up question on the marriage procedure after the first response from the lawyer, actively participating in the conversation.
Client: æˆ‘å’Œæˆ‘å¯¹è±¡æƒ³ç»“å©šï¼Œæˆ‘æƒ³é—®ä¸€ä¸‹ï¼Œç»“å©šçš„
æ³•å®šå©šé¾„æ˜¯å‡ å²å•Š
Lawyer: æ ¹æ®ä¸­åäººæ°‘å…±å’Œå›½å©šå§»æ³•ï¼Œç”·å¥³éƒ½è¦
æ»¡22å‘¨å²æ‰å¯ä»¥ç»“å©šï¼Œä¸è¶³22å‘¨å²çš„ç”·å¥³ä¸å¾—è‡ª è¡Œç»“å©šã€‚ä½†æ˜¯ï¼Œå¦‚æœç‰¹æ®Šæƒ…å†µéœ€è¦æå‰ç»“å©šï¼Œ å¯ä»¥å‘å©šå§»ç™»è®°æœºå…³æå‡ºç”³è¯·å®¡æ ¸ã€‚å¦‚æœæ‚¨æœ‰å…¶ ä»–æ–¹é¢çš„ç–‘é—®ï¼Œä¹Ÿå¯ä»¥å‘Šè¯‰æˆ‘ï¼Œæˆ‘å¯ä»¥å¸®åŠ©æ‚¨è¿› è¡Œè§£ç­”ã€‚å¦‚æœæ‚¨æœ‰å…¶ä»–æ–¹é¢çš„ç–‘é—®ï¼Œä¹Ÿå¯ä»¥å‘Šè¯‰ æˆ‘ï¼Œæˆ‘å¯ä»¥å¸®åŠ©æ‚¨è¿›è¡Œè§£ç­”ã€‚
Client: æˆ‘çŸ¥é“ï¼Œå¬åˆ«äººè¯´å©šå§»éœ€è¦æå‰åŠç†ä¸€äº›
æ‰‹ç»­ï¼Œé‚£æˆ‘éœ€è¦å‡†å¤‡å“ªäº›è¯ä»¶å’Œèµ„æ–™å‘¢ï¼Ÿæ˜¯å¦éœ€ è¦æå‰åŠç†å¥åº·æ£€æŸ¥å’Œä½“æ£€ï¼Ÿ
Lawyer: æ˜¯çš„ï¼Œå©šå§»ç™»è®°éœ€è¦æ‚¨å‡†å¤‡ä¸€äº›è¯ä»¶å’Œèµ„

3

æ–™ï¼ŒåŒ…æ‹¬ï¼š...
Retrieval Augmented Generation We observe that ChatGPT tends to provide law articles that are outdated, or do not exist at all when generating responses, a phenomenon known as hallucination (Ji et al., 2023b). We attempt to alleviate this problem by providing related law articles to ChatGPT. For a one-turn response, we append the top 3 related law articles selected by a law article retrieval model to the input prompt. For a multi-turn dialogue, we assume that the topic of the conversation will not change and use the same 3 law articles across the dialogue. We will explore strategies to dynamically retrieve related law articles during each turn of the dialogue.
In the following example (English translation in Table 7), we retrieve 3 law articles to facilitate a more faithful generation. Note that two of them are irrelevant to the consultation and ChatGPT is able to ignore them in its response.
Related Articles: ã€Šæ°‘æ³•å…¸ã€‹ç¬¬ä¸€åƒé›¶å››åä¸ƒæ¡ï¼š
ç»“å©šå¹´é¾„ï¼Œç”·ä¸å¾—æ—©äºäºŒåäºŒå‘¨å²ï¼Œå¥³ä¸å¾—æ—©äº äºŒåå‘¨å²ã€‚ ã€Šæ°‘æ³•å…¸ã€‹ç¬¬ä¸€åƒé›¶å››åå…­æ¡ï¼šç»“å©šåº”å½“ç”·å¥³åŒ æ–¹å®Œå…¨è‡ªæ„¿ï¼Œç¦æ­¢ä»»ä½•ä¸€æ–¹å¯¹å¦ä¸€æ–¹åŠ ä»¥å¼ºè¿«ï¼Œ ç¦æ­¢ä»»ä½•ç»„ç»‡æˆ–è€…ä¸ªäººåŠ ä»¥å¹²æ¶‰ã€‚ æ°‘æ³•å…¸ã€‹ç¬¬ä¸‰ç™¾é›¶å››æ¡ï¼šå…±æœ‰äººå¯ä»¥åå•†ç¡®å®šåˆ† å‰²æ–¹å¼ã€‚è¾¾ä¸æˆåè®®...
Client: æˆ‘å’Œæˆ‘å¯¹è±¡æƒ³ç»“å©šï¼Œæˆ‘æƒ³é—®ä¸€ä¸‹ï¼Œç»“å©šçš„
æ³•å®šå©šé¾„æ˜¯å‡ å²å•Š
Laywer: æ ¹æ®ã€Šæ°‘æ³•å…¸ã€‹ç¬¬ä¸€åƒé›¶å››åä¸ƒæ¡è§„å®šï¼Œ
ç»“å©šçš„æ³•å®šå©šé¾„ä¸ºç”·ä¸å¾—æ—©äº22å‘¨å²ï¼Œå¥³ä¸å¾—æ—© äº20å‘¨å²ã€‚å¦‚æœæœªåˆ°æ³•å®šç»“å©šå¹´é¾„è€ŒåšæŒç»“å©šï¼Œ å°†è¢«è®¤å®šä¸ºæ— æ•ˆå©šå§»ã€‚ è¯·é—®æ‚¨å’Œæ‚¨çš„å¯¹è±¡çš„å¹´é¾„æ˜¯å¦å·²è¾¾åˆ°æ³•å®šå©šé¾„ï¼Ÿ å¦‚æœæœªè¾¾åˆ°ï¼Œå»ºè®®æ‚¨ç­‰å¾…è‡³æ³•å®šå©šé¾„å†è€ƒè™‘ç»“å©š çš„é—®é¢˜ã€‚
Client: æˆ‘ä»¬éƒ½æ»¡è¶³æ³•å®šç»“å©šå¹´é¾„äº†ï¼Œä½†æ˜¯æˆ‘ä»¬çš„
å®¶é•¿åå¯¹ï¼Œä»–ä»¬è¯´æˆ‘ä»¬è¿˜å¤ªå¹´è½»ï¼Œä¸åº”è¯¥æ€¥äºç»“ å©šï¼Œä½ æ€ä¹ˆçœ‹ï¼Ÿ
Lawyer: æ ¹æ®ã€Šæ°‘æ³•å…¸ã€‹ç¬¬ä¸€åƒé›¶å››åå…­æ¡è§„
å®šï¼Œç»“å©šåº”å½“ç”·å¥³åŒæ–¹å®Œå…¨è‡ªæ„¿ï¼Œç¦æ­¢ä»»ä½•ä¸€æ–¹ å¯¹å¦ä¸€æ–¹åŠ ä»¥å¼ºè¿«ï¼Œç¦æ­¢ä»»ä½•ç»„ç»‡æˆ–è€…ä¸ªäººåŠ ä»¥ å¹²æ¶‰ã€‚å› æ­¤ï¼Œå¦‚æœæ‚¨å’Œæ‚¨çš„å¯¹è±¡å®Œå…¨è‡ªæ„¿å¹¶ä¸”å·² è¾¾åˆ°æ³•å®šç»“å©šå¹´é¾„ï¼Œæ‚¨ä»¬å¯ä»¥åˆæ³•ç»“å©šï¼Œå³ä½¿å®¶ é•¿åå¯¹ã€‚å½“ç„¶ï¼Œä½œä¸ºå°Šé‡å®¶é•¿ã€ä¿æŒè‰¯å¥½äº²æƒ…å…³ ç³»çš„ä¸€ç§æ–¹å¼ï¼Œå¯ä»¥å°è¯•ä¸å®¶é•¿æ²Ÿé€šï¼Œäº†è§£å…¶å å¯¹çš„åŸå› ï¼Œå¹¶è€ƒè™‘å¦‚ä½•è§£å†³å…¶å…³æ³¨çš„é—®é¢˜ã€‚å¦‚æœ å®¶é•¿åå¯¹å¹¶ä¸åˆç†ï¼Œæ‚¨å’Œæ‚¨çš„å¯¹è±¡æœ‰æƒåˆ©è‡ªä¸»å†³ å®šã€‚

In total, we collect about 16,000 one-turn responses and 5,000 dialogues with 2 or 3 turns.
3 Training Process
Figure 3 illustrates how we fine-tune the opensource model LLaMA step by step to make it suitable for the Chinese legal domain.
Due to the scarce Chinese text that LLaMA used during its training process, our initial task is to enhance LLaMAâ€™s ability to comprehend Chinese (s1 in Figure 3). Then, we supplement the model with the legal knowledge in Chinese that it lacks (s4 in Figure 3), and teach the model to use the knowledge to solve practical problems through downstream tasks like case analysis (s7 in Figrue 3). Finally, to further improve the reliability of the modelâ€™s responses, we introduce a law article retrieval module, enabling the model to generate responses with evidence (s12 in Figure 3).
3.1 Enhancing LLaMAâ€™s Ability in Chinese
To improve the comprehension and generation ability in Chinese, we continually pre-train LLaMA on the Chinese General Corpus introduced in Section 2.1. The first problem we meet is that many Chinese characters are not included in the vocabulary of LLaMA. Actually, only 700 Chinese characters are included and LLaMA uses bytes to decompose other Chinese characters in UTF-8 encoding. To improve the decoding efficiency of Chinese sentences, Cui et al. (2023) expand the vocabulary by adding common Chinese characters and re-training these newly added word embeddings along with the model parameters. However, our prior study shows that expanding the vocabulary does not seem to bring further improvement on downstream Chinese NLU tasks. We therefore choose to keep LLaMAâ€™s vocabulary unchanged during the training. Another question is whether we should continue pre-training on Chinese corpus solely, or we should also add texts in English. We decide to use a mixture corpus of both English and Chinese, because we believe many complex reasoning abilities may come from the pre-training on English and we expect the model to keep them during our continual pre-training. Please refer to Section 2.1 for detailed information about the training data.

4

Consultation from User
à±¯Ş¾à±¯à©’á¨à°®á•®à§’Ò…à±¯à°®á³¯Ó Ó¥Ò…á•®à§’áŒ±á€©à¨§à§’á¼»à¸â€«Ù¾â€¬àªà ¡

Law Retriever

Retrieved Legal Articles
Ì½à¿†á€©ÙÌ¾á’«Óâ€«Ü‰â€¬áµ­à¢¥â€«Üˆâ€¬Ó à¹µÒ“á•®à§’à¬™á¼»Ò…áŠ«Ó§ à®‘à·±Ô­Ô«â€«Üˆâ€¬Ô«Ş®àªÒ…à¦€Ó§à®‘à·±Ô­Ô«â€«Ş®Üˆâ€¬àªÌ¶ Ì½à¿†á€©ÙÌ¾á’«Óâ€«Ü‰â€¬áµ­à¢¥â€«Ù…Üˆâ€¬à¹µÒ“Å Ì½à¿†á€©ÙÌ¾á’«Ó£áŒ¯áµ­à¢¥à¹µÅ

Model Input
á§—İ‡á˜á•³â€«ÚŠâ€¬áŒ±İ‡á˜á€©à¹µÒ…à¢§à¥”áŠ à²áŒ±ßá§ƒá³¯á·ŒÌ¶İ‡á˜á€©à¹µÓ¾â€«İ¢â€¬áš†à¨‚à£Ó¨ßá§ƒà·«â€«Ù‰â€¬áŒ±á€©à¹µÒ…á§—à¢§à¥”à·¸Ó§ á¥à­šáŠ á¬¯Ô¶à·«â€«Ù‰â€¬áŒ±á€©à¹µÌ¶ +XPDQà±¯Ş¾à±¯à©’á¨à°®á•®à§’Ò…à±¯à°®á³¯ÓÓ¥Ò…á•®à§’áŒ±á€©à¨§à§’á¼»à¸â€«Ù¾â€¬àªà ¡ İ‡á˜á€©à¹µ Ì½à¿†á€©ÙÌ¾á’«Óâ€«Ü‰â€¬áµ­à¢¥â€«Üˆâ€¬Ó à¹µÒ“á•®à§’à¬™á¼»Ò…áŠ«Ó§à®‘à·±Ô­Ô«â€«Üˆâ€¬Ô«Ş®àªÒ…à¦€Ó§à®‘à·±Ô­Ô«â€«Ş®Üˆâ€¬àªÌ¶ Ì½à¿†á€©ÙÌ¾á’«Óâ€«Ü‰â€¬áµ­à¢¥â€«Ù…Üˆâ€¬à¹µÒ“Å Ì½à¿†á€©ÙÌ¾á’«Ó£áŒ¯áµ­à¢¥à¹µÅ $VVLVWDQW

Lawyer LLaMA
Model Output à»‘à´Ì½à¿†á€©ÙÌ¾á’«Óâ€«Ü‰â€¬áµ­à¢¥â€«Üˆâ€¬Ó à¹µá¥¢à¨§Ò…á•®à§’áŒ±á€©à¨§à§’á¼»Ô…áŠ«Ó§à®‘à·±Ô­Ş®àªÒ…à¦€Ó§à®‘à·±Ô­Ş®àªÌ¶à¦‡ àºà¹šâ€«Ú©â€¬á€©à¨§á•®à§’à¬™á¼»á˜’à£™à³®á•®à§’Ò…à©™á¤©á¦Šà¨§Ô…à·«à¶´à§’à¦´Ì¶ á§—á³¯à°Ş¾à°áŒ±à©’á¨áŒ±à¬™á¼»à¸â€«Şˆâ€¬à«ªá¬¡â€«Ú©â€¬á€©à¨§à§’á¼»Ò˜à¦‡àºà¹šá¬¡â€«Ò…Ú©â€¬à­Œá¦“à°á’µà®‡á›—á€©à¨§à§’á¼»Ùšá˜á¡¤á•®à§’áŒ±á³¯á·ŒÌ¶

Figure 2: An illustration of the inference process of Lawyer LLaMA. See the English translations in Figure 5.

Pre-trained on Multilingual General Corpus

Trained w/ general SFT examples, Trained w/ Judicial Exami-

Judicial Examination, legal consul- nation, legal consultations,

tations, and dialogues together

and dialogues together

Pre-trained on Chin-

s6

s8

ese Legal Corpus

Trained w/ multi-

turn dialogues Trained 1 epoch w/ general

s10

SFT examples, Judicial Examination, legal consultations, and dia-

logues augmented with articles

s0

s1

s4

s7

s9

s12

Original LLaMA

s2

s3

Trained w/ general

s5

SFT examples

Trained w/ general SFT examples

SFT w/ Judicial Examination and legal consultations

Trained w/ general SFT examples, sparsely replay with Multilingual Genral Corpus

Trained 3 epochs w/

Judicial Examination and legal consultations

s11

Trained 1 epoch w/ general SFT examples, Judicial Examination, legal consultations, and dialogues together

Figure 3: The training process of Lawyer LLaMA, where each node si represents the i-th training stage.

3.2 Injecting Legal Knowledge
To make up for the deficiency of legal knowledge in the original LLaMA model, we collect a variety of plain Chinese texts in the legal domain, including law articles and judicial interpretations, for further continual training. We also add general texts to prevent our model from overfitting the legal corpus.
3.3 Learning Reasoning Skills
Solving practical problems requires the model to have reasoning skills in the legal domain. To this end, we select supervised data from downstream tasks and train our model following the instructiontuning method. We collect question-answer pairs about distinguishing terminologies and case analysis from JEC-QA, and ask the ChatGPT to provide detailed explanations for them. During training, the QA pairs are treated as instructions and our model is required to give explanations. Furthermore, we

collect a set of queries that real-world users consulted with lawyers from Chen (2018), and obtain corresponding responses through ChatGPT. Then we use the collected supervised data to train the model in providing proper responses to specific queries.
3.4 Retrieving Relevant Law Articles
As China is a country adopting the civil law, law articles are necessary whenever a judge makes a judgment or a lawyer gives advice. However, our preliminary experiment shows that even if the model repeatedly learns the articles during its continual training phase, it cannot use them correctly when generating a response. It might refer to an irrelevant law article or it may replace a terminology with a semantically similar word that actually has a drastically different meaning in the legal domain. To make the model produce more reliable responses,

5

we incorporate it with a law article retrieval module.
To train the retrieval module, we first collect some user questions for legal consultation and ask legal professionals to annotate up to 3 necessary law articles to answer each question. We then train a law article retrieval model based on RoBERTa (Liu et al., 2019). It could achieve 0.85 Recall@1 and 0.94 Recall@5 on the held-out testing set. In fact, law article classification is not a new task. Previous works (Zhong et al., 2020b, 2018) predict the necessary law articles for judgment based on the fact descriptions in judicial documents. We do not directly adopt such classifiers because there is a huge gap between the fact descriptions in judicial documents and user queries in daily life. Fact descriptions are formal and wellwritten, encompassing the complete cause, process, and outcome of an incident. However, user queries are colloquially posed and contain only limited, incomplete information, such as "I want a divorce, what should I do?". Thus, the classifiers trained on fact descriptions cannot perform well in practical scenarios.
When we directly concatenate the retrieved law articles and the userâ€™s question as new input, we find that the model tends to quote all the provided law articles in its response, without distinguishing whether they were truly relevant to the current scenario. Given that our law article retrieval module cannot guarantee all retrieved articles are highly relevant to the userâ€™s question, this behavior of the model will introduce more noise. To address this problem, we add irrelevant articles to the context during training, forcing the model to ignore that distracted information. Besides, we modify the prompt during inference, and explicitly inform the model that there might be irrelevant information among the provided law articles, for example, " There may be irrelevant articles in the reference articles, so please avoid quoting those unrelated articles when replying.."
4 Experiments
Lawyer LLaMA is a generative dialogue model, like ChatGPT and ChatGLM. Such a large language model cannot only respond to dialogue queries from humans, but can also handle many NLP tasks like text classification, and multiplechoice question answering. To evaluate the zeroshot performance of our Lawyer LLaMA at dif-

ferent training stages, we transfer every employed task to a multiple-choice task. The model can then predict by choosing a text with the lowest perplexity, without any fine-tuning on specific tasks. Our experiments are based on LLaMA-7B.
4.1 Evaluation on General Tasks
We first wonder how the ability to solve general NLP tasks of LLaMA changes, when we continuously train it on different corpora or with different supervised-fine-tuning datasets. For example, we can examine whether or not the model performance of English reasoning tasks will deteriorate after continually pre-trained on Chinese corpora.
Following previous work (Biderman et al., 2023), we collect both English and Chinese general tasks of various domains, including natural language inference, sentiment analysis, commonsense reasoning, dialogue question answering, etc.
The results are shown in the left part of Table 1. Comparing the results of s0 and s1, we can find LLaMA obtain an improvement of +5.3% accuracy on C3 (Sun et al., 2020), a Chinese question answering dataset, after continually pre-trained on Multilingual General Corpus. When we focus on English commonsense reasoning, the model performance on SciQ (Welbl et al., 2017) and PIQA (Bisk et al., 2020) at s1 is also not worse than that at s0. It shows that pre-training on Multilingual General Corpus can enhance the model to represent Chinese, without sacrificing its ability to represent English.
We then compare the results of CMNLI (Xu et al., 2020a) of s2 and s3, s7 and s9/s8/s6. We can find the model at s3/s9/s8/s6, which are fine-tuned with Judicial Examination examples and legal consultations, can handle the Chinese NLI task better, with an improvement up to +9.3% accuracy. However, there are no CMNLI-style questions exist in our collected Judicial Examination examples and legal consultations. Thus, we guess the model may be able to further enhance its ability to comprehend and reason in Chinese via learning Judicial Examination instances and legal consultations.
Unfortunately, we also find the model fails to handle English NLI and sentiment analysis tasks. At all stages, the model can only output Yes for all MRPC examples (Dolan and Brockett, 2005). And when we continually train LLaMA, it cannot obtain obvious improvement for SST-2 (Socher et al., 2013). We guess it is because there are not

6

si (1) (2) (3) (4) (5) (6) Chinese General Tasks

English General Tasks

Chinese Legal Tasks

C3 CMNLI TNews SciQ PIQA MRPC SST-2 JE-C JE-S CAM CAP

s0

44.97 32.10 0.45 66.30 77.42 68.38 68.92 25.40 28.30 47.76 50.33

s1

50.29 32.23 6.46 66.80 77.53 68.38 51.95 26.50 28.10 10.00 32.80

s2

53.14 32.83 6.81 71.30 78.18 68.38 61.24 28.10 28.25 12.76 52.00

s3

50.50 38.76 7.37 67.50 78.29 68.38 62.04 27.30 27.05 26.84 53.37

s4

51.76 33.30 7.36 66.20 77.37 68.38 51.61 30.60 31.15 69.39 38.70

s5

51.68 32.85 6.86 71.20 78.02 68.38 61.12 28.50 28.90 82.14 52.67

s6

52.92 41.26 6.46 75.10 79.05 68.38 61.35 28.10 26.45 52.24 50.80

s7

53.28 34.57 7.91 72.30 78.07 68.38 68.92 29.30 31.15 73.88 53.17

s8

50.89 37.07 9.02 66.40 76.12 68.38 63.42 26.20 28.00 45.92 52.83

s9

50.55 40.68 3.19 67.30 78.18 68.38 67.20 26.00 27.40 56.33 57.33

s10

52.12 41.36 3.78 69.80 78.13 68.38 66.74 26.90 27.90 54.18 55.97

s11

52.54 41.68 7.67 68.30 77.91 68.38 74.43 27.10 27.80 55.10 64.73

s12

53.51 40.23 7.78 69.90 77.37 68.38 67.78 29.00 28.20 60.10 61.10

Table 1: Model performance on various NLP tasks at different stages, where the details of each stage si can refer to Figure 3. And (1)~(6) represent the pre-training corpus or the supervised fine-tuning datasets employed to train Lawyer LLaMA at different stages. To be detailed, (1) for Multilingual General Corpus, (2) for Chinese Legal Corpus, (3) for General SFT Dataset, (4) for Judicial Examination and legal consultations, (5) for multi-turn legal dialogues, and (6) for multi-turn legal dialogues augmented with retrieved law articles. The tick ( ) means that the corresponding corpus/dataset has been used at the previous stage, while the flower ( ) means the corpus/dataset is employed for training at the current stage.

enough SFT examples of English NLI and sentiment analysis. Then the model cannot understand the instructions in the prompts of such a task.
4.2 Evaluation on Chinese Legal Tasks
Beyond handling general NLP tasks, we mainly concern with whether Lawyer LLaMA can learn legal knowledge and solve domain-specific tasks. Therefore, we construct several Chinese legal tasks to evaluate the domain-specific performance of our model.
We first create multiple-choice tasks based on the objective questions of Chinese Judicial Examination (JE). For each question of JE, there are four human-written choices, among which one to four choices can be the correct answers. Since we evaluate zero-shot model performance, the model can select only one answer with the lowest perplexity. Therefore, we only evaluate the questions with one correct choice for the legal tasks. And these questions can be divided into two parts: Concept Discrimination and Scenario Planning, which are denoted as JE-C and JE-S.
Since our Chinese Legal Corpus and legal consultation datasets mainly focus on the Law of Marriage, we also examine Lawyer LLaMAâ€™s ability to solve tasks about the Law of Marriage at every training stage. In this work, we collect judgment documents from the website of China Courts, and

design classification tasks: to predict the causes of action for each document. To reduce noise, we only select verdicts of the civil first-instance.
In this work, we collect 4,875 verdicts about marriage to construct a 4-class task (CAM), and 10K verdicts about property for a 10-class task (CAP). For more details, please refer to Appendix A.
We list the results of each Chinese legal task in Table 1. Compared with s0, continually pretrained on Chinese Legal Corpus can bring the model at s4 an improvement of more than +21.6% accuracy on CAM. It shows the model can learn knowledge about marriage from the Chinese Legal Corpus. However, we find at s1 the model performance obviously deteriorates on CAM and CAP, after training on Multilingual General Corpus. Although results of C3 indicate the model at s1 can represent Chinese text better than it at s0, it still loses the ability to comprehend judgment documents. We guess the tremendous pre-training corpus of LLaMA might contain a part of legal texts, and then the original LLaMA can handle simple legal classification tasks like CAM and CAP. But the texts in Multilingual General Corpus are mainly news and Wikipedia pages, which belong to a different domain from legal texts.
Comparing the results of JE-C and JE-S at s0/s4/s7, we find that training LLaMA with Chi-

7

Question 1: ä»€ä¹ˆæ˜¯å¤«å¦»å…±åŒè´¢äº§

BELLE: å¤«å¦»å…±
åŒè´¢äº§æ˜¯æŒ‡å¤«å¦» åœ¨å©šå§»å…³ç³»ä¸­å– å¾—çš„ä»»ä½•è´¢äº§ï¼Œ åŒ…æ‹¬ä½†ä¸é™äº å·¥èµ„ã€å¥–é‡‘ã€è‚¡ ç¥¨ã€æˆ¿äº§ç­‰ã€‚

ChatGLM: å¤«å¦»å…±åŒè´¢äº§æ˜¯æŒ‡å¤«å¦»åŒæ–¹åœ¨å©š å§»æœŸé—´å…±åŒç§¯ç´¯çš„è´¢äº§ï¼ŒåŒ…æ‹¬å¤«å¦»å…±åŒå±… ä½çš„æˆ¿å±‹ã€å…±åŒæ‹¥æœ‰çš„è´¢äº§ã€å…±åŒæ”¯ä»˜çš„å·¥ èµ„ã€å¥–é‡‘ç­‰æ”¶å…¥ï¼Œä»¥åŠå…±åŒæ‰¿æ‹…çš„å€ºåŠ¡ç­‰ã€‚ æ ¹æ®ä¸­å›½æ³•å¾‹ï¼Œå¤«å¦»å…±åŒè´¢äº§æ˜¯å¤«å¦»åŒæ–¹ åœ¨å©šå§»æœŸé—´å…±åŒç§¯ç´¯çš„è´¢äº§ï¼Œé™¤éæœ‰ç‰¹åˆ«çº¦ å®šï¼Œå¦åˆ™å±äºå¤«å¦»å…±åŒè´¢äº§ã€‚åœ¨ç¦»å©šæ—¶ï¼Œå¤«
å¦»å…±åŒè´¢äº§åº”å½“å¹³ç­‰åˆ†å‰²ã€‚ å¤«å¦»å…±åŒè´¢äº§çš„åˆ†å‰²æ¯”ä¾‹ç”±å¤«å¦»åŒæ–¹åå•† ç¡®å®šï¼Œå¦‚æœæ— æ³•åå•†ï¼Œåˆ™æŒ‰ç…§ä¸‹åˆ—æ¯”ä¾‹åˆ†
å‰² ï¼š(ä¸€)å¦» å­ è· å¾— è´¢ äº§ çš„ æ¯” ä¾‹ åº” å½“ ä½ äº ä¸ˆ
å¤«ï¼›(äºŒ)å¦»å­è·å¾—è´¢äº§çš„æ¯”ä¾‹åº”å½“ä½äºä¸ˆå¤«
è·å¾—è´¢äº§çš„æ¯”ä¾‹ï¼›...

Lawyer LLaMA: å¤«å¦»å…±åŒè´¢äº§æ˜¯æŒ‡å¤«å¦»åœ¨ å©šå§»å…³ç³»å­˜ç»­æœŸé—´æ‰€å¾—çš„å·¥èµ„ã€å¥–é‡‘ã€åŠ³ åŠ¡æŠ¥é…¬ã€ç”Ÿäº§ã€ç»è¥ã€æŠ•èµ„çš„æ”¶ç›Šã€çŸ¥è¯†äº§ æƒçš„æ”¶ç›Šã€ç»§æ‰¿æˆ–è€…å—èµ çš„è´¢äº§ç­‰ï¼Œä»¥åŠå…¶ ä»–åº”å½“å½’å…±åŒæ‰€æœ‰çš„è´¢äº§ã€‚æ ¹æ®ã€Šæ°‘æ³•å…¸ã€‹
ç¬¬ä¸€åƒé›¶å…­åäºŒæ¡è§„å®šï¼Œå¤«å¦»å…±åŒè´¢äº§å½’å¤« å¦»å…±åŒæ‰€æœ‰ï¼Œå¤«å¦»å¯¹å…±åŒè´¢äº§æœ‰å¹³ç­‰çš„å¤„ç† æƒã€‚å› æ­¤ï¼Œå¤«å¦»å…±åŒè´¢äº§æ˜¯å¤«å¦»å…±åŒæ‰€æœ‰ï¼Œ
å¤«å¦»åŒæ–¹éƒ½æœ‰æƒäº«æœ‰å’Œç®¡ç†ã€‚

Question 2: ç¦»å©šå’Œæ— æ•ˆå©šå§»çš„åŒºåˆ«

BELLE: ç¦»å©šå’Œ æ— æ•ˆå©šå§»çš„åŒºåˆ« åœ¨äºï¼Œç¦»å©šæ˜¯æŒ‡ å¤«å¦»å…³ç³»å·²ç»ç»“ æŸï¼Œè€Œæ— æ•ˆå©šå§» æ˜¯æŒ‡å¤«å¦»å…³ç³»ä» æœªå­˜åœ¨è¿‡ã€‚

ChatGLM: ç¦»å©šå’Œæ— æ•ˆå©šå§»æ˜¯æŒ‡åœ¨å©šå§»åˆ¶åº¦ ä¸­çš„ä¸¤ç§ä¸åŒçŠ¶æ€ã€‚ ç¦»å©šæ˜¯æŒ‡å©šå§»å…³ç³»å·²ç»æ— æ•ˆæˆ–è¢«æ’¤é”€ï¼ŒåŒ æ–¹å¯ä»¥é€šè¿‡æ³•å¾‹ç¨‹åºç»“æŸå©šå§»å…³ç³»ã€‚ç¦»å©šçš„
æ•ˆåŠ›å–å†³äºå©šå§»æ— æ•ˆçš„åŸå› ï¼Œå¦‚æœå©šå§»æ— æ•ˆ æ˜¯å› ä¸ºåŒæ–¹æ²¡æœ‰æ„Ÿæƒ…æˆ–æ²¡æœ‰æŒ‰ç…§æ³•å¾‹è§„å®šç»“ å©šï¼Œé‚£ä¹ˆç¦»å©šå°±æ˜¯æœ‰æ•ˆçš„ã€‚å¦‚æœå©šå§»æ— æ•ˆæ˜¯ å› ä¸ºå©šå§»æœ¬èº«å­˜åœ¨é—®é¢˜ï¼Œä¾‹å¦‚è¿åäº†æ³•å¾‹è§„ å®šæˆ–æ²¡æœ‰æ„Ÿæƒ…ï¼Œé‚£ä¹ˆç¦»å©šå°±æ— æ•ˆã€‚ æ— æ•ˆå©šå§»æ˜¯æŒ‡å©šå§»å…³ç³»æ²¡æœ‰ä¾æ³•æˆç«‹ï¼Œæˆ–è€… ä¾æ³•æˆç«‹çš„å©šå§»å…³ç³»æœ¬èº«å­˜åœ¨ç¼ºé™·ã€‚æ— æ•ˆå©š
å§»çš„æ•ˆåŠ›å–å†³äºå©šå§»æ— æ•ˆçš„åŸå› ã€‚å¦‚æœå©šå§» æ— æ•ˆæ˜¯å› ä¸ºåŒæ–¹æ²¡æœ‰æ„Ÿæƒ…æˆ–æ²¡æœ‰æŒ‰ç…§æ³•å¾‹è§„
å®šç»“å©šï¼Œé‚£ä¹ˆå©šå§»å°±æ˜¯æ— æ•ˆçš„ã€‚å¦‚æœå©šå§»æ—  æ•ˆæ˜¯å› ä¸ºå©šå§»æœ¬èº«å­˜åœ¨é—®é¢˜ï¼Œä¾‹å¦‚è¿åäº†æ³• å¾‹è§„å®šæˆ–æ²¡æœ‰æ„Ÿæƒ…ï¼Œé‚£ä¹ˆå©šå§»å°±æ˜¯æ— æ•ˆçš„ã€‚ å› æ­¤ï¼Œç¦»å©šå’Œæ— æ•ˆå©šå§»ä¹‹é—´çš„åŒºåˆ«åœ¨äºå©šå§» çš„æ— æ•ˆåŸå› å’Œå©šå§»çš„æ•ˆåŠ›ã€‚ç¦»å©šæ˜¯å©šå§»å…³ç³» å·²ç»æ— æ•ˆæˆ–è¢«æ’¤é”€ï¼Œè€Œæ— æ•ˆå©šå§»æ˜¯æŒ‡å©šå§»å…³ ç³»æ²¡æœ‰ä¾æ³•æˆç«‹æˆ–ä¾æ³•æˆç«‹çš„å©šå§»å…³ç³»æœ¬èº«
å­˜åœ¨ç¼ºé™·ã€‚

Lawyer LLaMA: ç¦»å©šå’Œæ— æ•ˆå©šå§»æ˜¯ä¸¤ç§ä¸åŒ çš„æƒ…å†µã€‚ç¦»å©šæ˜¯æŒ‡å¤«å¦»åŒæ–¹è‡ªæ„¿è§£é™¤å©šå§»å…³ ç³»ï¼Œè€Œæ— æ•ˆå©šå§»æ˜¯æŒ‡åœ¨ä¸€å®šæƒ…å†µä¸‹ï¼Œå©šå§»å…³ ç³»ä»ä¸€å¼€å§‹å°±æ²¡æœ‰æˆç«‹æˆ–è¢«è®¤å®šæ— æ•ˆã€‚ æ ¹æ®ã€Šæ°‘æ³•å…¸ã€‹ç¬¬ä¸€åƒé›¶äº”åä¸€æ¡è§„å®šï¼Œå¦‚ æœå­˜åœ¨ä»¥ä¸‹æƒ…å½¢ä¹‹ä¸€ï¼Œå©šå§»å°†è¢«è§†ä¸ºæ— æ•ˆï¼š é‡å©šã€æœ‰ç¦æ­¢ç»“å©šçš„äº²å±å…³ç³»ã€æœªåˆ°æ³•å®šå©š é¾„ã€‚å¦‚æœå©šå§»è¢«è®¤å®šä¸ºæ— æ•ˆï¼Œé‚£ä¹ˆå¤«å¦»åŒæ–¹ å¹¶ä¸å…·å¤‡å¤«å¦»çš„æƒåˆ©å’Œä¹‰åŠ¡ï¼Œä¹Ÿå°±ä¸å­˜åœ¨ç¦» å©šçš„é—®é¢˜ã€‚ è€Œå¯¹äºæ— æ•ˆå©šå§»çš„è´¢äº§å¤„ç†ï¼Œæ ¹æ®ã€Šæ°‘æ³• å…¸ã€‹ç¬¬ä¸€åƒé›¶äº”åå››æ¡è§„å®šï¼Œæ— æ•ˆçš„æˆ–è€… è¢«æ’¤é”€çš„å©šå§»è‡ªå§‹æ²¡æœ‰æ³•å¾‹çº¦æŸåŠ›ï¼ŒåŒå±…æœŸ é—´æ‰€å¾—çš„è´¢äº§ï¼Œç”±å½“äº‹äººåè®®å¤„ç†ï¼›åè®®ä¸ æˆçš„ï¼Œç”±äººæ°‘æ³•é™¢æ ¹æ®ç…§é¡¾æ— è¿‡é”™æ–¹çš„åŸåˆ™ åˆ¤å†³ã€‚åŒæ—¶ï¼Œå¯¹äºé‡å©šå¯¼è‡´çš„æ— æ•ˆå©šå§»çš„è´¢ äº§å¤„ç†ï¼Œä¸å¾—ä¾µå®³åˆæ³•å©šå§»å½“äº‹äººçš„è´¢äº§æƒ
ç›Šã€‚ æ­¤å¤–ï¼Œã€Šæ°‘æ³•å…¸ã€‹ç¬¬ä¸€åƒé›¶å…­åä¸‰æ¡è§„å®šäº† å¤«å¦»å…±åŒè´¢äº§å’Œä¸ªäººè´¢äº§çš„èŒƒå›´ï¼Œå…¶ä¸­ä¸€æ–¹ çš„å©šå‰è´¢äº§å±äºä¸ªäººè´¢äº§ã€‚å› æ­¤ï¼Œå¦‚æœå©šå§» è¢«è®¤å®šä¸ºæ— æ•ˆï¼Œå¤«å¦»åŒæ–¹çš„å©šå‰è´¢äº§ä»å±äº å„è‡ªæ‰€æœ‰ã€‚ ç»¼ä¸Šæ‰€è¿°ï¼Œç¦»å©šå’Œæ— æ•ˆå©šå§»æ˜¯ä¸¤ç§ä¸åŒçš„æƒ… å†µï¼Œéœ€è¦æ ¹æ®å…·ä½“æƒ…å†µè¿›è¡Œåˆ¤æ–­ã€‚å¦‚æœæ‚¨éœ€ è¦è¿›ä¸€æ­¥äº†è§£ç›¸å…³æ³•å¾‹é—®é¢˜ï¼Œå»ºè®®æ‚¨å’¨è¯¢ä¸“ ä¸šå¾‹å¸ˆã€‚

Table 2: Case Study (Legal Concept). See the English translations in Table 8.

nese legal texts and general SFT examples can bring an improvement of more than +3.9% accuracy on JE-C and +2.85% accuracy on JE-S. It shows the model can handle legal tasks like Judicial Examination better after injecting legal knowledge. However, when the model is trained on SFT data with the form of dialogue, it seems to fail to solve Judicial Examination. We think it might be due to the dissimilarity between examination questions and consulting queries.
We compare the results of every stage comprehensively, we find the model at s12 can obviously outperform the model at s0 on most tasks, with an improvement of +8.5% accuracy on C3, +8.1% on CMNLI, +3.6% on SciQ, +3.6% on JE-C, +12.3% on CAM, and +10.8% on CAP. It is also our best

automatic assistant to respond to multi-turn legal consultations, which generates all shown cases in this paper.
5 Case study
We provide several output samples from BELLE (Ji et al., 2023a), ChatGLM (Du et al., 2022) and our Lawyer LLaMA. We test the understanding of legal concepts (Table 2) and the ability to answer legal consultations (Table 3) of different models.
5.1 Legal Concepts
Understanding legal concepts is a prerequisite for solving practical legal problems. We compare the modelsâ€™ abilities to explain and distinguish between legal terms.

8

We first ask three models to explain the concept community property (å¤«å¦»å…±åŒè´¢äº§) in Question 1 of Table 2. BELLE has a basic understanding of this term but its interpretation is imprecise, such as the expression any property (ä»»ä½•è´¢äº§). ChatGLM wrongly treats the joint debt (å…±åŒå€ºåŠ¡) as a type of joint property. It even talks nonsense about how the joint property is divided. By contrast, our model appropriately cites the related legal articles and concisely explains the concept.
We further ask the three models to distinguish between two similar concepts, divorce (ç¦»å©š) and void marriage (æ— æ•ˆå©šå§») in Question 2 of Table 2. Both BELLE and ChatGLM misunderstand the legal concept divorce and give wrong explanations. Another problem with BELLE is that its response is too colloquial and may lead to ambiguity, such as the expression never existed (ä»æœªå­˜åœ¨è¿‡). On the contrary, our model uses accurate expressions and comprehensively answers the question with related legal articles.
The above two examples demonstrate that Lawyer LLaMA is capable of understanding and distinguishing legal terminologies, thanks to the injection of legal knowledge by continual pretraining.

6 Conclusion
This paper proposes a method of applying LLMs to specific domains. First, the model needs to be pretrained on a large amount of domain-related text to help the model learn the necessary knowledge. Then, we need to design domain-specific tasks and use supervised finetuning to teach the model how to solve practical problems. Finally, we find that a retrieval model which provides external evidence would be very helpful to alleviate the hallucination problem and improve the reliability of the modelâ€™s responses.
Ethics Statement
The purpose of this paper is to explore how large language models can be applied to specific domains, with the legal domain chosen as an example. However, it should be noted that the data we have collected has not been rigorously vetted and may contain erroneous content, which should be used with caution. More importantly, the output of our trained model is not professional legal advice and may include errors. If you need legal assistance, please seek help from professionals.

5.2 Legal Consultations
An important objective of large pre-trained legal models is to use their legal knowledge to analyze practical cases under real-world scenarios. We compare the outputs of two legal cases from the three models.
Regarding Question 1 in Table 3, all three models give a detailed response. However, there are some logical problems in the outputs of BELLE and ChatGLM, such as the paragraphs about the childâ€™s wishes. Our modelâ€™s output is more logically coherent and is supported by appropriately cited law articles.
Regarding Question 2 in Table 3, BELLE cites an irrelevant law article and ChatGLM confuses void marriage (æ— æ•ˆå©šå§») and voidable marriage (å¯æ’¤é”€å©šå§»). Our model analyzes the case with related legal articles and gives practical advice.
In summary, the above two examples demonstrate that our model shows strong abilities to apply legal knowledge to solve real-world problems.
9

Question 1: å¾æŸï¼ˆç”·ï¼‰ä¸ææŸï¼ˆå¥³ï¼‰å©šåè‚²æœ‰ä¸€å­ï¼ˆ1å²åŠï¼‰ã€ä¸€å¥³ï¼ˆ8å²ï¼‰ï¼Œä¸¤äººå‡†å¤‡ç¦»å©šï¼Œéƒ½æƒ³äº‰å–å­©å­æŠšå…» æƒã€‚å¾æŸç»æµçŠ¶å†µè¾ƒå¥½ï¼Œå·¥èµ„æ”¶å…¥æ˜¯ææŸçš„æ•°å€ï¼Œè®¤ä¸ºè‡ªå·±èƒœåˆ¸åœ¨æ¡ã€‚ææŸè™½æ”¶å…¥ä¸é«˜ï¼Œä½†å¥³å„¿æ›´æ„¿æ„éšå…¶ç”Ÿæ´»ã€‚

BELLE: å¥½çš„ï¼Œæ ¹æ®ä½ æä¾›çš„ ä¿¡æ¯ï¼Œå¾æŸå’ŒææŸå‡†å¤‡ç¦»å©šï¼Œ å¹¶ä¸”ä¸¤äººéƒ½æƒ³äº‰å–å­©å­æŠšå…» æƒã€‚å¾æŸçš„ç»æµçŠ¶å†µè¾ƒå¥½ï¼Œå·¥ èµ„æ”¶å…¥æ˜¯ææŸçš„æ•°å€ï¼Œä»–è®¤ä¸º è‡ªå·±èƒœåˆ¸åœ¨æ¡ã€‚è€ŒææŸè™½ç„¶æ”¶
å…¥ä¸é«˜ï¼Œä½†å¥³å„¿æ›´æ„¿æ„éšå…¶ç”Ÿ æ´»ã€‚ é¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦äº†è§£åœ¨ä¸­å›½ç¦» å©šæ¡ˆä»¶ä¸­ï¼Œå­©å­çš„æŠšå…»æƒé€šå¸¸ æ˜¯ç”±æ³•é™¢å†³å®šçš„ã€‚æ³•é™¢ä¼šæ ¹æ®
å­©å­çš„å¹´é¾„ã€æ€§åˆ«ã€çˆ¶æ¯çš„ç» æµçŠ¶å†µã€çˆ¶æ¯çš„è¡Œä¸ºã€çˆ¶æ¯ä¹‹ é—´çš„çŸ›ç›¾ç­‰å› ç´ æ¥åšå‡ºå†³å®šã€‚ åœ¨è¿™ä¸ªæ¡ˆä¾‹ä¸­ï¼Œå¾æŸçš„ç»æµçŠ¶ å†µè¾ƒå¥½ï¼Œä½†å¥³å„¿æ›´æ„¿æ„éšå…¶ç”Ÿ æ´»ã€‚è¿™æ„å‘³ç€ï¼Œå¦‚æœå¾æŸèƒ½å¤Ÿ è¯æ˜ä»–èƒ½å¤Ÿæä¾›æ›´å¥½çš„ç”Ÿæ´»æ¡ ä»¶ï¼Œæ³•é™¢å¯èƒ½ä¼šç»™ä»–å­©å­çš„æŠš å…»æƒã€‚ ç„¶è€Œï¼Œå¦‚æœææŸèƒ½å¤Ÿè¯æ˜å¥¹èƒ½
å¤Ÿæä¾›æ›´å¥½çš„å®¶åº­ç¯å¢ƒï¼Œæ³•é™¢ ä¹Ÿå¯èƒ½ä¼šç»™å¥¹å­©å­çš„æŠšå…»æƒã€‚ å› æ­¤ï¼Œåœ¨è¿™ä¸ªæ¡ˆä¾‹ä¸­ï¼Œæ³•é™¢ å¯èƒ½ä¼šè€ƒè™‘å¤šç§å› ç´ æ¥åšå‡ºå†³
å®šï¼Œè€Œå­©å­çš„æ„æ„¿åªæ˜¯å…¶ä¸­ä¹‹
ä¸€ã€‚

ChatGLM: è¿™ä¸ªæ¡ˆä¾‹æ¶‰åŠåˆ°å­©å­æŠšå…»æƒ çš„é—®é¢˜ï¼Œéœ€è¦è€ƒè™‘å¤šä¸ªå› ç´ æ¥åšå‡ºå†³ å®šã€‚ä»¥ä¸‹æ˜¯ä¸€äº›åˆ†æå’Œå»ºè®®ï¼š 1. å­©å­çš„å¹´é¾„å’Œéœ€æ±‚ï¼šå­©å­åœ¨ä¸åŒçš„å¹´ é¾„æ®µæœ‰ä¸åŒçš„éœ€æ±‚å’Œç‰¹ç‚¹ã€‚1å²åŠçš„å­© å­é€šå¸¸éœ€è¦å…³æ³¨ä»–ä»¬çš„æ—¥å¸¸ç”Ÿæ´»ï¼Œå¦‚é¥® é£Ÿã€ç¡çœ ã€å®‰å…¨ç­‰ã€‚8å²çš„å­©å­å·²ç»è¿› å…¥äº†é’æ˜¥æœŸï¼Œéœ€è¦æ›´å¤šçš„å…³æ³¨å’Œæ”¯æŒï¼Œ ä»¥å¸®åŠ©ä»–ä»¬å‘å±•ç¤¾äº¤æŠ€èƒ½å’Œæƒ…æ„Ÿå¥åº·ã€‚ å› æ­¤ï¼ŒæŠšå…»æƒçš„å½’å±éœ€è¦è€ƒè™‘å­©å­çš„éœ€
æ±‚å’Œå¹´é¾„ã€‚ 2. å­©å­çš„æŠšå…»æƒå½’å±éœ€è¦è€ƒè™‘çˆ¶æ¯ä¹‹ é—´çš„å·®å¼‚ï¼šå¾æŸå’ŒææŸçš„æ”¶å…¥å’ŒèŒä¸š èƒŒæ™¯å·®å¼‚å¯èƒ½ä¼šå¯¹æŠšå…»æƒçš„å½’å±äº§ç”Ÿå½± å“ã€‚ä½†æ›´é‡è¦çš„æ˜¯ï¼Œå­©å­åº”è¯¥ç”±æœ€é€‚åˆ ä»–ä»¬çš„äººæŠšå…»ã€‚å› æ­¤ï¼Œéœ€è¦æ¯”è¾ƒåŒæ–¹çš„ æ€§æ ¼ã€ä¹ æƒ¯ã€æ•™è‚²æ–¹å¼ã€å®¶åº­ç¯å¢ƒç­‰å›  ç´ ï¼Œä»¥ç¡®å®šè°æ›´é€‚åˆæŠšå…»å­©å­ã€‚ 3. å­©å­çš„æ„æ„¿ï¼šå°½ç®¡ææŸçš„å¥³å„¿æ›´æ„¿æ„ éšå…¶ç”Ÿæ´»ï¼Œä½†è¿™å¹¶ä¸ä»£è¡¨å­©å­æœ¬äººä¼šåŒ æ„è·Ÿå¾æŸç”Ÿæ´»ã€‚å› æ­¤ï¼ŒæŠšå…»æƒçš„å½’å±éœ€ è¦è€ƒè™‘å­©å­çš„æ„æ„¿å’Œæ„æ„¿ã€‚å¦‚æœå­©å­æ„¿ æ„è·ŸææŸç”Ÿæ´»ï¼Œé‚£ä¹ˆè¿™å¯èƒ½ä¼šæ›´æœ‰åˆ©äº å­©å­çš„å¥åº·æˆé•¿ã€‚ 4. æ³•å¾‹å› ç´ ï¼šæ ¹æ®ä¸­å›½çš„æ³•å¾‹ï¼Œå­©å­ çš„æŠšå…»æƒåº”è¯¥æ ¹æ®æœ‰åˆ©äºå­©å­æˆé•¿çš„åŸ åˆ™è¿›è¡Œè£å†³ã€‚è¿™æ„å‘³ç€ï¼Œå¾æŸçš„ç»æµçŠ¶ å†µå¯èƒ½ä¼šå¯¹æŠšå…»æƒçš„å½’å±äº§ç”Ÿå½±å“ã€‚ä½† æ˜¯ï¼Œæœ€ç»ˆè£å†³çš„åº”è¯¥æ˜¯æ³•å¾‹ç¨‹åºï¼Œè€Œä¸
æ˜¯ä¸ªäººæ„è§ã€‚ ç»¼åˆä»¥ä¸Šå› ç´ ï¼Œå¦‚æœå¾æŸçš„ç»æµçŠ¶å†µ æ›´å¥½ï¼Œå¹¶ä¸”å­©å­æ›´æ„¿æ„è·ŸææŸç”Ÿæ´»ï¼Œé‚£ ä¹ˆææŸå¯èƒ½ä¼šè·å¾—å­©å­çš„æŠšå…»æƒã€‚ä½†æœ€ ç»ˆç»“æœè¿˜éœ€è¦æ ¹æ®æ³•å¾‹ç¨‹åºæ¥è£å†³ï¼Œå›  æ­¤ï¼Œå»ºè®®åŒæ–¹åœ¨æ³•å¾‹æ¡†æ¶å†…è¿›è¡Œæ²Ÿé€šå’Œ åå•†ï¼Œä»¥è¾¾æˆä¸€ä¸ªæœ€å¥½çš„è§£å†³æ–¹æ¡ˆã€‚

Lawyer LLaMA: æ ¹æ®ã€Šä¸­åäººæ°‘å…±å’Œ å›½å©šå§»æ³•ã€‹çš„è§„å®šï¼Œå­å¥³æŠšå…»æƒçš„åˆ¤ å†³åº”å½“ä»¥æœªæˆå¹´å­å¥³çš„åˆ©ç›Šä¸ºå‡ºå‘ç‚¹ï¼Œ ç»¼åˆè€ƒè™‘çˆ¶æ¯çš„ç»æµæ¡ä»¶ã€èº«ä½“æƒ…å†µã€ å®¶åº­ç¯å¢ƒç­‰æƒ…å†µï¼Œæœ€ç»ˆç¡®å®šæŠšå…»äººã€‚å›  æ­¤ï¼Œåœ¨æœ¬æ¡ˆä¸­ï¼Œæ³•é™¢ä¼šç»¼åˆè€ƒè™‘åŒæ–¹çš„ ç»æµèƒ½åŠ›ã€å·¥ä½œæ—¶é—´ã€å±…ä½ç¯å¢ƒç­‰å› ç´  æ¥åˆ¤æ–­æŠšå…»æƒå½’å±é—®é¢˜ã€‚ é¦–å…ˆï¼Œå¾æŸçš„ç»æµæ¡ä»¶å¥½ï¼Œæ”¶å…¥æ°´å¹³è¾ƒ é«˜ï¼Œå¯ä»¥ä¸ºå­©å­çš„æˆé•¿æä¾›æ›´å¥½çš„ç‰©è´¨ æ¡ä»¶ã€‚ä½†æ˜¯ï¼Œå¦‚æœææŸèƒ½å¤Ÿè¯æ˜è‡ªå·±æœ‰ æ›´å¥½çš„ç…§é¡¾å­©å­çš„èƒ½åŠ›ï¼Œæˆ–è€…å­©å­çš„èº« å¿ƒå¥åº·éœ€è¦ç”±å¥¹æ¥ç…§é¡¾ï¼Œé‚£ä¹ˆå¥¹ä¹Ÿæœ‰å¯ èƒ½è·å¾—æŠšå…»æƒã€‚ å…¶æ¬¡ï¼Œå¥³å„¿å¹´é¾„è¾ƒå¤§ï¼Œå·²ç»å…«å²ï¼Œå¯¹äº å¥¹çš„æ„æ„¿ä¹Ÿåº”è¯¥æœ‰æ‰€è€ƒè™‘ã€‚å¦‚æœå¥³å„¿è¡¨ ç¤ºæ„¿æ„éšæ¯äº²ç”Ÿæ´»ï¼Œæ³•é™¢ä¹Ÿä¼šè€ƒè™‘åˆ°è¿™
ä¸€å› ç´ ã€‚ ç»¼ä¸Šæ‰€è¿°ï¼Œåœ¨æœ¬æ¡ˆä¸­ï¼Œæ³•é™¢ä¼šç»¼åˆè€ƒ è™‘åŒæ–¹çš„ç»æµèƒ½åŠ›ã€ç…§é¡¾å­©å­çš„èƒ½åŠ›ã€ å¥³å„¿çš„æ„æ„¿ç­‰å› ç´ æ¥åˆ¤å†³æŠšå…»æƒå½’å±é—® é¢˜ã€‚å»ºè®®åŒæ–¹ç§¯ææ²Ÿé€šåå•†ï¼Œå°½é‡è¾¾æˆ ä¸€è‡´æ„è§ï¼Œå¹¶åœ¨æ³•å¾‹ç¨‹åºä¸­ç»´æŠ¤è‡ªå·±çš„ åˆæ³•æƒç›Šã€‚

Question 2: 21 å²å¥³å­ç”²ä¸25 å²ç”·å­ä¹™åœ¨ç½‘ä¸ŠèŠå¤©åäº§ç”Ÿå¥½æ„Ÿï¼Œä¹™å°†ç”²å‘ç»™å…¶çš„ç§å¯†ç…§ç‰‡ä¿å­˜ã€‚åä¹™è¦æ±‚ä¸ç”²ç»“å©š, ç”²ä¸åŒæ„ã€‚ä¹™å¨èƒè¦å…¬å¸ƒå…¶ç…§ç‰‡ï¼Œç”²åªå¥½åŒæ„ç»“å©šå¹¶åŠç†äº†ç™»è®°ã€‚ç”²æ˜¯å¦å¯ä»¥è‡ªå©šå§»ç™»è®°ä¹‹æ—¥èµ·1å¹´å†…è¯·æ±‚æ’¤é”€è¯¥ å©šå§»ï¼Ÿ

BELLE: æ ¹æ®ã€Šä¸­åäººæ°‘å…±å’Œ
å›½å©šå§»æ³•ã€‹è§„å®šï¼Œå©šå§»ç™»è®°æ»¡ ä¸€å¹´åï¼ŒåŒæ–¹å¯ä»¥æå‡ºç¦»å©šï¼Œ ä½†éœ€è¦æ»¡è¶³ä¸€å®šçš„æ¡ä»¶ã€‚å›  æ­¤ï¼Œç”²å¯ä»¥åœ¨å©šå§»ç™»è®°ä¸€å¹´å ç”³è¯·ç¦»å©šã€‚

ChatGLM: æ ¹æ®ä¸­å›½ã€Šå©šå§»æ³•ã€‹çš„è§„ å®šï¼Œå©šå§»å…³ç³»æ˜¯è‡ªå©šå§»å¼€å§‹æ—¶å³å­˜åœ¨ çš„ï¼Œéç»æ³•å®šç¨‹åºè€Œä¸èƒ½æ¶ˆç­ã€‚å©šå§»æ—  æ•ˆçš„åŸå› æœ‰ï¼š(ä¸€)é‡å©šçš„ï¼›(äºŒ)æœ‰ç¦æ­¢ ç»“å©šçš„ç–¾ç—…è€Œç»“å©šæ— æ•ˆçš„ï¼›(ä¸‰)å› èƒè¿« è€Œç»“å©šæ— æ•ˆçš„ï¼›(å››)è‡ªå§‹æ²¡æœ‰æ„Ÿæƒ…è€Œç»“ å©šæ— æ•ˆçš„ï¼›(äº”)å› æ¬ºè¯ˆè€Œç»“å©šæ— æ•ˆçš„ã€‚ å©šå§»æ— æ•ˆã€å¯ä¾æ³•æ’¤é”€ã€‚ç”²å¯ä»¥è‡ªå©šå§» ç™»è®°ä¹‹æ—¥èµ·1 å¹´å†…å‘å½“åœ°äººæ°‘æ³•é™¢æèµ· æ’¤é”€å©šå§»çš„è¯‰è®¼ã€‚

Lawyer LLaMA: ä¾æ®ã€Šæ°‘æ³•å…¸ã€‹ç¬¬ä¸€ åƒé›¶äº”åäºŒæ¡è§„å®šï¼Œå› èƒè¿«ç»“å©šçš„ï¼Œå— èƒè¿«çš„ä¸€æ–¹å¯ä»¥å‘äººæ°‘æ³•é™¢è¯·æ±‚æ’¤é”€å©š å§»ã€‚å› æ­¤ï¼Œå¦‚æœç”²æ˜¯åœ¨å—åˆ°èƒè¿«çš„æƒ…å†µ ä¸‹åŒæ„ç»“å©šçš„ï¼Œé‚£ä¹ˆç”²å¯ä»¥åœ¨è‡ªå©šå§»ç™» è®°ä¹‹æ—¥èµ·ä¸€å¹´å†…å‘äººæ°‘æ³•é™¢è¯·æ±‚æ’¤é”€å©š å§»ã€‚ä½†æ˜¯ï¼Œéœ€è¦è¿›ä¸€æ­¥äº†è§£çš„æ˜¯ï¼Œæ˜¯å¦ å­˜åœ¨èƒè¿«çš„æƒ…å†µï¼Œä»¥åŠèƒè¿«çš„æ–¹å¼å’Œæ‰‹
æ®µç­‰ã€‚ å¦å¤–ï¼Œæ ¹æ®ã€Šæ°‘æ³•å…¸ã€‹ç¬¬ä¸€åƒé›¶å››åå…­ æ¡è§„å®šï¼Œç»“å©šåº”å½“ç”·å¥³åŒæ–¹å®Œå…¨è‡ªæ„¿ï¼Œ ç¦æ­¢ä»»ä½•ä¸€æ–¹å¯¹å¦ä¸€æ–¹åŠ ä»¥å¼ºè¿«ï¼Œç¦æ­¢ ä»»ä½•ç»„ç»‡æˆ–è€…ä¸ªäººåŠ ä»¥å¹²æ¶‰ã€‚å¦‚æœç”²æ˜¯ åœ¨å®Œå…¨è‡ªæ„¿çš„æƒ…å†µä¸‹åŒæ„ç»“å©šçš„ï¼Œé‚£ä¹ˆ ç”²æ— æ³•è¯·æ±‚æ’¤é”€å©šå§»ã€‚ ç»¼ä¸Šæ‰€è¿°ï¼Œéœ€è¦è¿›ä¸€æ­¥äº†è§£ç”²åŒæ„ç»“ å©šçš„å…·ä½“æƒ…å†µï¼Œä»¥åŠæ˜¯å¦å­˜åœ¨èƒè¿«çš„æƒ… å†µã€‚å»ºè®®ç”²å’¨è¯¢ä¸“ä¸šå¾‹å¸ˆï¼Œå¯»æ±‚ä¸“ä¸šçš„
æ³•å¾‹å¸®åŠ©ã€‚

Table 3: Case Study (Legal Consultation). See the English translations in Table 9 and Table 10.

10

References
Stella Biderman, Hailey Schoelkopf, Quentin Anthony, Herbie Bradley, Kyle Oâ€™Brien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, Aviya Skowron, Lintang Sutawika, and Oskar van der Wal. 2023. Pythia: A suite for analyzing large language models across training and scaling.
Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi. 2020. Piqa: Reasoning about physical commonsense in natural language. In Thirty-Fourth AAAI Conference on Artificial Intelligence.
Feida Chen. 2018. The legal consultation data and corpus of the thesis from China law network.Replication Data for: Design and research of legal consultation text classification system.
Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. 2023. Vicuna: An opensource chatbot impressing gpt-4 with 90%* chatgpt quality.
Yiming Cui, Ziqing Yang, and Xin Yao. 2023. Efficient and effective text encoding for chinese llama and alpaca. arXiv preprint arXiv:2304.08177.
Cyprien de Masson dâ€™Autume, Sebastian Ruder, Lingpeng Kong, and Dani Yogatama. 2019. Episodic memory in lifelong language learning. In Proceedings of the 33rd International Conference on Neural Information Processing Systems, pages 13132â€“13141.
William B. Dolan and Chris Brockett. 2005. Automatically constructing a corpus of sentential paraphrases. In Proceedings of the Third International Workshop on Paraphrasing (IWP2005).
Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and Jie Tang. 2022. GLM: General language model pretraining with autoregressive blank infilling. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 320â€“335, Dublin, Ireland. Association for Computational Linguistics.
Yunjie Ji, Yong Deng, Yan Gong, Yiping Peng, Qiang Niu, Lei Zhang, Baochang Ma, and Xiangang Li. 2023a. Exploring the impact of instruction data scaling on large language models: An empirical study on real-world use cases. arXiv preprint arXiv:2303.14742.
Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea Madotto, and Pascale Fung. 2023b. Survey of hallucination in natural language generation. ACM Comput. Surv., 55(12).

Jinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon Kim, Sunkyu Kim, Chan Ho So, and Jaewoo Kang. 2019. BioBERT: a pre-trained biomedical language representation model for biomedical text mining. Bioinformatics, 36(4):1234â€“1240.
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. RoBERTa: A robustly optimized BERT pretraining approach. CoRR, abs/1907.11692.
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 21(140):1â€“67.
Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and Christopher Potts. 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1631â€“1642, Seattle, Washington, USA. Association for Computational Linguistics.
Kai Sun, Dian Yu, Dong Yu, and Claire Cardie. 2020. Investigating prior knowledge for challenging Chinese machine reading comprehension. Transactions of the Association for Computational Linguistics, 8:141â€“ 155.
Mingxu Tao, Yansong Feng, and Dongyan Zhao. 2023. Can BERT refrain from forgetting on sequential tasks? a probing study. In The Eleventh International Conference on Learning Representations.
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, TimothÃ©e Lacroix, Baptiste RoziÃ¨re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971.
Johannes Welbl, Nelson F. Liu, and Matt Gardner. 2017. Crowdsourcing multiple choice science questions. In Proceedings of the 3rd Workshop on Noisy User-generated Text, pages 94â€“106, Copenhagen, Denmark. Association for Computational Linguistics.
Liang Xu, Hai Hu, Xuanwei Zhang, Lu Li, Chenjie Cao, Yudong Li, Yechen Xu, Kai Sun, Dian Yu, Cong Yu, Yin Tian, Qianqian Dong, Weitang Liu, Bo Shi, Yiming Cui, Junyi Li, Jun Zeng, Rongzhao Wang, Weijian Xie, Yanting Li, Yina Patterson, Zuoyu Tian, Yiwen Zhang, He Zhou, Shaoweihua Liu, Zhe Zhao, Qipeng Zhao, Cong Yue, Xinrui Zhang, Zhengliang Yang, Kyle Richardson, and Zhenzhong Lan. 2020a. CLUE: A Chinese language understanding evaluation benchmark. In Proceedings of the 28th International Conference on Computational

11

Linguistics, pages 4762â€“4772, Barcelona, Spain (Online). International Committee on Computational Linguistics. Liang Xu, Xuanwei Zhang, and Qianqian Dong. 2020b. Cluecorpus2020: A large-scale chinese corpus for pre-training language model. Sha Yuan, Hanyu Zhao, Zhengxiao Du, Ming Ding, Xiao Liu, Yukuo Cen, Xu Zou, Zhilin Yang, and Jie Tang. 2021. Wudaocorpora: A super large-scale chinese corpora for pre-training language models. AI Open, 2:65â€“68. Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, Weng Lam Tam, Zixuan Ma, Yufei Xue, Jidong Zhai, Wenguang Chen, Zhiyuan Liu, Peng Zhang, Yuxiao Dong, and Jie Tang. 2023. GLM-130b: An open bilingual pre-trained model. In The Eleventh International Conference on Learning Representations (ICLR). Haoxi Zhong, Zhipeng Guo, Cunchao Tu, Chaojun Xiao, Zhiyuan Liu, and Maosong Sun. 2018. Legal judgment prediction via topological learning. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3540â€“3549, Brussels, Belgium. Association for Computational Linguistics. Haoxi Zhong, Chaojun Xiao, Cunchao Tu, Tianyang Zhang, Zhiyuan Liu, and Maosong Sun. 2020a. Jecqa: A legal-domain question answering dataset. In Proceedings of AAAI. Huilin Zhong, Junsheng Zhou, Weiguang Qu, Yunfei Long, and Yanhui Gu. 2020b. An element-aware multi-representation model for law article prediction. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 6663â€“6668, Online. Association for Computational Linguistics.
12

A Details of Legal Classification Tasks
For the classification task of causes of action, we collect verdicts from the website of China Courts. The website pages provide full text and humanannotated cause of action for every public judgement document. A judgement document always contains three main parts, plaintiff allegation, defence, and verdict. In this work, we first keep all judgement documents of the civil first-instance, since such documents have a similar structure and their verdicts only contain the evidence verified by Court and the decrees.
Judgement documents of the civil secondinstance are partly more complex, part of which might overrule decrees of the civil first-instance, while others support the civil first-instance. And the civil mediation documents always only contain a very simple sentence, Mediated by this Court, both litigants agree to reconciliation (ç»æœ¬é™¢è°ƒ è§£ï¼ŒåŒæ–¹å½“äº‹äººä¸€è‡´åŒæ„æ’¤è¯‰). Therefore, these two kinds of judgement documents are not proper for classification tasks.
For the classification task about marriage (CAM), we select four kinds of causes of action, whose meanings have no overlapping to each other. To be detailed, we choose dispute over family affairs and succession (å©šå§»å®¶åº­ã€ç»§æ‰¿çº çº·), dispute over engagement property (å©šçº¦è´¢äº§çº çº·), dispute over property after divorce (ç¦»å©šåè´¢äº§çº  çº·), and dispute over a void marriage (å©šå§»æ— æ•ˆçº  çº·). We sample 4,875 verdicts from all judgement documents, whose text lengths are between 150 and 500 characters. Among them, 3,895 verdicts are examples of the training set, and the other 980 ones for the test set.
For the classification task about property (CAP), we use similar rules to sample 70,000 verdicts for training and 30,000 for testing. We select ten kinds of causes of action about property, which are all common causes that appear on the website of China Courts. Specifically, they are (1) dispute over land contract in rural areas (å†œæ‘åœŸåœ°æ‰¿åŒ…åˆåŒçº çº·), (2) dispute over a rural house purchase contract (å†œ æ‘æˆ¿å±‹ä¹°å–åˆåŒçº çº·), (3) dispute over a commodity house contract (å•†å“æˆ¿é”€å”®åˆåŒçº çº·), (4) dispute over a house lease contract (æˆ¿å±‹ç§ŸèµåˆåŒçº çº·), (5) dispute over ill-gotten gains (ä¸å½“å¾—åˆ©çº çº·), (6) dispute over a small-sum loan contract (å°é¢å€Ÿæ¬¾åˆ åŒçº çº·), (7) dispute over employment contract (åŠ³åŠ¨ åˆåŒçº çº·), (8) dispute over labour contract (åŠ³åŠ¡åˆ åŒçº çº·), (9) dispute over alimony (æŠšå…»è´¹çº çº·), and

(10) dispute over repayment of ancestorâ€™s debts (è¢« ç»§æ‰¿äººå€ºåŠ¡æ¸…å¿çº çº·).
It is also worth noting that the training sets are not used when evaluating the zero-shot performance of the model at every stage. The results in Table 1 are all based on the test set (if existing, otherwise the validation set).
B English Translations of Examples
We prove the English translations of the Chinese examples in the tables and figures.

Question A :KDWLVWKHOHJDODJHIRUPDUULDJHLQ&KLQD"

BELLE
,QPDLQODQG &KLQDWKHOHJDO DJHIRUPDUULDJH LV\HDUVROG

Lawyer LLaMA
$FFRUGLQJWR$UWLFOHRIWKH&LYLO&RGHLQ &KLQDWKHOHJDODJHIRUPHQWRJHWPDUULHGLV \HDUVROGDQGIRUZRPHQLWLV\HDUVROG,Q RWKHUZRUGVLQ&KLQDLI\RXDUHPDOH\RXPXVW EHDWOHDVW\HDUVROGWROHJDOO\PDUU\DQGLI \RXDUHIHPDOH\RXPXVWEHDWOHDVW\HDUVROG WROHJDOO\PDUU\

Question B
5HIHUHQFH$UWLFOH$UWLFOHRIWKH&LYLO&RGHVWDWHVWKDWGXULQJWKH SHULRGRISUHJQDQF\ZLWKLQRQH\HDUDIWHUFKLOGELUWKRUZLWKLQVL[ PRQWKVDIWHUWKHWHUPLQDWLRQRISUHJQDQF\WKHPDQFDQQRWLQLWLDWHD GLYRUFH+RZHYHUWKLVGRHVQRWDSSO\LIWKHZRPDQLQLWLDWHVWKHGLYRUFH RULIWKHSHRSOH
VFRXUWGHHPVLWQHFHVVDU\WRDFFHSWWKHPDQ
VGLYRUFH UHTXHVW

,KDYHDPRQWKROGEDE\DQGP\KXVEDQGZDQWVWRGLYRUFHPH:KDW VKRXOG,GR"

BELLE
7KLVPHDQVWKDW \RXUKXVEDQG
V UHTXHVWPD\ QHHGWREH FRQVLGHUHGE\ WKHFRXUWWR GHWHUPLQH ZKHWKHULWLV QHFHVVDU\WR DFFHSW<RXFDQ FRQVXOWDODZ\HU WRXQGHUVWDQG \RXUULJKWVDQG REOLJDWLRQVDV ZHOODVKRZEHVW WRKDQGOHWKLV VLWXDWLRQ

Lawyer LLaMA
+HOORDFFRUGLQJWRWKHSURYLVLRQVRIWKH&LYLO &RGHLI\RXDUHZLWKLQRQH\HDUDIWHUJLYLQJ ELUWK\RXUKXVEDQGFDQQRWXQLODWHUDOO\Æ‰OHIRU GLYRUFH+RZHYHULI\RXZDQWWRGLYRUFH\RXUVHOI RULIWKHFRXUWGHHPVLWQHFHVVDU\WRDFFHSW\RXU KXVEDQG
VGLYRUFHUHTXHVWWKHQGLYRUFHLV SRVVLEOH,IERWK\RXDQG\RXUKXVEDQGDUHZLOOLQJ WRGLYRUFH\RXQHHGWRVLJQDZULWWHQGLYRUFH DJUHHPHQWDQGSHUVRQDOO\DSSO\IRUGLYRUFH UHJLVWUDWLRQDWWKHPDUULDJHUHJLVWUDWLRQRÆ‹FH 7KHGLYRUFHDJUHHPHQWVKRXOGVWDWHWKHPXWXDO FRQVHQWWRGLYRUFHDQGFRQVHQVXVRQPDWWHUV VXFKDVFKLOGFXVWRG\SURSHUW\GLYLVLRQDQGGHEW VHWWOHPHQW'R\RXQHHGIXUWKHULQIRUPDWLRQRQ FKLOGFXVWRG\SURSHUW\GLYLVLRQRUDQ\RWKHU UHODWHGPDWWHUV"

Figure 4: The English translation of Figure 1.

13

Consultation from User
,ZRXOGOLNHWRDVNZKDWLV WKHOHJDOPLQLPXPDJHIRU PDUULDJH"

Law Retriever

Retrieved Legal Articles
$UWLFOHRIWKH&LYLO&RGH7KHPLQLPXP PDUULDJHDJHLV\HDUVIRUPDOHVDQG \HDUVIRUIHPDOHV $UWLFOHRIWKH&LYLO&RGH $UWLFOHRIWKH&LYLO&RGH

Model Input 3OHDVHUHIHUWRWKHSURYLGHGUHIHUHQFHDUWLFOHVDQGUHSO\WRWKHXVHU
VLQTXLU\7KHUHPD\EH LUUHOHYDQWDUWLFOHVLQWKHUHIHUHQFHDUWLFOHVVRSOHDVHDYRLGTXRWLQJWKRVHXQUHODWHGDUWLFOHV ZKHQUHSO\LQJ +XPDQ0\SDUWQHUDQG,ZDQWWRJHWPDUULHG,ZRXOGOLNHWRDVNZKDWLVWKHOHJDOPLQLPXP DJHIRUPDUULDJH" 5HIHUHQFH$UWLFOHV $UWLFOHRIWKH&LYLO&RGH7KHPLQLPXPPDUULDJHDJHLV\HDUVIRUPDOHVDQG\HDUVIRU IHPDOHV $UWLFOHRIWKH&LYLO&RGH $UWLFOHRIWKH&LYLO&RGH
$VVLVWDQW

Lawyer LLaMA
Model Output $FFRUGLQJWR$UWLFOHRIWKH&LYLO&RGHWKHOHJDOPLQLPXPPDUULDJHDJHLV\HDUVROGIRU PDOHVDQG\HDUVROGIRUIHPDOHV,I\RXLQVLVWRQJHWWLQJPDUULHGEHIRUHUHDFKLQJWKHOHJDO PDUULDJHDJHWKHPDUULDJHZLOOEHFRQVLGHUHGLQYDOLG 0D\,DVNLI\RXDQG\RXUSDUWQHUKDYHUHDFKHGWKHOHJDOPDUULDJHDJH",I\RXKDYHQ
WLWLV UHFRPPHQGHGWKDW\RXZDLWXQWLO\RXUHDFKWKHOHJDOPDUULDJHDJHEHIRUHFRQVLGHULQJPDUULDJH
Figure 5: The English translation of Figure 2.

Question: å…³äºäº¤é€šè‚‡äº‹ç½ªä¸å…¶ä»–çŠ¯ç½ªå…³ç³»çš„ è®ºè¿°ï¼Œä¸‹åˆ—é€‰é¡¹æ˜¯æ­£ç¡®çš„å—ï¼Ÿä¸é…’åé©¾é©¶äº¤ é€šè‚‡äº‹åï¼Œç»§ç»­é©¾è½¦åˆ°å¤„å†²æ’ï¼Œé€ æˆäº”æ­»ä¹
ä¼¤ï¼Œåº”å®šäº¤é€šè‚‡äº‹ç½ªã€‚

Question: Is the following statement about the relationship between the crime of traffic accident and other crimes correct?After Bob drove under the influence of alcohol and caused a traffic accident, he continued driving and crashed into various places, resulting in five deaths and nine injuries. He should be charged with the crime of traffic accident.

Response: æ ¹æ®ã€Šä¸­åäººæ°‘å…±å’Œå›½åˆ‘æ³•ã€‹è§„ å®šï¼Œé†‰é…’åé©¾é©¶æœºåŠ¨è½¦è¾†ï¼Œå‘ç”Ÿé‡å¤§äº¤é€šäº‹ æ•…ï¼Œæ„æˆäº¤é€šè‚‡äº‹ç½ªã€‚ä¸é…’åé©¾é©¶å‘ç”Ÿäº¤é€š äº‹æ•…ï¼Œå·²ç»æ„æˆäº¤é€šè‚‡äº‹ç½ªã€‚ä½†æ˜¯ï¼Œä¸ç»§ç»­ é©¾è½¦åˆ°å¤„å†²æ’ï¼Œé€ æˆäº”æ­»ä¹ä¼¤ï¼Œå·²ç»è¶…å‡ºäº†
äº¤é€šè‚‡äº‹ç½ªçš„èŒƒç•´ï¼Œåº”å½“è®¤å®šä¸ºå±å®³å…¬å…±å®‰ å…¨ç½ªã€‚å› æ­¤ï¼Œä¸åº”å½“è¢«å®šç½ªä¸ºå±å®³å…¬å…±å®‰å…¨
ç½ªï¼Œè€Œä¸æ˜¯äº¤é€šè‚‡äº‹ç½ªã€‚

Response: According to the provisions of the Criminal Law of the Peopleâ€™s Republic of China, driving a motor vehicle under the influence of alcohol and causing a serious traffic accident constitutes the crime of traffic accident. After driving under the influence, Bob caused a traffic accident, which has already constituted the crime of traffic accident. However, Bob continued to drive and collided everywhere, resulting in five deaths and nine injuries, which goes beyond the scope of the crime of traffic accident and should be identified as the crime of endangering public safety. Therefore, Bob should be convicted of the crime of endangering public safety instead of the crime of traffic accident.

Table 4: The English translation of the example in Section 2.2.

Question: æˆ‘å’Œæˆ‘å¯¹è±¡æƒ³ç»“å©šï¼Œæˆ‘æƒ³é—®ä¸€ä¸‹ï¼Œ Question: I would like to ask, what is the legal minimum age for marriage? ç»“å©šçš„æ³•å®šå©šé¾„æ˜¯å‡ å²å•Šï¼Ÿ

Response: æ ¹æ®ã€Šä¸­åäººæ°‘å…±å’Œå›½å©šå§»æ³•ã€‹
è§„å®šï¼Œç”·å¥³ç»“å©šçš„æ³•å®šå©šé¾„ä¸ºç”·æ»¡22å‘¨å²ï¼Œ å¥³ æ»¡20å‘¨ å² ã€‚ æœª è¾¾ åˆ° æ³• å®š å©š é¾„ çš„ äºº ä¸ å¾— ç»“ å©šã€‚ä½†æœ‰ç‰¹æ®Šæƒ…å†µçš„å¯ä»¥æå‰ç»“å©šï¼Œéœ€è¦å¾— åˆ°çˆ¶æ¯æˆ–è€…ç›‘æŠ¤äººçš„ä¹¦é¢åŒæ„ï¼Œå¹¶ç»å½“åœ°å©š å§»ç™»è®°æœºå…³å®¡æŸ¥æ‰¹å‡†ã€‚

Response: According to the Marriage Law of the Peopleâ€™s Republic of China, the legal minimum age for marriage is 22 years for men and 20 years for women. Individuals who have not reached the legal minimum age are not allowed to get married. However, under special circumstances, it is possible to marry before reaching the legal minimum age with the written consent of parents or guardians, and approval from the local marriage registration authority after examination.

Table 5: The English translation of the one-turn legal consultation in Section 2.3.

14

Client: æˆ‘å’Œæˆ‘å¯¹è±¡æƒ³ç»“å©šï¼Œæˆ‘æƒ³é—®ä¸€ä¸‹ï¼Œç»“ Client: I would like to ask, what is the legal minimum age for marriage? å©šçš„æ³•å®šå©šé¾„æ˜¯å‡ å²å•Šï¼Ÿ

Lawyer: æ ¹æ®ã€Šä¸­åäººæ°‘å…±å’Œå›½å©šå§»æ³•ã€‹è§„
å®šï¼Œç”·å¥³ç»“å©šçš„æ³•å®šå©šé¾„ä¸ºç”·æ»¡22å‘¨å²ï¼Œå¥³ æ»¡20å‘¨å²ã€‚æœªè¾¾åˆ°æ³•å®šå©šé¾„çš„äººä¸å¾—ç»“å©šã€‚ ä½†æœ‰ç‰¹æ®Šæƒ…å†µçš„å¯ä»¥æå‰ç»“å©šï¼Œéœ€è¦å¾—åˆ°çˆ¶ æ¯æˆ–è€…ç›‘æŠ¤äººçš„ä¹¦é¢åŒæ„ï¼Œå¹¶ç»å½“åœ°å©šå§»ç™» è®°æœºå…³å®¡æŸ¥æ‰¹å‡†ã€‚

Lawyer: According to the Marriage Law of the Peopleâ€™s Republic of China, the legal minimum age for marriage is 22 years for men and 20 years for women. Individuals who have not reached the legal minimum age are not allowed to get married. However, under special circumstances, it is possible to marry before reaching the legal minimum age with the written consent of parents or guardians, and approval from the local marriage registration authority after examination.

Client: æˆ‘çŸ¥é“ï¼Œå¬åˆ«äººè¯´å©šå§»éœ€è¦æå‰åŠ Client: I know that I have to go through some procedures in advance for

ç†ä¸€äº›æ‰‹ç»­ï¼Œé‚£æˆ‘éœ€è¦å‡†å¤‡å“ªäº›è¯ä»¶å’Œèµ„æ–™ marriage. What documents and materials do I need to prepare? Do I need

å‘¢ï¼Ÿæ˜¯å¦éœ€è¦æå‰åŠç†å¥åº·æ£€æŸ¥å’Œä½“æ£€ï¼Ÿ

to arrange a health check and physical examination in advance?

Lawyer: æ˜¯çš„ï¼Œå©šå§»ç™»è®°éœ€è¦æ‚¨å‡†å¤‡ä¸€äº›è¯ Lawyer: Yes, marriage registration requires you to prepare some docu-

ä»¶å’Œèµ„æ–™ï¼ŒåŒ…æ‹¬ï¼š...

ments and information, including ...

Table 6: The English translation of the multi-turn legal consultation in Section 2.3.

Related Articles: ã€Šæ°‘æ³•å…¸ã€‹ç¬¬ä¸€åƒé›¶å››åä¸ƒ æ¡ï¼šç»“å©šå¹´é¾„ï¼Œç”·ä¸å¾—æ—©äºäºŒåäºŒå‘¨å²ï¼Œå¥³ ä¸å¾—æ—©äºäºŒåå‘¨å²ã€‚ã€Šæ°‘æ³•å…¸ã€‹ç¬¬ä¸€åƒé›¶å›› åå…­æ¡ï¼šç»“å©šåº”å½“ç”·å¥³åŒæ–¹å®Œå…¨è‡ªæ„¿ï¼Œç¦æ­¢ ä»»ä½•ä¸€æ–¹å¯¹å¦ä¸€æ–¹åŠ ä»¥å¼ºè¿«ï¼Œç¦æ­¢ä»»ä½•ç»„ç»‡ æˆ–è€…ä¸ªäººåŠ ä»¥å¹²æ¶‰ã€‚ã€Šæ°‘æ³•å…¸ã€‹ç¬¬ä¸‰ç™¾é›¶å›› æ¡ï¼šå…±æœ‰äººå¯ä»¥åå•†ç¡®å®šåˆ†å‰²æ–¹å¼ã€‚è¾¾ä¸æˆ
åè®®...

Related Articles: Article 1,047 of the Civil Code: The minimum marriage age is twenty-two years for men and twenty years for women. Article 1,046 of the Civil Code: Marriage shall be based on the complete voluntary will of both men and women. It is prohibited for either party to coerce the other, and any interference by any organization or individual is also prohibited. Article 304 of the Civil Code: Co-owners may negotiate and determine the method of partition. If no agreement can be reached...

Client: æˆ‘å’Œæˆ‘å¯¹è±¡æƒ³ç»“å©šï¼Œæˆ‘æƒ³é—®ä¸€ä¸‹ï¼Œç»“ Client: I would like to ask, what is the legal minimum age for marriage? å©šçš„æ³•å®šå©šé¾„æ˜¯å‡ å²å•Šï¼Ÿ

Lawyer: æ ¹æ®ã€Šæ°‘æ³•å…¸ã€‹ç¬¬ä¸€åƒé›¶å››åä¸ƒ æ¡è§„å®šï¼Œç»“å©šçš„æ³•å®šå©šé¾„ä¸ºç”·ä¸å¾—æ—©äº22å‘¨ å²ï¼Œå¥³ä¸å¾—æ—©äº20å‘¨å²ã€‚å¦‚æœæœªåˆ°æ³•å®šç»“å©š å¹´é¾„è€ŒåšæŒç»“å©šï¼Œå°†è¢«è®¤å®šä¸ºæ— æ•ˆå©šå§»ã€‚ è¯·é—®æ‚¨å’Œæ‚¨çš„å¯¹è±¡çš„å¹´é¾„æ˜¯å¦å·²è¾¾åˆ°æ³•å®šå©š é¾„ï¼Ÿå¦‚æœæœªè¾¾åˆ°ï¼Œå»ºè®®æ‚¨ç­‰å¾…è‡³æ³•å®šå©šé¾„å† è€ƒè™‘ç»“å©šçš„é—®é¢˜ã€‚

Lawyer: According to Article 1,047 of the Civil Code, the legal marriageable age is a minimum of 22 years old for males and 20 years old for females. If individuals insist on getting married before reaching the legal marriageable age, the marriage will be deemed void. May I ask if both you and your partner have reached the legal marriageable age? If not, I would suggest waiting until you reach the legal marriageable age before considering marriage.

Client: æˆ‘ä»¬éƒ½æ»¡è¶³æ³•å®šç»“å©šå¹´é¾„äº†ï¼Œä½†æ˜¯æˆ‘ Client: We both meet the legal marriage age requirement, but our parents

ä»¬çš„å®¶é•¿åå¯¹ï¼Œä»–ä»¬è¯´æˆ‘ä»¬è¿˜å¤ªå¹´è½»ï¼Œä¸åº” are against it. They say weâ€™re still too young and shouldnâ€™t rush into

è¯¥æ€¥äºç»“å©šï¼Œä½ æ€ä¹ˆçœ‹ï¼Ÿ

marriage. What do you think?

Lawyer: æ ¹æ®ã€Šæ°‘æ³•å…¸ã€‹ç¬¬ä¸€åƒé›¶å››åå…­æ¡ è§„å®šï¼Œç»“å©šåº”å½“ç”·å¥³åŒæ–¹å®Œå…¨è‡ªæ„¿ï¼Œç¦æ­¢ä»» ä½•ä¸€æ–¹å¯¹å¦ä¸€æ–¹åŠ ä»¥å¼ºè¿«ï¼Œç¦æ­¢ä»»ä½•ç»„ç»‡æˆ– è€…ä¸ªäººåŠ ä»¥å¹²æ¶‰ã€‚å› æ­¤ï¼Œå¦‚æœæ‚¨å’Œæ‚¨çš„å¯¹è±¡ å®Œå…¨è‡ªæ„¿å¹¶ä¸”å·²è¾¾åˆ°æ³•å®šç»“å©šå¹´é¾„ï¼Œæ‚¨ä»¬å¯ ä»¥åˆæ³•ç»“å©šï¼Œå³ä½¿å®¶é•¿åå¯¹ã€‚å½“ç„¶ï¼Œä½œä¸ºå°Š é‡å®¶é•¿ã€ä¿æŒè‰¯å¥½äº²æƒ…å…³ç³»çš„ä¸€ç§æ–¹å¼ï¼Œå¯ ä»¥å°è¯•ä¸å®¶é•¿æ²Ÿé€šï¼Œäº†è§£å…¶åå¯¹çš„åŸå› ï¼Œå¹¶ è€ƒè™‘å¦‚ä½•è§£å†³å…¶å…³æ³¨çš„é—®é¢˜ã€‚å¦‚æœå®¶é•¿åå¯¹ å¹¶ä¸åˆç†ï¼Œæ‚¨å’Œæ‚¨çš„å¯¹è±¡æœ‰æƒåˆ©è‡ªä¸»å†³å®šã€‚

Lawyer: According to Article 1,046 of the Civil Code, marriage should be based on the full and voluntary consent of both parties, and it is prohibited to coerce either party or for any organization or individual to interfere. Therefore, if you and your partner are fully willing and have reached the legal marriageable age, you can legally get married, even if your parents are against it. However, as a way to respect your parents and maintain a good relationship with them, you can try to communicate with them, understand their reasons for opposition, and consider how to address their concerns. If their opposition is unreasonable, you and your partner have the right to make an autonomous decision.

Table 7: The English translation of the retrieval augmented legal consultation in Section 2.3.

15

Question 1: What is community property?

BELLE: Community property refers to any property acquired by a married couple during their marital relationship, including but not limited to wages, bonuses, stocks, real estate, and so on.

ChatGLM: Community property refers to the property accumulated jointly by both parties during the marriage, including the jointly owned house, jointly owned assets, income such as wages and bonuses paid jointly, as well as jointly debt, etc. According to Chinese law, community property refers to the property accumulated jointly by both parties during the marriage, unless otherwise agreed upon. Upon divorce, the community property of husband and wife should be divided equally. The proportion of division of community property is determined through negotiation between the husband and wife. If no agreement can be reached, the following proportions shall be applied: (1) The proportion of property acquired by the wife should be lower than that of the husband; (2) The proportion of property acquired by the wife should be lower than the proportion of property acquired by the husband; ...

Lawyer LLaMA: Community property refers to the wages, bonuses, remuneration for labor, proceeds from production, operation, investment, and intellectual property rights, as well as property acquired through inheritance or gift during the existence of the marriage, and other property that should be jointly owned. According to Article 1062 of the Civil Code, the community property of a husband and wife belongs to both of them jointly, and they have equal rights to deal with the community property. Therefore, the community property of a husband and wife is jointly owned by both parties, and both husband and wife have the right to enjoy and manage it.

Question 2: What is the difference between divorce and void marriage?

BELLE: The difference between divorce and void marriage lies in the fact that divorce refers to the termination of a marital relationship, while a void marriage refers to a marital relationship that never existed.

ChatGLM: Divorce and void marriage refer to two different states within the institution of marriage. Divorce signifies that the marital relationship has become void or has been revoked, and both parties can terminate the marriage through legal procedures. The effectiveness of divorce depends on the reasons for the marriageâ€™s invalidity. If the marriage is deemed void due to a lack of affection between the parties or a failure to comply with legal requirements, then the divorce is considered valid. However, if the marriage is void due to inherent issues such as violating law articles or lacking affection, then the divorce is considered void. On the other hand, a void marriage refers to a marital relationship that has not been legally established or has inherent defects despite being legally recognized. The effectiveness of a void marriage also depends on the reasons for its invalidity. If the marriage is deemed void due to a lack of affection between the parties or a failure to comply with legal requirements, then the marriage is considered void. However, if the marriage is void due to inherent issues such as violating law articles or lacking affection, then the marriage is considered void. Therefore, the difference between divorce and a void marriage lies in the reasons for the marriageâ€™s invalidity and its effectiveness. Divorce signifies that the marital relationship has become void or has been revoked, whereas a void marriage refers to a marital relationship that has not been legally established or has inherent defects despite being legally recognized.

Lawyer LLaMA: Divorce and void marriage are two different situations. Divorce refers to the voluntary termination of the marital relationship by both parties, while a void marriage refers to a situation where the marital relationship was not established from the beginning or has been deemed void under certain circumstances. According to Article 1051 of the Civil Code, a marriage will be considered void if any of the following circumstances exist: bigamy, prohibited kinship, or failure to meet the legal marriage age requirement. If a marriage is declared void, the spouses do not possess the rights and obligations of a married couple, and the issue of divorce does not arise. Regarding the property division of a void marriage, as stipulated in Article 1054 of the Civil Code, a void or revoked marriage has no legal binding force from the beginning. The property acquired during cohabitation is to be handled by the parties through an agreement. In case of failure to reach an agreement, the Peopleâ€™s Court will make a judgment based on the principle of taking care of the innocent party. Furthermore, the property division in cases of void marriage resulting from bigamy must not infringe upon the property rights and interests of the parties in lawful marriage. In addition, Article 1063 of the Civil Code defines the scope of joint property and personal property for spouses, with one partyâ€™s premarital property classified as personal property. Therefore, if a marriage is deemed void, the premarital property of both spouses remains individually owned. In conclusion, divorce and void marriage are two different situations that require judgment based on specific circumstances. If you need further understanding of related legal issues, it is recommended to consult a professional

Table 8: The English translations of the questions in Table 2.

16

Question 1: Mr. Xu (male) and Ms. Li (female) have a son (1 and a half years old) and a daughter (8 years old) after their marriage. They are planning to get a divorce and both want to fight for custody of the children.Mr. Xu is in a better financial condition, with a salary several times higher than Ms. Liâ€™s, and he believes he has the upper hand. Although Ms. Liâ€™s income is not high, their daughter prefers to live with her.

BELLE: Okay, based on the information you provided, Mr. Xu and Ms. Li are preparing for a divorce, and both of them want to fight for custody of their child. Mr. Xu is in a better financial condition, with a salary several times higher than Ms. Liâ€™s, and he believes he has the upper hand. However, although Ms. Li has a lower income, their daughter prefers to live with her. First, we need to understand that in divorce cases in China, the custody of the child is usually determined by the court. The court takes into account factors such as the childâ€™s age, gender, parentsâ€™ financial condition, parentsâ€™ behavior, and conflicts between the parents to make a decision. In this case, Mr. Xu has a better financial condition, but their daughter prefers to live with Ms. Li. This means that if Mr. Xu can prove that he can provide better living conditions, the court may grant him custody of the child. However, if Ms. Li can prove that she can provide a better family environment, the court may grant her custody of the child. Therefore, in this case, the court may consider multiple factors to make a decision, and the childâ€™s preference is just one of them.

ChatGLM: This case involves the issue of child custody and requires considering multiple factors to make a decision. Here are some analysis and recommendations: 1. Childâ€™s age and needs: Children have different needs and characteristics at different stages of their lives. A one-and-a-half-yearold child typically requires attention to their daily life, such as diet, sleep, and safety. An eight-year-old child has entered adolescence and requires more attention and support to help them develop social skills and emotional well-being. Therefore, the allocation of custody needs to consider the childâ€™s needs and age. 2. Parental differences: The income and occupational background differences between Xu and Li may impact the custody decision. However, more importantly, the child should be raised by the person who is best suited for them. Therefore, it is necessary to compare factors such as personalities, habits, parenting styles, and family environment to determine who is more suitable for raising the child. 3. Childâ€™s wishes: Although Liâ€™s daughter may prefer to live with her, it does not necessarily mean the child would agree to live with Xu. Therefore, the allocation of custody needs to consider the childâ€™s wishes and preferences. If the child is willing to live with Li, it may be more beneficial for the childâ€™s healthy development. 4. Legal factors: According to Chinese law, the custody of a child should be decided based on what is in the best interest of the childâ€™s growth. This means that Xuâ€™s financial situation may influence the custody decision. However, the final decision should be based on legal procedures rather than personal opinions. Considering the above factors, if Xu has a better financial situation and the child is more willing to live with Li, Li may be granted custody of the child. However, the final outcome should be determined through legal procedures. Therefore, it is recommended that both parties engage in communication and negotiation within the legal framework to reach the best possible solution.

Lawyer LLaMA: According to the provisions of the Marriage Law of the Peopleâ€™s Republic of China, the judgment of child custody should be based on the best interests of the minor children, taking into account the parentsâ€™ economic conditions, physical health, family environment, and other factors, and ultimately determining the custodian. Therefore, in this case, the court will consider the partiesâ€™ economic ability, working hours, living environment, and other factors to determine the issue of child custody. Firstly, Xu has a good financial condition and a higher income, which can provide better material conditions for the childâ€™s upbringing. However, if Li can prove that she has a better ability to take care of the child or that the childâ€™s physical and mental health needs to be taken care of by her, she may also obtain custody. Secondly, the daughter is older, already eight years old, and her wishes should also be taken into consideration. If the daughter expresses a willingness to live with her mother, the court will also take this factor into account. In conclusion, in this case, the court will comprehensively consider the partiesâ€™ economic ability, caregiving ability, the daughterâ€™s wishes, and other factors in determining the issue of child custody. It is recommended that both parties actively communicate and negotiate, try to reach a consensus, and protect their legitimate rights and interests through legal procedures.

Table 9: The English translation of the first example in Table 3.

17

Question 2: A 21-year-old woman, referred to as Alice, developed a liking for a 25-year-old man, referred to as Bob, after chatting online. Bob saved the private photos sent by Alice. Later, Bob asked Alice to marry him, but she refused. Bob threatened to publish her photos, so Alice reluctantly agreed to marry him and completed the registration. Can Alice request to dissolve the marriage within one year from the date of marriage registration?

BELLE: According to the Marriage Law of the Peopleâ€™s Republic of China, after one year of marriage registration, both parties can apply for a divorce, but certain conditions need to be met. Therefore, Party A can apply for a divorce after one year of marriage registration.

ChatGLM: According to the provisions of the Chinese Marriage Law, a marital relationship exists from the beginning of the marriage and cannot be terminated without legal procedures. The reasons for the invalidity of marriage are as follows: (1) bigamy; (2) marriage void due to a disease that prohibits marriage; (3) marriage void due to coercion; (4) marriage void due to lack of emotional attachment from the beginning; (5) marriage void due to fraud. void marriages can be revoked according to the law. Party A can file a lawsuit to revoke the marriage within one year from the date of marriage registration with the local Peopleâ€™s Court.

Lawyer LLaMA: According to Article 1,052 of the Civil Code, if a marriage is contracted under coercion, the coerced party may request the court to revoke the marriage. Therefore, if Party A agreed to marry under coercion, Party A can request the court to revoke the marriage within one year from the date of marriage registration. However, it is necessary to further understand whether there was coercion, as well as the methods and means of coercion. Furthermore, according to Article 1,046 of the Civil Code, marriage should be based on the full voluntary will of both parties, and it is prohibited for either party to coerce the other or for any organization or individual to interfere. If Party A agreed to marry under full voluntary will, Party A cannot request the revocation of the marriage. In conclusion, it is necessary to further understand the specific circumstances of Party Aâ€™s agreement to marry and whether there was coercion. It is advised that Party A consults a professional lawyer to seek specialized legal assistance.

Table 10: The English translation of the second example in Table 3.

18

