Conditional Prompt Learning for Vision-Language Models

Kaiyang Zhou

Jingkang Yang

Chen Change Loy

Ziwei Liu

S-Lab, Nanyang Technological University, Singapore

{kaiyang.zhou, jingkang001, ccloy, ziwei.liu}@ntu.edu.sg

Abstract
With the rise of powerful pre-trained vision-language models like CLIP, it becomes essential to investigate ways to adapt these models to downstream datasets. A recently proposed method named Context Optimization (CoOp) introduces the concept of prompt learning—a recent trend in NLP—to the vision domain for adapting pre-trained visionlanguage models. Specifically, CoOp turns context words in a prompt into a set of learnable vectors and, with only a few labeled images for learning, can achieve huge improvements over intensively-tuned manual prompts. In our study we identify a critical problem of CoOp: the learned context is not generalizable to wider unseen classes within the same dataset, suggesting that CoOp overfits base classes observed during training. To address the problem, we propose Conditional Context Optimization (CoCoOp), which extends CoOp by further learning a lightweight neural network to generate for each image an input-conditional token (vector). Compared to CoOp’s static prompts, our dynamic prompts adapt to each instance and are thus less sensitive to class shift. Extensive experiments show that CoCoOp generalizes much better than CoOp to unseen classes, even showing promising transferability beyond a single dataset; and yields stronger domain generalization performance as well. Code is available at https://github.com/ KaiyangZhou/CoOp.
1. Introduction
Recent research in large-scale vision-language pretraining has achieved striking performance in zero-shot image recognition [13,24,33,40], demonstrating a potential in learning open-world visual concepts for such a paradigm. The key design lies in how visual concepts are modeled. In traditional supervised learning where labels are discretized, each category is associated with a randomly initialized weight vector that is learned to minimize the distance with images containing the same category. Such a learning
Corresponding author

method focuses on closed-set visual concepts, limiting the model to a pre-defined list of categories, and is unscalable when it comes to new categories unseen during training.
In contrast, for vision-language models1 like CLIP [40] and ALIGN [24], the classification weights are diametrically generated by a parameterized text encoder (e.g., a Transformer [48]) through prompting [34]. For instance, to differentiate pet images containing different breeds of dogs and cats, one can adopt a prompt template like “a photo of a {class}, a type of pet” as input to the text encoder, and as a result, class-specific weights for classification can be synthesized by filling in the “{class}” token with real class names. Compared to discrete labels, vision-language models’ source of supervision comes from natural language, which allows open-set visual concepts to be broadly explored and has been proven effective in learning transferable representations [24, 40].
With the rise of such powerful vision-language models, the community has recently started to investigate potential solutions to efficiently adapt these models to downstream datasets [14, 53, 56, 62]. To fit web-scale data, such as the 400 million pairs of images and texts used by CLIP, visionlanguage models are purposefully designed to have high capacity, entailing that the model size would be enormous, typically with hundreds of millions of parameters or even billions. Therefore, fine-tuning the entire model, as often adopted in deep learning research [18], is impractical and might even damage the well-learned representation space.
A safer approach is to tune a prompt by adding some context that is meaningful to a task, like “a type of pet” for the pet dataset mentioned above, which has been found effective in improving performance [40]. However, prompt engineering is extremely time-consuming and inefficient as it has to be based on trial and error, and does not guarantee an optimal prompt either. To automate prompt engineering, Zhou et al. [62] have recently explored the concept of prompt learning—a recent trend in NLP [15, 25, 30, 32, 44, 60]—for adapting pre-trained vision-language models. Their approach, Context Optimization (CoOp), turns con-
1We follow existing studies [13,24,33,40] to refer to CLIP-like models as vision-language models.

16816

Bas e <latexitsha1_base64="0ivCj/QEdXYHnnI7rZsyExeY+nE=">AAADQXicbVLPb9MwFHYyfowCYxs3uFh0k8olSnroJk6DXbgAQ6LbpCSqHMdZrdpxZL+0q6Ic+Gu4wt/AX8GfwA1x5YKbdVO67klxvve99z7bn5wUghvw/V+Ou3Hv/oOHm486j5883Xq2vbN7alSpKRtSJZQ+T4hhgudsCBwEOy80IzIR7CyZHC/qZ1OmDVf5F5gXLJbkIucZpwQsNdpxXuzvhVEiq2k9CuI9fJP0F0k0ThWYFvvBsiFownNsFzEj89jrdFoSvQW4rF+vSLXJdclW9S7paNoMNHB8DSNgl5Bk1bH6VNStvPk3rlSapXX1kc0wFcQYZuqmj8liXL2ltNSEzt/gg4F3OLCF6/l31sqbgdF21/f8JvA6CJagi5ZxYt3cilJFS8lyaETCwC8grogGTgWrO1FpWEHohFyw0MKcSGbiqjlvjfctk+JMafvlgBu2PVERacxcJrZTEhib27UFeVctLCE7jCueFyWwnF5tlJUCg8KLJ4FTrhkFMbeAUM3tWTEdE2sQ2IezsksiV+5QyVIA12q2yiZKTYAkpu5YB4Pbfq2D074XDDz/c7971Ft6uYleoleohwJ0gI7Qe3SChog6X51vznfnh/vT/e3+cf9etbrOcuY5Wgn3339dNAbl</latexit>

classes

.<latexitsha1_base64="CrDUidhbV+gYx4dWg1zM4wwFCGM=">AAADNnicjVLLbtQwFHXCqwToA5ZsLKaVhs0omUVhWTEbNogiMW2lJBrZjtOxxokj+2baUeQ/YQvfwK+wYYfY8gk46YAyHRZcydHxub7nOEemlRQGwvCb59+5e+/+g52HwaPHT3b39g+enhlVa8anTEmlLygxXIqST0GA5BeV5qSgkp/TxaTtny+5NkKVH2FV8bQgl6XIBSPgqNmBt3d0GCe0aJZ2FqWH+O9m3G6SeabA9Nh3jo0T4NfQWTeaZ7YBTUSJ3UdekZVNR0HQ0xy24Nq+3NDuk9seve5/eSXLTiEI/mg5spujeTNRE/W+sj2mp0hlzW3zxsWHmSTGcGPtbH8QjsKu8DaI1mCA1nXqItxNMsXqgpfQqcRRWEHaEA2CSW6DpDa8ImxBLnnsYEkKbtKmu4HFR47JcK60WyXgju1PNKQwZlVQd7IgMDe3ey35r15cQ/46bURZ1cBLdmOU1xKDwu07wJnQnIFcOUCYFu6umM2JJgzca9lwocXGPzRFLUFodbXJUqUWQKixgUswup3XNjgbj6LjUfhhPDgZrrPcQc/RCzREEXqFTtBbdIqmiHlL75P32fvif/W/+z/8nzdHfW898wxtlP/rN106BlU=</latexit>

..

Zero-shot <latexitsha1_base64="IJtCaraVVLlW6HHagghvMZyBOHc=">AAACOHicbVA9TwJBEN3DL8QvxNLmIjGxkdxRqCWJjaUmokQgZHeZkw27t5fdOYVc+Cu2+hv8J3Z2xtZf4AJXiPqSSV7em8nMPJZIYTEI3rzC0vLK6lpxvbSxubW9U96t3FidGg5NrqU2LUYtSBFDEwVKaCUGqGISbtnwfOrfPoCxQsfXOE6gq+h9LCLBKTqpV650EEbIouwOjD62A42TXrka1IIZ/L8kzEmV5Ljs7Xrbnb7mqYIYuaTWtsMgwW5GDQouYVLqpBYSyof0HtqOxlSB7Waz4yf+oVP6fqSNqxj9mfpzIqPK2rFirlNRHNjf3lT8z2unGJ11MxEnKULM54uiVPqo/WkSfl8Y4CjHjlBuhLvV5wNqKEeX18IWphZ+yFQqURj9uKgyrYdImZ2UXILh77z+kpt6LTypBVf1auMoz7JI9skBOSIhOSUNckEuSZNwMiJP5Jm8eK/eu/fhfc5bC14+s0cW4H19AzV1rS8=</latexit>

<latexit sha1_base64="+xVRXtBc7t53www2R1akeGF/K1k=">AAACSnicbVC7TgMxEPSFVwivACWNRUCiiu4ogBKJhjJIJCAdp2jP8SVW7PPJ3guKTvkBvoYWvoEf4DfoEA3OoyAJI9kezexqvRNnUlj0/U+vtLK6tr5R3qxsbe/s7lX3D1pW54bxJtNSm8cYLJci5U0UKPljZjioWPKHuH8z9h8G3Fih03scZjxS0E1FIhigk9rVkxAiGmY9jdq9OnHXWABjxAAk7QLyqN6u1vy6PwFdJsGM1MgMjfa+t/vU0SxXPEUmwdow8DOMCjAomOSjylNueQasD10eOpqC4jYqJuuM6KlTOjTRxp0U6UT921GAsnaoYlepAHt20RuL/3lhjslVVIg0y5GnbDooySVFTcfZ0I4wnKEcOgLMCPdXynpggKFLcG5KrOZ2KFQuURj9PK/GWvcRYjuquASDxbyWSeu8HlzU/bvz2vXZLMsyOSLH5IwE5JJck1vSIE3CyAt5JW/k3fvwvrxv72daWvJmPYdkDqXVX8LtsjE=</latexit>
[a]

[photo]

[of ]

[a]

[arrival

gate].

...<latexitsha1_base64="4TW2Q2kvoIwAfSidpPz6R0O1N8s=">AAADAHicjVFNj9MwEHXC1xJg6cKRi0W7UrlUSQ/AsVIvXBCLRHdXaqLKdpytVTuO7El3qygXfg03xJV/Ar8GJ1uhZMuBkWy9efPxxmNaSGEhDH95/r37Dx4+OnocPHn67Pj54OTFudWlYXzBtNTmkhLLpcj5AgRIflkYThSV/IJu5k38YsuNFTr/AruCJ4pc5SITjICjVoPfp6NlTFW1rVdRMsJ/nWnjxOtUg+2wHx27jIHfQKtcGZ7WFRgicuwueU12dTIJgk7PcQNu6je93l3yUKMT/Q+tUbxtGzjVNplm1VzP9aeiXg2G4SRsDR+CaA+GaG9nqxPvOE41KxXPgUli7TIKC0gqYkAwyesgLi0vCNuQK750MCeK26Rqx6vxqWNSnGnjTg64ZbsVFVHW7hR1mYrA2t6NNeS/YssSsvdJJfKiBJ6zW6GslBg0bn4Up8JwBnLnAGFGuFkxWxNDGLh/76lQ1XtDpUoJwujrPku13gChtg7cBqO7+zoE59NJ9HYSfp4OZ+P9Lo/QK/QajVGE3qEZ+oDO0AIxb+ZlnvYK/6v/zf/u/7hN9b19zUvUM//nHw/58a8=</latexit>

<latexit sha1_base64="oyFSkVR5pVK+BaFeHpURW7SEkEo=">AAACR3icbVBNTwIxEO3iF+IX6NFLlZhwIrsc1COJF4+YyEeybEi3dKGh3W7aWQ3ZcPbXeNXf4E/wV3gzHu0CBwEnaeflvZnMzAsTwQ247qdT2Nre2d0r7pcODo+OT8qV045RqaasTZVQuhcSwwSPWRs4CNZLNCMyFKwbTu5yvfvEtOEqfoRpwgJJRjGPOCVgqUH5wicB9pOxAmWziuyXE1Yes6EmIqgPylW37s4DbwJvCapoGa1BxTnuDxVNJYuBCmKM77kJBBnRwKlgs1I/NSwhdEJGzLcwJpKZIJvfMsNXlhniSGn7YsBz9m9HRqQxUxnaSmmXNOtaTv6n+SlEt0HG4yQFFtPFoCgVGBTOjcFDrhkFMbWAUM3trpiOiSYUrH0rU0K5ckMmUwFcq+dVNlRqAiQ0s5J10Fv3axN0GnXvuu4+NKrN2tLLIjpHl6iGPHSDmugetVAbUfSCXtEbenc+nC/n2/lZlBacZc8ZWomC8wtbS7EJ</latexit>
[a]

[photo]

[of ]

[a]

[cathedral].

<latexit sha1_base64="nJGHb+fSeYbFHtGpvQfsLkYuVT0=">AAADMnicjVLLbtQwFHXCq6RQWliysZhWGjajZBaFZaVuugGKxLSVkmjkOE7HGjuO7JuZjqL8R7fwDfwM7BBbPgInHVAyw4IrJTr3HN9zkisnheAGfP+b4967/+Dho53H3u6Tp3vP9g+eXxhVasomVAmlrxJimOA5mwAHwa4KzYhMBLtM5qeNfrlg2nCVf4JVwWJJrnOecUrAUtMDZ/foMIwSWS3qaRAf4r/NuGmiWarAdNh3lg0jYDfQRleapXUFmvAc25dYklUdjzyv4zlswE39uufdJbczOup/ZUWL1qGFszVs55KsOlUfitoqf/pNv/dsiakgxjBT19P9gT/y28LbIFiDAVrXuV3fXpQqWkqWQ+sSBn4BcUU0cCpY7UWlYQWhc3LNQgtzIpmJqza/xkeWSXGmtH1ywC3bnaiINGYlE3tSEpiZTa0h/6WFJWRv44rnRQksp3dBWSkwKNzcAZxyzSiIlQWEam6/FdMZ0YSCvSm9lET2/qGSpQCu1bLPJkrNgSSm9uwGg819bYOL8Sg4Hvkfx4OT4XqXO+gleoWGKEBv0Ak6Q+dogqijnVvns/PF/ep+d3+4P++Ous565gXqlfvrN8MdBL0=</latexit>
CoOp

<latexit sha1_base64="f1SUBf8cVkB8dE/1Q5lxwIU6Dy4=">AAADOXicbVJNb9NAEF2br2KgtOXIZUVaKVwiO4e04lTUCxegSKStZFvRer1JVtn1WrvjpJbl38IVfgO/hCM3xJU/wMYJECeMZOvNm5k3u0+b5IIb8P1vjnvn7r37D/Yeeo8eP9l/enB4dGVUoSkbUiWUvkmIYYJnbAgcBLvJNSMyEew6mV0s69dzpg1X2UcocxZLMsn4mFMClhodOkfHYZTIal6PgvgY/036yySapgrMBvvWsiHRms+JwBMCLO553sk/he4S3NYvW0qb5K7iRjWMgN1Cc6dKs7SuQBOeYfsTC1LWq13RvFFo4PQPbAaTcXWh3uf1Rr4t+I4tMBXEGGbqpo/JfFq9prTQhJav8OmgdzaoRwcdv+c3gXdBsAYdtI5L6+J+lCpaSJZBIx8Gfg5xRTRwKljtRYVhOaEzMmGhhRmRzMRVc7Aan1gmxWOl7ZcBbtjNiYpIY0qZ2E5JYGq2a0vyf7WwgPFZXPEsL4BldLVoXAgMCi+fAk65ZhREaQGhmtuzYjol1gmwD6a1JZGtO1SyEMC1WrTZRKkZkMTUnnUw2PZrF1z1e8Gg53/od867ay/30HP0AnVRgE7ROXqDLtEQUad0PjmfnS/uV/e7+8P9uWp1nfXMM9QK99dv+RAE2A==</latexit>
[v1

]

[v2]

...

[vM ] ...<latexitsha1_base64="4TW2Q2kvoIwAfSidpPz6R0O1N8s=">AAADAHicjVFNj9MwEHXC1xJg6cKRi0W7UrlUSQ/AsVIvXBCLRHdXaqLKdpytVTuO7El3qygXfg03xJV/Ar8GJ1uhZMuBkWy9efPxxmNaSGEhDH95/r37Dx4+OnocPHn67Pj54OTFudWlYXzBtNTmkhLLpcj5AgRIflkYThSV/IJu5k38YsuNFTr/AruCJ4pc5SITjICjVoPfp6NlTFW1rVdRMsJ/nWnjxOtUg+2wHx27jIHfQKtcGZ7WFRgicuwueU12dTIJgk7PcQNu6je93l3yUKMT/Q+tUbxtGzjVNplm1VzP9aeiXg2G4SRsDR+CaA+GaG9nqxPvOE41KxXPgUli7TIKC0gqYkAwyesgLi0vCNuQK750MCeK26Rqx6vxqWNSnGnjTg64ZbsVFVHW7hR1mYrA2t6NNeS/YssSsvdJJfKiBJ6zW6GslBg0bn4Up8JwBnLnAGFGuFkxWxNDGLh/76lQ1XtDpUoJwujrPku13gChtg7cBqO7+zoE59NJ9HYSfp4OZ+P9Lo/QK/QajVGE3qEZ+oDO0AIxb+ZlnvYK/6v/zf/u/7hN9b19zUvUM//nHw/58a8=</latexit>

[arrival

gate].

<latexit sha1_base64="w2uawpOynxeIAr2p+Zz3wz9wQgQ=">AAADNnicbVJNb9NAEF2br2KgH3DksiKtFC6WnUNacSrqhQvQSk1bybai9XrSrLLrtXbXSSPL/4Qr/Ab+ChduiCs/gY0TIE4YydabN/Nmdp82LTjTJgi+Oe69+w8ePtp57D15+mx3b//g+ZWWpaIwoJJLdZMSDZzlMDDMcLgpFBCRcrhOJ2eL+vUUlGYyvzTzAhJBbnM2YpQYSw0PnL3DKE5FNa2HYXKI/ya9RRKPM2n0GvvespGVjiFThCe+5x39k3cX4K5+3RqzTm6PW6tGsYE701yoUpDVlVGE5dj++IzM6+WueNpMaOD4D2yE6ag6kx+Lei3fHPgBZphyojXouukDUYyrt5SWitD5G3zc90/69XC/E/hBE3gbhCvQQas4txbuxpmkpYDcNOOjMChMUhFlGOVQe3GpoSB0Qm4hsjAnAnRSNQer8ZFlMjySyn65wQ27rqiI0HouUtsprO96s7Yg/1eLSjM6SSqWF6WBnC4XjUqOjcSLd4AzpoAaPreAUMXsWTEdE+uEsa+ltSUVrTtUouSGKTlrs6mUE0NSXXvWwXDTr21w1fPDvh9c9Dqn3ZWXO+gleoW6KETH6BS9Q+dogKgzdT45n50v7lf3u/vD/blsdZ2V5gVqhfvrN5lQA7A=</latexit>
[v1

]

[v2]

...

[vM ]

[cathedral].

<latexit sha1_base64="8143cVdce6SsJFqV+hM9gRzsI3Q=">AAADNHicjVJNb9MwGHbC1whsbHDkYtFNKpcq6WFwnNQLF2BIdJuURJXjuKtVJ47sN+2qyH+EK/wG/gsSN8SV34CTBZS0HHilWI+fx+/zxK+cFIJr8P1vjnvn7r37D/Yeeo8e7x88OTx6eqFlqSibUimkukqIZoLnbAocBLsqFCNZIthlspzU+uWKKc1l/hE2BYszcp3zOacELDU7cvZPjsMoyaqVmQXxMf67GdebaJFK0B32rWXDCNgNNNGVYqmpQBGeY7uINdmYeOR5Hc9hDW7My553l9zN6Kj/lRWtGocGLlrY9CXzaiIn8n1hrPaH2XZ8x9aYCqI108bMDgf+yG8K74KgBQPU1rkd4EGUSlpmLIfGJQz8AuKKKOBUMONFpWYFoUtyzUILc5IxHVdNvsEnlknxXCr75YAbtttRkUzrTZbYkxmBhd7WavJfWljC/HVc8bwogeX0NmheCgwS168Ap1wxCmJjAaGK23/FdEEUoWDfSi8lyXp3qLJSAFdy3WcTKZdAEm08O8Fge1674GI8Ck5H/ofx4GzYznIPPUcv0BAF6BU6Q2/QOZoi6oDzyfnsfHG/ut/dH+7P26Ou0/Y8Q71yf/0GApsFgw==</latexit>
CoCoOp

<latexit sha1_base64="zX3nn3Sqn+w/qDQYjs0d4xdW0sQ=">AAADKHicbVLLjtMwFHXCawgwD1iysWgrlU2VdNEZsRo0GzbAINGZkZKochy3sWrHkX3TThXlF9jCN/A17NBs+RKcTIF2ypWSnHvuvcfOsZNCcAO+f+O49+4/ePho77H35Omz/YPDo+cXRpWasjFVQumrhBgmeM7GwEGwq0IzIhPBLpP5WVO/XDBtuMo/w6pgsSSznE85JWCpyZHj9LphlMhqUU+CuIv/JsMmibJUgdlg31s2BE14ju1LLMkqHnjeP4V+A67r11tKm+Su4kY1JFrzBRF4RoA1wr1utGj7W5j9gRGwa0im1Zn6WNQbefttPak0S+vqA1tiKogxzNRtH5NFVr2ltNSErt7g49HgZFRPDjv+wG8D74JgDTpoHefWtP0oVbSULIdWPgz8AuKKaOBUsNqLSsMKQudkxkILcyKZiat2YzXuWSbFU6XtkwNu2c2JikhjVjKxnZJAZu7WGvJ/tbCE6Ulc8bwogeX0dqFpKTAo3Jw8TrlmFMTKAkI1t3vFNCPWCbD3Y2uVRG79QyVLAVyr5TabKDUHkpjasw4Gd/3aBRfDQTAa+J+GndP+2ss99BK9Qn0UoGN0it6hczRG1MmcL85X55v73f3h/nRvbltdZz3zAm2F++s3r/P8tA==</latexit>
[v1

(x)]

[v2(x)]

...

[vM (x)] ...<latexitsha1_base64="4TW2Q2kvoIwAfSidpPz6R0O1N8s=">AAADAHicjVFNj9MwEHXC1xJg6cKRi0W7UrlUSQ/AsVIvXBCLRHdXaqLKdpytVTuO7El3qygXfg03xJV/Ar8GJ1uhZMuBkWy9efPxxmNaSGEhDH95/r37Dx4+OnocPHn67Pj54OTFudWlYXzBtNTmkhLLpcj5AgRIflkYThSV/IJu5k38YsuNFTr/AruCJ4pc5SITjICjVoPfp6NlTFW1rVdRMsJ/nWnjxOtUg+2wHx27jIHfQKtcGZ7WFRgicuwueU12dTIJgk7PcQNu6je93l3yUKMT/Q+tUbxtGzjVNplm1VzP9aeiXg2G4SRsDR+CaA+GaG9nqxPvOE41KxXPgUli7TIKC0gqYkAwyesgLi0vCNuQK750MCeK26Rqx6vxqWNSnGnjTg64ZbsVFVHW7hR1mYrA2t6NNeS/YssSsvdJJfKiBJ6zW6GslBg0bn4Up8JwBnLnAGFGuFkxWxNDGLh/76lQ1XtDpUoJwujrPku13gChtg7cBqO7+zoE59NJ9HYSfp4OZ+P9Lo/QK/QajVGE3qEZ+oDO0AIxb+ZlnvYK/6v/zf/u/7hN9b19zUvUM//nHw/58a8=</latexit>

[arrival

gate].

<latexit sha1_base64="Sds3QJMIKgV/HNcJ+p5gIs1s934=">AAADJHicbVJNj9MwEHXC11Jg6cKRi0VbqVyqpIfuitOivXABFonurpREleNMNlbtOLKddqsof4Ar/AZ+DTfEgQs/BeFkC7RbRorz5s34efySuOBMG8/74bi3bt+5e2/vfufBw0f7j7sHT860LBWFKZVcqouYaOAsh6lhhsNFoYCImMN5PD9p6ucLUJrJ/INZFRAJcpmzlFFiLDXr/hr0gzAW1aKe+VEf/03GTRJmiTR6g31j2cAownJsF74kq2jU6fxTGDbgqn6xpbRJ7ipuVAM7VAaJIrxRHfTDRdvcwuwPDA1cmTitTuS7ot7I23frR6Ugqau3sMSUE61B120fiCKrXlFaKkJXL/HhZHQ0qWfdnjfy2sC7wF+DHlrH6ezA2Q8TSUsBuWnlA98rTFQRZRjlUHfCUkNB6JxcQmBhTgToqGoHq/HAMglOpbJPbnDLbu6oiNB6JWLbKawX+matIf9XC0qTHkUVy4vSQE6vD0pLjo3EzVfHCVNADV9ZQKhidlZMM2KdMPbf2DolFlt3qETJDVNyuc3GUs4NiXXdsQ76N/3aBWfjkT8Zee/HvePh2ss99Aw9R0Pko0N0jF6jUzRF1Emcj84n57P7xf3qfnO/X7e6znrPU7QV7s/f+gr8gQ==</latexit>
[v1

(x)]

[v2(x)]

...

[vM (x)]

[cathedral].

<latexit sha1_base64="YrdloPpTXXetAZ2qlWj/z7dT0lQ=">AAADOHicbVLLbtNAFB2bVwlQGliyGZFWCpvIziKtWLXqhg1QJNJWiq1oPLlJRpnxWDPXecjyr7CFb+BP2LFDbPkCJm5ATtMr2T73nPsYH02SSWExCH54/r37Dx4+2nvcePL02f7zg+aLS6tzw6HPtdTmOmEWpEihjwIlXGcGmEokXCWz87V+NQdjhU4/4yqDWLFJKsaCM3TUsOk1jw4HUaKKeTkM40P6P+muk2g60mhr7HvHDtAwkVL3kgu2ijuNRm1Eew2W5ZutUXVyd2RNvWt0NK8aKjj9ByOEJSbj4lx/zMpaXn0rVwoDo7L4AAvKJbMWbFnVgcqmxRnnuWF89ZYe9zonPSecGSPmTNIJQxgetIJOUAXdBeEGtMgmLpyJ+9FI81xBitWyQRhkGBfMoOASykaUW8gYn7EJDBxMmQIbF9UxS3rkmBEda+OeFGnF1jsKpqxdqcRVKoZTe1tbk3dpgxzHJ3Eh0ixHSPnNonEuKWq6vgl0JAxwlCsHGDfCnZXyKXO+oLsvW1sStfUPhcolCqMX22yi9QxZYsuGczC87dcuuOx2wl4n+NRtnbY3Xu6RV+Q1aZOQHJNT8o5ckD7h3tL74n31vvnf/Z/+L//3TanvbXpekq3w//wF8BACsw==</latexit>
Arrival

gate

Cat h ed r al <latexitsha1_base64="X9DUCorVWpoVUM8/d/6DB0Zwkhw=">AAADNXicbVJNb9MwGHbC1wiwDzhysegmlUuU9NBNnIZ64QIMiW6TkqhynLerVTuObKddFeWXcIXfwG/hwA1x5S/gZAWl614pzvM+76cfOS040yYIfjjuvfsPHj7aeew9efpsd2//4Pm5lqWiMKaSS3WZEg2c5TA2zHC4LBQQkXK4SOejJn6xAKWZzD+bVQGJIFc5mzJKjKUmB87u0WEUp6Ja1JMwOcT/nUHjxLNMGt1h31s2MoqwHNuDL8kq8T2v06LfgOv69UarLrndshO9q3W8aAtaOPsHYwPXJp1WI/mxqDt++29VqRRkdfUBlphyojXous0DUcyqt5SWitDVG3w89E+GNjAiZgaZInyy3wv8oDW8DcI16KG1nTUKxpmkpYDctJOiMChMUhFlGOVQe3GpoSB0Tq4gsjAnAnRStTvW+MgyGZ5KZb/c4JbtVlREaL0Sqc0UdkV9O9aQd8Wi0kxPkorlRWkgpzeDpiXHRuLmGeCMKaCGrywgVDG7K6YzYkUx9rFsTEnFxh0qUXLDlFxusqmUc0NSXXtWwfC2XtvgfOCHQz/4NOid9tda7qCX6BXqoxAdo1P0Dp2hMaJO6Xxxvjrf3O/uT/eX+/sm1XXWNS/Qhrl//gKV5AGL</latexit>

<latexit sha1_base64="9fR6HmABMleNcUoWa6oyKg/n/6s=">AAACPHicbZDLTgIxFIY7eEO8gSZu3DQSE1ZkBhO8rDBuXGIilwQI6ZQCDe100p7RkJGXcavP4Hu4d2fcurbALAT8kyZ//nNOTs/nh4IbcN0PJ7W2vrG5ld7O7Ozu7R9kc4d1oyJNWY0qoXTTJ4YJHrAacBCsGWpGpC9Ywx/dTuuNR6YNV8EDjEPWkWQQ8D6nBGzUzR63mQyH8Q2lkSZ0fI3LV8Xz8qSbzbtFdya8arzE5FGiajfn7Ld7ikaSBUAFMabluSF0YqKBU8EmmXZkWEjoiAxYy9qASGY68eyACT6zSQ/3lbYvADxL/07ERBozlr7tlASGZrk2Df+rtSLoX3ZiHoQRsIDOF/UjgUHhKQ3c45pREGNrCNXc/hXTIbEkwDJb2OLLhRtiGQngWj0tpr5SIyC+mWQsQW+Z16qpl4peuejel/KVQsIyjU7QKSogD12gCrpDVVRDFD2jF/SK3px359P5cr7nrSknmTlCC3J+fgEWTa10</latexit>
Accuracy:

69.36

<latexit sha1_base64="mCP733lRKW729yXdlkP1ECjR4Hs=">AAADS3icjVLBbtNAEF27FEqA0sKRy4qkUrhYdg6l4lTUCxegSKStZFvRer2pV9n1WrvjpJblL+BruMI38AF8BzfEgY0bkJNwYKS13ryZebN+2qQQ3IDvf3fcnTu7d+/t3e89ePho//HB4ZMLo0pN2ZgqofRVQgwTPGdj4CDYVaEZkYlgl8nsbFm/nDNtuMo/QlWwWJLrnE85JWCpyaEzOBqEUSLreTMJ4gH+m4yWSZSlCkyHfWvZMAJ2A+3qWrO0qUETnmP7EQtSNbHX63U0h0tw07xY0+6S2zs61f/aFc1bhRZmf2A7mEzrM/W+aDr5puA7tsBUEGOYaWxfxGSR1a8pLTWh1St84nvHfjM56Pue3wbeBsEK9NEqzq2v+1GqaClZDq16GPgFxDXRwKlgTS8qDSsInZFrFlqYE8lMXLf3avCRZVI8VdqeHHDLdidqIo2pZGI7JYHMbNaW5L9qYQnTk7jmeVECy+ntomkpMCi8fBw45ZpREJUFhGpu74ppRqwTYJ/Q2pZErv1DLUsBXKvFOpsoNQOSmKZnHQw2/doGFyMvOPb8D6P+6XDl5R56hp6jIQrQS3SK3qBzNEbU+eR8dr44X91v7g/3p/vrttV1VjNP0Vrs7P4GptUMcA==</latexit>
Accuracy:

80.60

<latexit sha1_base64="kASuoAz9VVVabPM2RlbBZuw3XFo=">AAADS3icjVLBbtNAEF27FIqB0sKRy4qkUrhEdlQR4FTUCxegSKStZFvRer1uVtn1WrvjpJHlL+BruMI38AF8BzfEgY0bkJNwYKS13ryZebN+2qQQ3IDvf3fcnVu7t+/s3fXu3X+w//Dg8NG5UaWmbESVUPoyIYYJnrMRcBDsstCMyESwi2R6uqxfzJg2XOUfYVGwWJKrnGecErDU+NDpHnXDKJHVrB4HcRf/TQbLJJqkCkyLfWvZMAJ2Dc3qSrO0rkATnmP7EXOyqOO+57U0e0twXT9b026T2zta1f/aFc0ahQZO/sBmMMmqU/W+qFv5puA7NsdUEGOYqW1fxGQxqV5TWmpCF6/w8GV/eFyPDzp+328Cb4NgBTpoFWfW1/0oVbSULIdGPQz8AuKKaOBUsNqLSsMKQqfkioUW5kQyE1fNvWp8ZJkUZ0rbkwNu2PZERaQxC5nYTklgYjZrS/JftbCE7EVc8bwogeX0ZlFWCgwKLx8HTrlmFMTCAkI1t3fFdEKsE2Cf0NqWRK79QyVLAVyr+TqbKDUFkpjasw4Gm35tg/NBP3je9z8MOie9lZd76Al6inooQEN0gt6gMzRC1PnkfHa+OF/db+4P96f766bVdVYzj9Fa7Oz+Br3HDH0=</latexit>
Accuracy:

79.74

(a) Both CoOp and CoCoOp work well on the base classes observed during training and beat manual prompts by a significant margin.

New <latexitsha1_base64="tQWd+dbC3pYqVRjIn2dwUiR7CO4=">AAADQHicbVJNb9QwEHXCV1mgtHBCXCy2lZZLlOxhW3Eq6oULUCS2rbSJVo7jbay148ie7DaKIvFruMJv4F/wD7ghrpzwpluUdDtSnDdvZp7tJ8e54AZ8/6fj3rl77/6DrYe9R4+fbD/d2X12alShKRtTJZQ+j4lhgmdsDBwEO881IzIW7CyeH6/qZwumDVfZZyhzFklykfEZpwQsNd11XuzvTcJYVot6GkR7+H8yXCVhmigwLfa9ZSegCc+wXcSSlJHX67UkBitwWb/uSLXJTclW9TbpcNEMNDC9hiGwS4hn1bH6mNetvPk3rlSaJXX1gS0xFcQYZuqmj8k8rd5SWmhCyzf4YOQdjmzher7dP93p+57fBN4EwRr00TpOrJnbYaJoIVkGjcgk8HOIKqKBU8HqXlgYlhM6JxdsYmFGJDNR1Ry3xvuWSfBMaftlgBu2PVERaUwpY9spCaTmZm1F3labFDA7jCqe5QWwjF5tNCsEBoVXLwInXDMKorSAUM3tWTFNifUH7Lvp7BLLzh0qWQjgWi27bKzUHEhs6p51MLjp1yY4HXrByPM/DftHg7WXW+gleoUGKEAH6Ai9QydojKjzxfnqfHO+uz/cX+5v989Vq+usZ56jTrh//wFJJgaK</latexit>

classes

Zero-shot <latexitsha1_base64="IJtCaraVVLlW6HHagghvMZyBOHc=">AAACOHicbVA9TwJBEN3DL8QvxNLmIjGxkdxRqCWJjaUmokQgZHeZkw27t5fdOYVc+Cu2+hv8J3Z2xtZf4AJXiPqSSV7em8nMPJZIYTEI3rzC0vLK6lpxvbSxubW9U96t3FidGg5NrqU2LUYtSBFDEwVKaCUGqGISbtnwfOrfPoCxQsfXOE6gq+h9LCLBKTqpV650EEbIouwOjD62A42TXrka1IIZ/L8kzEmV5Ljs7Xrbnb7mqYIYuaTWtsMgwW5GDQouYVLqpBYSyof0HtqOxlSB7Waz4yf+oVP6fqSNqxj9mfpzIqPK2rFirlNRHNjf3lT8z2unGJ11MxEnKULM54uiVPqo/WkSfl8Y4CjHjlBuhLvV5wNqKEeX18IWphZ+yFQqURj9uKgyrYdImZ2UXILh77z+kpt6LTypBVf1auMoz7JI9skBOSIhOSUNckEuSZNwMiJP5Jm8eK/eu/fhfc5bC14+s0cW4H19AzV1rS8=</latexit>

<latexit sha1_base64="nJGHb+fSeYbFHtGpvQfsLkYuVT0=">AAADMnicjVLLbtQwFHXCq6RQWliysZhWGjajZBaFZaVuugGKxLSVkmjkOE7HGjuO7JuZjqL8R7fwDfwM7BBbPgInHVAyw4IrJTr3HN9zkisnheAGfP+b4967/+Dho53H3u6Tp3vP9g+eXxhVasomVAmlrxJimOA5mwAHwa4KzYhMBLtM5qeNfrlg2nCVf4JVwWJJrnOecUrAUtMDZ/foMIwSWS3qaRAf4r/NuGmiWarAdNh3lg0jYDfQRleapXUFmvAc25dYklUdjzyv4zlswE39uufdJbczOup/ZUWL1qGFszVs55KsOlUfitoqf/pNv/dsiakgxjBT19P9gT/y28LbIFiDAVrXuV3fXpQqWkqWQ+sSBn4BcUU0cCpY7UWlYQWhc3LNQgtzIpmJqza/xkeWSXGmtH1ywC3bnaiINGYlE3tSEpiZTa0h/6WFJWRv44rnRQksp3dBWSkwKNzcAZxyzSiIlQWEam6/FdMZ0YSCvSm9lET2/qGSpQCu1bLPJkrNgSSm9uwGg819bYOL8Sg4Hvkfx4OT4XqXO+gleoWGKEBv0Ak6Q+dogqijnVvns/PF/ep+d3+4P++Ous565gXqlfvrN8MdBL0=</latexit>
CoOp

<latexit sha1_base64="8143cVdce6SsJFqV+hM9gRzsI3Q=">AAADNHicjVJNb9MwGHbC1whsbHDkYtFNKpcq6WFwnNQLF2BIdJuURJXjuKtVJ47sN+2qyH+EK/wG/gsSN8SV34CTBZS0HHilWI+fx+/zxK+cFIJr8P1vjnvn7r37D/Yeeo8e7x88OTx6eqFlqSibUimkukqIZoLnbAocBLsqFCNZIthlspzU+uWKKc1l/hE2BYszcp3zOacELDU7cvZPjsMoyaqVmQXxMf67GdebaJFK0B32rWXDCNgNNNGVYqmpQBGeY7uINdmYeOR5Hc9hDW7My553l9zN6Kj/lRWtGocGLlrY9CXzaiIn8n1hrPaH2XZ8x9aYCqI108bMDgf+yG8K74KgBQPU1rkd4EGUSlpmLIfGJQz8AuKKKOBUMONFpWYFoUtyzUILc5IxHVdNvsEnlknxXCr75YAbtttRkUzrTZbYkxmBhd7WavJfWljC/HVc8bwogeX0NmheCgwS168Ap1wxCmJjAaGK23/FdEEUoWDfSi8lyXp3qLJSAFdy3WcTKZdAEm08O8Fge1674GI8Ck5H/ofx4GzYznIPPUcv0BAF6BU6Q2/QOZoi6oDzyfnsfHG/ut/dH+7P26Ou0/Y8Q71yf/0GApsFgw==</latexit>
CoCoOp

W i n d <latexitsha1_base64="RkBZMghrkxbWYtXoTL4CTyYAvLU=">AAADNXicbVLLbtNAFB2bVzHQByzZjEgrhU1kZ5FWrIq6YQMUiTSVbCsaTybNKDMea+Y6qWX5S9jCN/AtLNghtvwCYzcgu+mVbJ97zn2MjybJBDfg+z8c9979Bw8f7Tz2njx9tru3f/D8wqhcUzamSih9mRDDBE/ZGDgIdplpRmQi2CRZntX6ZMW04Sr9DEXGYkmuUj7nlIClpgfO7tFhGCWyXFXTID7E/5NhnUSLmQLTYt9bNgRNeIrtS6xJEQ88rzWiX4Pr6nVnVJvcHtlS7xodrZqGBi7+wQjYNSTz8kx9zKpW3nwbV0rNZlX5ga0xFcQYZqqmjslsUb6lNNeEFm/w8WhwMrLChKczPCdaTvd7/sBvAm+DYAN6aBPntYPRTNFcshSaTWHgZxCXRAOnglVelBuWEbokVyy0MCWSmbhszljhI8vYxUrbJwXcsO2OkkhjCpnYSklgYW5rNXmXFuYwP4lLnmY5sJTeLJrnAoPC9TXAM64ZBVFYQKjm9qyYLog1Bexl6WxJZOcfSpkL4Fqtu2yi1BJIYirPOhjc9msbXAwHwWjgfxr2TvsbL3fQS/QK9VGAjtEpeofO0RhRJ3e+OF+db+5396f7y/19U+o6m54XqBPun79BNQFb</latexit>

farm

.<latexitsha1_base64="CrDUidhbV+gYx4dWg1zM4wwFCGM=">AAADNnicjVLLbtQwFHXCqwToA5ZsLKaVhs0omUVhWTEbNogiMW2lJBrZjtOxxokj+2baUeQ/YQvfwK+wYYfY8gk46YAyHRZcydHxub7nOEemlRQGwvCb59+5e+/+g52HwaPHT3b39g+enhlVa8anTEmlLygxXIqST0GA5BeV5qSgkp/TxaTtny+5NkKVH2FV8bQgl6XIBSPgqNmBt3d0GCe0aJZ2FqWH+O9m3G6SeabA9Nh3jo0T4NfQWTeaZ7YBTUSJ3UdekZVNR0HQ0xy24Nq+3NDuk9seve5/eSXLTiEI/mg5spujeTNRE/W+sj2mp0hlzW3zxsWHmSTGcGPtbH8QjsKu8DaI1mCA1nXqItxNMsXqgpfQqcRRWEHaEA2CSW6DpDa8ImxBLnnsYEkKbtKmu4HFR47JcK60WyXgju1PNKQwZlVQd7IgMDe3ey35r15cQ/46bURZ1cBLdmOU1xKDwu07wJnQnIFcOUCYFu6umM2JJgzca9lwocXGPzRFLUFodbXJUqUWQKixgUswup3XNjgbj6LjUfhhPDgZrrPcQc/RCzREEXqFTtBbdIqmiHlL75P32fvif/W/+z/8nzdHfW898wxtlP/rN106BlU=</latexit>

..

<latexit sha1_base64="9z/5HEKimh9eQz1kLtmR+HpiKRA=">AAACR3icbVC7TgMxEPSFd3glUNIYIiSq6I4CKJFoKINEINJxivYcH7Hix8neA0Wn1HwNLXwDn8BX0CFKnJCCEEayPZrd1XgnzaVwGIbvQWVhcWl5ZXWtur6xubVdq+/cOFNYxtvMSGM7KTguheZtFCh5J7ccVCr5bTq4GNdvH7h1wuhrHOY8UXCvRSYYoJe6tf0YEhrnfYPGvybz11h4FLpHM7AqaXZrjbAZTkDnSTQlDTJFq1sPtu56hhWKa2QSnIujMMekBIuCST6q3hWO58AGcM9jTzUo7pJyssuIHnrFWxvrj0Y6UX9PlKCcG6rUdyrAvvtbG4v/1eICs7OkFDovkGv2Y5QVkqKh42BoT1jOUA49AWaF/ytlfbDA0Mc345KqmR1KVUgU1jzOqqkxA4TUjao+wehvXvPk5rgZnTTDq+PG+dE0y1WyRw7IEYnIKTknl6RF2oSRJ/JMXshr8BZ8BJ/B109rJZjO7JIZVIJvBjyw2Q==</latexit>
[a]

[photo]

[of ]

[a]

[wind

farm].

...<latexitsha1_base64="4TW2Q2kvoIwAfSidpPz6R0O1N8s=">AAADAHicjVFNj9MwEHXC1xJg6cKRi0W7UrlUSQ/AsVIvXBCLRHdXaqLKdpytVTuO7El3qygXfg03xJV/Ar8GJ1uhZMuBkWy9efPxxmNaSGEhDH95/r37Dx4+OnocPHn67Pj54OTFudWlYXzBtNTmkhLLpcj5AgRIflkYThSV/IJu5k38YsuNFTr/AruCJ4pc5SITjICjVoPfp6NlTFW1rVdRMsJ/nWnjxOtUg+2wHx27jIHfQKtcGZ7WFRgicuwueU12dTIJgk7PcQNu6je93l3yUKMT/Q+tUbxtGzjVNplm1VzP9aeiXg2G4SRsDR+CaA+GaG9nqxPvOE41KxXPgUli7TIKC0gqYkAwyesgLi0vCNuQK750MCeK26Rqx6vxqWNSnGnjTg64ZbsVFVHW7hR1mYrA2t6NNeS/YssSsvdJJfKiBJ6zW6GslBg0bn4Up8JwBnLnAGFGuFkxWxNDGLh/76lQ1XtDpUoJwujrPku13gChtg7cBqO7+zoE59NJ9HYSfp4OZ+P9Lo/QK/QajVGE3qEZ+oDO0AIxb+ZlnvYK/6v/zf/u/7hN9b19zUvUM//nHw/58a8=</latexit>

<latexit sha1_base64="J6UHTDMJ1GOsD18m3zfvpkS9P0s=">AAACS3icbVBLSgNBEO2J//hLdOmmMQiuwowLdRlw41LBmMA4hJpOj2nSn6G7xhCGnMDTuNUzeADP4U5c2IlZGPVBVz1eVVFdL82lcBiGb0FlaXlldW19o7q5tb2zW6vv3TpTWMbbzEhjuyk4LoXmbRQoeTe3HFQqeScdXkzrnQdunTD6Bsc5TxTca5EJBuilXu0ohoTG+cCg8dlkPkwFtCA09UGOYJw0e7VG2AxnoH9JNCcNMsdVrx7s3PUNKxTXyCQ4F0dhjkkJFgWTfFK9KxzPgQ3hnseealDcJeXsngk98kqfZsb6p5HO1J8TJSjnxir1nQpw4H7XpuJ/tbjA7Dwphc4L5Jp9L8oKSdHQqTm0LyxnKMeeALPC/5WyAVhg6C1c2JKqhRtKVUgU1owW1dSYIULqJlXvYPTbr7/k9qQZnTbD65NG63ju5To5IIfkmETkjLTIJbkibcLII3kiz+QleA3eg4/g87u1Esxn9skCKitf3saywA==</latexit>
[a]

[photo]

[of ]

[a]

[train

railway].

<latexit sha1_base64="bYymbds63ROIjnQFB2XBSZyp6V0=">AAADNnicbVJNb9NAEF2brxKgH3DksiKtFC6WnUNacSrqhQvQSk1bybai9XrTrLLrtXbHSSPL/4Qr/Ab+ChduiCs/gbUTIE4YydabNzNvdp82yQU34PvfHPfe/QcPH+087jx5+mx3b//g+ZVRhaZsSJVQ+iYhhgmesSFwEOwm14zIRLDrZHpW169nTBuusktY5CyW5DbjY04JWGp04OwdhlEiy1k1CuJD/Dfp10k0SRWYNfa9ZcM5z1I8JlrGXqdz9G+8V4O76nVLZp3cllurhhGwO2guVGqWViVowjNsf2JOFtVyVzRrFBo4+QObwWRcnqmPebWWbwp+YHNMBTGGmarpYzKflG8pLTShizf4eOCdDKrRftf3/CbwNghWoItWcW4t3I1SRQvJMmjkw8DPIS6JBk4FqzpRYVhO6JTcstDCjEhm4rI5WIWPLGPdVNp+GeCGXZ8oiTRmIRPbKQlMzGatJv9XCwsYn8Qlz/ICWEaXi8aFwKBw/Q5wyjWjIBYWEKq5PSumE2KdAPtaWlsS2bpDKQsBXKt5m02UmgJJTNWxDgabfm2Dq74XDDz/ot897a283EEv0SvUQwE6RqfoHTpHQ0SdmfPJ+ex8cb+6390f7s9lq+usZl6gVri/fgMcsQOA</latexit>
[v1

]

[v2]

...

[vM ]

[wind

farm].

...<latexitsha1_base64="4TW2Q2kvoIwAfSidpPz6R0O1N8s=">AAADAHicjVFNj9MwEHXC1xJg6cKRi0W7UrlUSQ/AsVIvXBCLRHdXaqLKdpytVTuO7El3qygXfg03xJV/Ar8GJ1uhZMuBkWy9efPxxmNaSGEhDH95/r37Dx4+OnocPHn67Pj54OTFudWlYXzBtNTmkhLLpcj5AgRIflkYThSV/IJu5k38YsuNFTr/AruCJ4pc5SITjICjVoPfp6NlTFW1rVdRMsJ/nWnjxOtUg+2wHx27jIHfQKtcGZ7WFRgicuwueU12dTIJgk7PcQNu6je93l3yUKMT/Q+tUbxtGzjVNplm1VzP9aeiXg2G4SRsDR+CaA+GaG9nqxPvOE41KxXPgUli7TIKC0gqYkAwyesgLi0vCNuQK750MCeK26Rqx6vxqWNSnGnjTg64ZbsVFVHW7hR1mYrA2t6NNeS/YssSsvdJJfKiBJ6zW6GslBg0bn4Up8JwBnLnAGFGuFkxWxNDGLh/76lQ1XtDpUoJwujrPku13gChtg7cBqO7+zoE59NJ9HYSfp4OZ+P9Lo/QK/QajVGE3qEZ+oDO0AIxb+ZlnvYK/6v/zf/u/7hN9b19zUvUM//nHw/58a8=</latexit>

<latexit sha1_base64="CXyZvQbzqGgRKR8uT6+QZOyCWjs=">AAADOnicbVJNj9MwEHXC1xJgP9gjF4vuSuVSJT10V5wW7YULsEh0d6UkqhzX3Vq148ietBui/Beu8Bv4I1y5Ia78AJy0QNMyUqI3b2be2E9OMsEN+P43x71z9979BzsPvUePn+zu7R88vTQq15QNqRJKXyfEMMFTNgQOgl1nmhGZCHaVzM7r+tWcacNV+gGKjMWS3KR8wikBS40OnMOjMEpkOa9GQXyE/yb9OommYwVmjX1j2RA04Sm2P7EgRdzzvON/Et0a3FYvWlLr5LbkWjWMgN1Cc6lSs3FVtlZVy13RvFFo4PQPbAaTSXmu3mXVWr4p+JYtMBXEGGaqpo/JbFq+ojTXhBYv8cmgdzqoRvsdv+c3gbdBsAIdtIoLa+NuNFY0lyyFRj4M/AzikmjgVLDKi3LDMkJn5IaFFqZEMhOXzcEqfGyZMZ4obb8UcMOuT5REGlPIxHZKAlOzWavJ/9XCHCanccnTLAeW0uWiSS4wKFy/BTzmmlEQhQWEam7PiumUWCfAvpjWlkS27lDKXADXatFmE6VmQBJTedbBYNOvbXDZ7wWDnv++3znrrrzcQc/Qc9RFATpBZ+g1ukBDRJ2Pzifns/PF/ep+d3+4P5etrrOaOUStcH/9BowyBWc=</latexit>
[v1

]

[v2]

...

[vM ]

[train

railway].

<latexit sha1_base64="knM3erqhsh6bC2DEtwwf35p1uZE=">AAADJHicbVJNb9MwGHbC1ygwOjhysWgrlUuU9NBNnIZ24QIMiW6TkqhyHHe1aseR/aZdFOUPcIXfwK/hhjhw4acgnKxAu/JKdp73eV8/th8nyQU34Ps/HPfW7Tt37+3d7zx4+Gj/cffgyZlRhaZsQpVQ+iIhhgmesQlwEOwi14zIRLDzZHHS1M+XTBuusg9Q5iyW5DLjM04JWGra/TXoh1Eiq2U9DeI+/puMmiSapwrMBvvGsiFowjNsJ7EiZex1Ov8Uhg24ql9sKW2Su4ob1XDFsxTPiJaN6qAfLdvmFs7/wAjYFSSz6kS9y+uNvP22flSapXX1lq0wFcQYZuq2j8l8Xr2itNCEli/x4dg7GtfTbs/3/DbwLgjWoIfWcTo9cPajVNFCsgxa+TDwc4grooFTwepOVBiWE7oglyy0MCOSmbhqD1bjgWXsDZW2IwPcspsrKiKNKWViOyWBublZa8j/1cICZkdxxbO8AJbR641mhcCgcPPqOOWaURClBYRqbs+K6ZxYJ8D+G1u7JHLrDpUsBHCtVttsotQCSGLqjnUwuOnXLjgbecHY89+PesfDtZd76Bl6joYoQIfoGL1Gp2iCqJM6H51Pzmf3i/vV/eZ+v251nfWap2gr3J+/AZGr/FE=</latexit>
[v1

(x)]

[v2(x)]

...

[vM (x)] ...<latexitsha1_base64="4TW2Q2kvoIwAfSidpPz6R0O1N8s=">AAADAHicjVFNj9MwEHXC1xJg6cKRi0W7UrlUSQ/AsVIvXBCLRHdXaqLKdpytVTuO7El3qygXfg03xJV/Ar8GJ1uhZMuBkWy9efPxxmNaSGEhDH95/r37Dx4+OnocPHn67Pj54OTFudWlYXzBtNTmkhLLpcj5AgRIflkYThSV/IJu5k38YsuNFTr/AruCJ4pc5SITjICjVoPfp6NlTFW1rVdRMsJ/nWnjxOtUg+2wHx27jIHfQKtcGZ7WFRgicuwueU12dTIJgk7PcQNu6je93l3yUKMT/Q+tUbxtGzjVNplm1VzP9aeiXg2G4SRsDR+CaA+GaG9nqxPvOE41KxXPgUli7TIKC0gqYkAwyesgLi0vCNuQK750MCeK26Rqx6vxqWNSnGnjTg64ZbsVFVHW7hR1mYrA2t6NNeS/YssSsvdJJfKiBJ6zW6GslBg0bn4Up8JwBnLnAGFGuFkxWxNDGLh/76lQ1XtDpUoJwujrPku13gChtg7cBqO7+zoE59NJ9HYSfp4OZ+P9Lo/QK/QajVGE3qEZ+oDO0AIxb+ZlnvYK/6v/zf/u/7hN9b19zUvUM//nHw/58a8=</latexit>

[wind

farm].

<latexit sha1_base64="w8KDLMXs6SaYywgazGIUXWr+c4c=">AAADKXicbVLLjtMwFHXCawgwL5ZsLNpKZVMlXXRGrGY0GzbAINGZkZqoclx3atWOI/umnSjKN7CFb+Br2AFbfgQnUyCZcqU4555777F9kjgV3IDv/3Dce/cfPHy089h78vTZ7t7+weGFUZmmbEyVUPoqJoYJnrAxcBDsKtWMyFiwy3h5VtUvV0wbrpKPkKcskuQ64XNOCVhqeuC4ve4kjGWxKqdB1MV/k2GVhIuZAtNg31p2AprwBNtFrEkeDTzvn0K/Ajflq5ZSk9xWbFS3lXvdcFUP1HDxB4bAbiCeF2fqfVo28vpdm1JoNiuLd2yNqSDGMFPWfUymi+KU0kwTmr/GR6PB8aic7nf8gV8H3gbBBnTQJs6ta7vhTNFMsgRq+UngpxAVRAOngpVemBmWErok12xiYUIkM1FRH6zEPcvM8Fxp+ySAa7Y5URBpTC5j2ykJLMzdWkX+rzbJYH4cFTxJM2AJvd1ongkMClefHs+4ZhREbgGhmtuzYrog1gmwP0hrl1i27lDITADXat1mY6WWQGJTetbB4K5f2+BiOAhGA//DsHPS33i5g16gl6iPAnSETtAbdI7GiDrc+eR8dr64X91v7nf3522r62xmnqNWuL9+AynP/UM=</latexit>
[v1

(x)]

[v2(x)]

...

[vM (x)]

[train

railway].

<latexit sha1_base64="8F2SgDUKU2CwjF7V4lnnVuQDSfM=">AAADOXicbVLLbtNAFB2bVzHQF0s2I9JKYRPZWaQVq6Ju2ABFatpKthWNJ5NmlBmPNXOd1LL8LWzhG/gSluwQW36AsRuQ3fRKts895z7GR5Nkghvw/R+O++Dho8dPtp56z56/2N7Z3du/MCrXlI2pEkpfJcQwwVM2Bg6CXWWaEZkIdpksTmv9csm04So9hyJjsSTXKZ9xSsBSkz1n//AgjBJZLqtJEB/g/8mwTqL5VIFpsR8sG4ImPMX2JVakiAee1xrRr8FN9aYzqk1ujmyp942Olk1DA+f/YATsBpJZeao+ZVUrb76NK6Vm06r8yFaYCmIMM1VTx2Q2L99RmmtCi7f4aDQ4HlnhvL13stvzB34TeBMEa9BD6zizLm5HU0VzyVJotoWBn0FcEg2cClZ5UW5YRuiCXLPQwpRIZuKyOWeFDy0zxTOl7ZMCbth2R0mkMYVMbKUkMDd3tZq8TwtzmB3HJU+zHFhKbxfNcoFB4foq4CnXjIIoLCBUc3tWTOfEGgP2wnS2JLLzD6XMBXCtVl02UWoBJDGVZx0M7vq1CS6Gg2A08D8Peyf9tZdb6BV6jfooQEfoBL1HZ2iMqFM4X5yvzjf3u/vT/eX+vi11nXXPS9QJ989fXHMDQg==</latexit>

Train

railway

<latexit sha1_base64="spEv4ej9mg3LJ30AXIGKzolRdcY=">AAACPHicbZDLTgIxFIY7eENUBE3cuGkkJqzIDAYxrjBuXGIilwQI6ZQCDe100p7RkJGXcavP4Hu4d2fcurZcFgL+SZM//zknp+fzQ8ENuO6Hk9jY3NreSe6m9vYP0oeZ7FHdqEhTVqNKKN30iWGCB6wGHARrhpoR6QvW8Ee303rjkWnDVfAA45B1JBkEvM8pARt1MydtJsNhfENppAkdX+NyqXBRmnQzObfgzoTXjbcwObRQtZt10u2eopFkAVBBjGl5bgidmGjgVLBJqh0ZFhI6IgPWsjYgkplOPDtggs9t0sN9pe0LAM/SvxMxkcaMpW87JYGhWa1Nw/9qrQj6V52YB2EELKDzRf1IYFB4SgP3uGYUxNgaQjW3f8V0SCwJsMyWtvhy6YZYRgK4Vk/Lqa/UCIhvJilL0FvltW7qxYJ3WXDvi7lKfsEyiU7RGcojD5VRBd2hKqohip7RC3pFb8678+l8Od/z1oSzmDlGS3J+fgEPPa1w</latexit>
Accuracy:

75.35

<latexit sha1_base64="1srz8pVJrnh6c3fD9AduK7Gk5D0=">AAADS3icjVLBbtNAEF27FEqA0sKRy4qkUrhEdiRK21NRL1yAIpG2UmxF6/W6XmXXa+2Ok0aWv4Cv4QrfwAfwHdwQB9ZuQE7CgZHWevNm5s36aaNccAOe991xt+5s3723c7/z4OGj3cd7+08ujCo0ZSOqhNJXETFM8IyNgINgV7lmREaCXUbTs7p+OWPacJV9hEXOQkmuM55wSsBSk32nd9AbB5EsZ9XED3v4bzKskyCNFZgW+9ay4wDYDTSrS83iqgRNeIbtR8zJogoHnU5Ls1+Dm+rFinab3NzRqv7XrmDWKDQw/QObwSgpz9T7vGrl64Lv2BxTQYxhprJ9AZN5Wr6mtNCELk7w4cvB0XE12et6A68JvAn8JeiiZZxbX3eDWNFCsgwa9bHv5RCWRAOnglWdoDAsJ3RKrtnYwoxIZsKyuVeFDywT40RpezLADdueKIk0ZiEj2ykJpGa9VpP/qo0LSI7Ckmd5ASyjt4uSQmBQuH4cOOaaURALCwjV3N4V05RYJ8A+oZUtkVz5h1IWArhW81U2UmoKJDJVxzror/u1CS6GA/9w4H0Ydk/7Sy930DP0HPWRj16hU/QGnaMRos4n57PzxfnqfnN/uD/dX7etrrOceYpWYmv7N796DH4=</latexit>
Accuracy:

65.89

<latexit sha1_base64="oKJTGrxy+9QoHd9nSFUvs70YTHk=">AAADS3icjVLNbtNAEF67FEqA/sCRy4qkUrhEdg5pxamoFy5AkUhbybai9WZTr7LrtXbHSS3LT8DTcIVn4AF4Dm6IA2s3ICfhwEhrffPNzDfrTxtnghvwvO+Ou3Nv9/6DvYedR4+f7B8cHj29NCrXlI2pEkpfx8QwwVM2Bg6CXWeaERkLdhXPz+v61YJpw1X6EYqMRZLcpHzGKQFLTY6c3nEvCGNZLqqJH/Xw32RYJ2EyVWBa7FvLBiGwW2hWl5pNqxI04Sm2H7EkRRUNOp2WZr8Gt9XLNe02ub2jVf2vXeGiUWhg8gc2g/GsPFfvs6qVbwq+Y0tMBTGGmcr2hUxmSfma0lwTWrzCJ6PB6aiaHHa9gdcE3gb+CnTRKi6sr/vhVNFcshQa9cD3MohKooFTwapOmBuWETonNyywMCWSmahs7lXhY8tM8Uxpe1LADdueKIk0ppCx7ZQEErNZq8l/1YIcZqdRydMsB5bSu0WzXGBQuH4ceMo1oyAKCwjV3N4V04RYJ8A+obUtsVz7h1LmArhWy3U2VmoOJDZVxzrob/q1DS6HA3808D4Mu2f9lZd76Dl6gfrIRyfoDL1BF2iMqPPJ+ex8cb6639wf7k/3112r66xmnqG12Nn9Db2/DH0=</latexit>
Accuracy:

76.86

(b) The instance-conditional prompts learned by CoCoOp are much more generalizable than CoOp to the unseen classes.

Figure 1. Motivation of our research: to learn generalizable prompts. The images are randomly selected from SUN397 [55], which is a widely-used scene recognition dataset.

text words in a prompt into a set of learnable vectors, taking advantage of the differentiable nature of neural networks. With only a few labeled images for learning, CoOp achieves huge improvements over intensively-tuned manual prompts across a wide range of image recognition datasets.
In our study, we identify a critical problem of CoOp: the learned context is not generalizable to wider unseen classes within the same task. Figure 1 illustrates the problem: the context learned by CoOp works well in distinguishing the base classes like “arrival gate” and “cathedral” but suffers a significant drop in accuracy when it is transferred to the new (unseen) classes, such as “wind farm” and “train railway”— even though the task’s nature remains the same, i.e., recognizing scenes. The results suggest that the learned context overfits the base classes, thus failing to capture more generalizable elements that are vital for broader scene recognition. We argue that such a problem is caused by CoOp’s static design: the context, which is fixed once learned, is optimized only for a specific set of (training) classes. On the contrary, the manually-designed prompts adopted by the zero-shot method are relatively generalizable.
To address the weak generalizability problem, we introduce a novel concept: conditional prompt learning. The key idea is to make a prompt conditioned on each input instance (image) rather than fixed once learned. To make the model parameter-efficient, we introduce a simple yet effective implementation of conditional prompt learning. Specifically, we extend CoOp by further learning a lightweight neural network to generate for each image an input-conditional token (vector), which is combined with the learnable context vectors. We call our approach Conditional Context Optimization (CoCoOp).2 An overview is shown in Figure 2. Interestingly, the paradigm of CoCoOp is analogous to image captioning [49], which explains why instance-
2Pronounced as /k@U­ku:p/.

conditional prompts are more generalizable: they are optimized to characterize each instance (more robust to class shift) rather than to serve only for some specific classes.
We present comprehensive experiments on 11 datasets covering a diverse set of visual recognition tasks. Specifically, we design a base-to-new generalization setting where a model is first learned using base classes and then tested on completely new classes. Compared with the zero-shot method [40] and CoOp [62], our approach achieves the best overall performance (Table 1). Importantly, CoCoOp gains significant improvements over CoOp in unseen classes (Figure 3(a)), allowing the gap between manual and learningbased prompts to be substantially reduced.
In a more challenging scenario where the context learned for one task is directly transferred to other tasks with drastically different classes, CoCoOp still beats CoOp with a clear margin (Table 2), suggesting that instance-conditional prompts are more transferable and have the potential to succeed at larger scale. CoCoOp also obtains stronger domain generalization performance than CoOp (Table 3), further justifying the strengths of dynamic prompts.
In summary, our research provides timely insights into the generalizability problem in prompt learning, and crucially, demonstrates the effectiveness of a simple idea in various problem scenarios. We hope our approach and the findings presented in this work can pave the way for future research in generalizable—and transferable—prompt learning.
2. Related Work
Vision-Language Models We mainly review studies focused on aligning images and texts to learn a joint embedding space [24, 40, 59]. The idea of cross-modality alignment is certainly not new and has been investigated since nearly a decade ago—though with dramatically different

16817

Conditional Context Optimization (CoCoOp)

technologies than today. A typical vision-language model consists of three key el-
ements: two for image and text encoding while the third is related to the design of loss functions. In early days, models for processing images and texts are often designed and also learned independently, with their outputs connected by extra modules (losses) for alignment. Images are often encoded using hand-crafted descriptors [10, 45] or neural networks [12, 29], while texts are encoded using, for instance, pre-trained word vectors [12, 45] or the frequency-based TF-IDF features [10, 29]. In terms of cross-modality alignment, common approaches include metric learning [12], multi-label classification [16, 26], and n-gram language learning [31]. Recently, a study suggests that training the vision part with an image captioning loss can make the visual representation more transferable [7].
Recent vision-language models [13,24,33,40] bridge the two modalities by learning two encoders jointly. Also, the models are now built with much larger neural networks. As discussed in Zhou et al. [62], recent successes in visionlanguage models are mainly attributed to the developments in i) Transformers [48], ii) contrastive representation learning [4, 17, 20], and iii) web-scale training datasets [24, 40]. A representative approach is CLIP [40], which trains two neural network-based encoders using a contrastive loss to match pairs of images and texts. After consuming 400 million data pairs, the CLIP model demonstrates a remarkable zero-shot image recognition capability. Similar to CoOp [62], our approach is orthogonal to the research of CLIP-like models [13,24,33,40], aiming to offer an efficient solution for adapting pre-trained vision-language models to downstream applications.
Prompt Learning This topic originates from the NLP domain. The motivation was to view pre-trained language models, such as BERT [8] or GPT [41], as knowledge bases from which information useful to downstream tasks is elicited [39]. Concretely, given a pre-trained language model, the task is often formulated as a “fill-in-the-blank” cloze test, such as asking the model to predict the masked token in “No reason to watch. It was [MASK]” as either “positive” or “negative” for sentiment classification. The key lies in how to design the underlined part, known as prompt (template), in such a format familiar to the model.
Instead of manually designing a prompt, research in prompt learning aims to automate the process with the help of affordable-sized labeled data. Jiang et al. [25] use text mining and paraphrasing to generate a group of candidate prompts, within which the optimal ones are chosen to have the highest training accuracy. Shin et al. [44] propose AutoPrompt, a gradient-based approach that selects from a vocabulary the best tokens that cause the greatest changes in gradients based on the label likelihood. Our research is most related to continuous prompt learning methods [30, 32, 60],

context tokens

<latexit sha1_base64="HOGtH0eXP5R5+EEu2fQHZwuL4xw=">AAACF3icbVA9TwJBEN3DL8Qv1NJmI5hYkTsKtSSxscREPiJcyN6ywIbdvcvuHIZc7l/YWOhfsTO2lv4TSxe4QsCXTPLy3kxm5gWR4AZc99vJbWxube/kdwt7+weHR8Xjk6YJY01Zg4Yi1O2AGCa4Yg3gIFg70ozIQLBWML6d+a0J04aH6gGmEfMlGSo+4JSAlR7L3UAmk7TnlXvFkltx58DrxMtICWWo94o/3X5IY8kUUEGM6XhuBH5CNHAqWFroxoZFhI7JkHUsVUQy4yfzi1N8YZU+HoTalgI8V/9OJEQaM5WB7ZQERmbVm4n/eZ0YBjd+wlUUA1N0sWgQCwwhnr2P+1wzCmJqCaGa21sxHRFNKNiQlrYEcumHRMYCuA6f0oKNylsNZp00qxXvquLeV0u1ahZaHp2hc3SJPHSNaugO1VEDUaTQM3pFb86L8+58OJ+L1pyTzZyiJThfv5cLoGY=</latexit>

v1

+<latexitsha1_base64="MdQQkHPETjye9ifJOXGRj5eF6M4=">AAACF3icbVC7SgNBFJ31GeMrammzGARBCLsp1DJgYxnBPDBZwuxkNhkyj2XmrhqW/QsbC/0VO7G19E8snSRbmMQDFw7n3Mu994QxZwY879tZWV1b39gsbBW3d3b39ksHh02jEk1ogyiudDvEhnImaQMYcNqONcUi5LQVjq4nfuuBasOUvINxTAOBB5JFjGCw0n0X6BOEUXqe9Uplr+JN4S4TPydllKPeK/10+4okgkogHBvT8b0YghRrYITTrNhNDI0xGeEB7VgqsaAmSKcXZ+6pVfpupLQtCe5U/TuRYmHMWIS2U2AYmkVvIv7ndRKIroKUyTgBKslsUZRwF5Q7ed/tM00J8LElmGhmb3XJEGtMwIY0tyUUcz+kIuHAtHrMijYqfzGYZdKsVvyLindbLdeqeWgFdIxO0Bny0SWqoRtURw1EkETP6BW9OS/Ou/PhfM5aV5x85gjNwfn6BZuRoQE=</latexit>

<latexit sha1_base64="/ObT7ntJtfRdwD1X3OgKDxBQKY0=">AAACF3icbVC7TgJBFJ3FF+ILtbSZCCZWZHcLtSSxscREHhE2ZHaYhQnz2MzMYshm/8LGQn/Fztha+ieWDrCFgCe5yck59+bee8KYUW1c99spbGxube8Ud0t7+weHR+Xjk5aWicKkiSWTqhMiTRgVpGmoYaQTK4J4yEg7HN/O/PaEKE2leDDTmAQcDQWNKEbGSo/VXsjTSdb3q/1yxa25c8B14uWkAnI0+uWf3kDihBNhMENadz03NkGKlKGYkazUSzSJER6jIelaKhAnOkjnF2fwwioDGEllSxg4V/9OpIhrPeWh7eTIjPSqNxP/87qJiW6ClIo4MUTgxaIoYdBIOHsfDqgi2LCpJQgram+FeIQUwsaGtLQl5Es/pDxhhir5lJVsVN5qMOuk5de8q5p771fqfh5aEZyBc3AJPHAN6uAONEATYCDAM3gFb86L8+58OJ+L1oKTz5yCJThfv5i2oGc=</latexit>

v2

+<latexitsha1_base64="MdQQkHPETjye9ifJOXGRj5eF6M4=">AAACF3icbVC7SgNBFJ31GeMrammzGARBCLsp1DJgYxnBPDBZwuxkNhkyj2XmrhqW/QsbC/0VO7G19E8snSRbmMQDFw7n3Mu994QxZwY879tZWV1b39gsbBW3d3b39ksHh02jEk1ogyiudDvEhnImaQMYcNqONcUi5LQVjq4nfuuBasOUvINxTAOBB5JFjGCw0n0X6BOEUXqe9Uplr+JN4S4TPydllKPeK/10+4okgkogHBvT8b0YghRrYITTrNhNDI0xGeEB7VgqsaAmSKcXZ+6pVfpupLQtCe5U/TuRYmHMWIS2U2AYmkVvIv7ndRKIroKUyTgBKslsUZRwF5Q7ed/tM00J8LElmGhmb3XJEGtMwIY0tyUUcz+kIuHAtHrMijYqfzGYZdKsVvyLindbLdeqeWgFdIxO0Bny0SWqoRtURw1EkETP6BW9OS/Ou/PhfM5aV5x85gjNwfn6BZuRoQE=</latexit>

.<latexitsha1_base64="i4zpmuRm7l6oli4lXwkT2OQ8Jsk=">AAACFXicbVC7TgJBFJ31ifhCLW02ggkV2aVQSxIbS0zkkcCGzM7OwoR5bGbuasiGn7Cx0F+xM7bW/omlA2wh4ElucnLOvbn3njDhzIDnfTsbm1vbO7uFveL+weHRcenktG1UqgltEcWV7obYUM4kbQEDTruJpliEnHbC8e3M7zxSbZiSDzBJaCDwULKYEQxW6lb6o0iBqQxKZa/mzeGuEz8nZZSjOSj99CNFUkElEI6N6fleAkGGNTDC6bTYTw1NMBnjIe1ZKrGgJsjm907dS6tEbqy0LQnuXP07kWFhzESEtlNgGJlVbyb+5/VSiG+CjMkkBSrJYlGccheUO3vejZimBPjEEkw0s7e6ZIQ1JmAjWtoSiqUfMpFyYFo9TYs2Kn81mHXSrtf8q5p3Xy83qnloBXSOLlAV+egaNdAdaqIWIoijZ/SK3pwX5935cD4XrRtOPnOGluB8/QImmZ+d</latexit>

..

<latexit sha1_base64="t4hsqUX1Tf4ub/K3HNBJTidAYC8=">AAACF3icbVA9TwJBEJ3zE/ELtbTZCCZW5I5CLUlsbEwwkY8IF7K37MGG3b3L7h6GXPgXNhb6V+yMraX/xNIFrhDwJZO8vDeTmXlBzJk2rvvtrK1vbG5t53byu3v7B4eFo+OGjhJFaJ1EPFKtAGvKmaR1wwynrVhRLAJOm8HwZuo3R1RpFskHM46pL3BfspARbKz0WOoEIh1NunelbqHolt0Z0CrxMlKEDLVu4afTi0giqDSEY63bnhsbP8XKMMLpJN9JNI0xGeI+bVsqsaDaT2cXT9C5VXoojJQtadBM/TuRYqH1WAS2U2Az0MveVPzPaycmvPZTJuPEUEnmi8KEIxOh6fuoxxQlho8twUQxeysiA6wwMTakhS2BWPghFQk3TEVPk7yNylsOZpU0KmXvsuzeV4rVShZaDk7hDC7Agyuowi3UoA4EJDzDK7w5L8678+F8zlvXnGzmBBbgfP0Cxb+ggg==</latexit>

vM

.<latexitsha1_base64="i4zpmuRm7l6oli4lXwkT2OQ8Jsk=">AAACFXicbVC7TgJBFJ31ifhCLW02ggkV2aVQSxIbS0zkkcCGzM7OwoR5bGbuasiGn7Cx0F+xM7bW/omlA2wh4ElucnLOvbn3njDhzIDnfTsbm1vbO7uFveL+weHRcenktG1UqgltEcWV7obYUM4kbQEDTruJpliEnHbC8e3M7zxSbZiSDzBJaCDwULKYEQxW6lb6o0iBqQxKZa/mzeGuEz8nZZSjOSj99CNFUkElEI6N6fleAkGGNTDC6bTYTw1NMBnjIe1ZKrGgJsjm907dS6tEbqy0LQnuXP07kWFhzESEtlNgGJlVbyb+5/VSiG+CjMkkBSrJYlGccheUO3vejZimBPjEEkw0s7e6ZIQ1JmAjWtoSiqUfMpFyYFo9TYs2Kn81mHXSrtf8q5p3Xy83qnloBXSOLlAV+egaNdAdaqIWIoijZ/SK3pwX5935cD4XrRtOPnOGluB8/QImmZ+d</latexit>

..

+<latexitsha1_base64="MdQQkHPETjye9ifJOXGRj5eF6M4=">AAACF3icbVC7SgNBFJ31GeMrammzGARBCLsp1DJgYxnBPDBZwuxkNhkyj2XmrhqW/QsbC/0VO7G19E8snSRbmMQDFw7n3Mu994QxZwY879tZWV1b39gsbBW3d3b39ksHh02jEk1ogyiudDvEhnImaQMYcNqONcUi5LQVjq4nfuuBasOUvINxTAOBB5JFjGCw0n0X6BOEUXqe9Uplr+JN4S4TPydllKPeK/10+4okgkogHBvT8b0YghRrYITTrNhNDI0xGeEB7VgqsaAmSKcXZ+6pVfpupLQtCe5U/TuRYmHMWIS2U2AYmkVvIv7ndRKIroKUyTgBKslsUZRwF5Q7ed/tM00J8LElmGhmb3XJEGtMwIY0tyUUcz+kIuHAtHrMijYqfzGYZdKsVvyLindbLdeqeWgFdIxO0Bny0SWqoRtURw1EkETP6BW9OS/Ou/PhfM5aV5x85gjNwfn6BZuRoQE=</latexit>

<latexit sha1_base64="4KzQHU+3d8F0bpLXiRHf1+GMPSk=">AAACF3icbVA9T8MwEL3wWcpXgZHFokViqpIOwFiJhbFI9EM0UeW4TmvVdiLbAVVR/wULA/wVNsTKyD9hxG0z0JYnnfT03p3u7oUJZ9q47reztr6xubVd2Cnu7u0fHJaOjls6ThWhTRLzWHVCrClnkjYNM5x2EkWxCDlth6Obqd9+pEqzWN6bcUIDgQeSRYxgY6WHih+KzE/YpNIrld2qOwNaJV5OypCj0Sv9+P2YpIJKQzjWuuu5iQkyrAwjnE6KfqppgskID2jXUokF1UE2u3iCzq3SR1GsbEmDZurfiQwLrccitJ0Cm6Fe9qbif143NdF1kDGZpIZKMl8UpRyZGE3fR32mKDF8bAkmitlbERlihYmxIS1sCcXCD5lIuWEqfpoUbVTecjCrpFWrepdV965Wrtfy0ApwCmdwAR5cQR1uoQFNICDhGV7hzXlx3p0P53PeuubkMyewAOfrF+USoJU=</latexit>

⇡

<latexit sha1_base64="4KzQHU+3d8F0bpLXiRHf1+GMPSk=">AAACF3icbVA9T8MwEL3wWcpXgZHFokViqpIOwFiJhbFI9EM0UeW4TmvVdiLbAVVR/wULA/wVNsTKyD9hxG0z0JYnnfT03p3u7oUJZ9q47reztr6xubVd2Cnu7u0fHJaOjls6ThWhTRLzWHVCrClnkjYNM5x2EkWxCDlth6Obqd9+pEqzWN6bcUIDgQeSRYxgY6WHih+KzE/YpNIrld2qOwNaJV5OypCj0Sv9+P2YpIJKQzjWuuu5iQkyrAwjnE6KfqppgskID2jXUokF1UE2u3iCzq3SR1GsbEmDZurfiQwLrccitJ0Cm6Fe9qbif143NdF1kDGZpIZKMl8UpRyZGE3fR32mKDF8bAkmitlbERlihYmxIS1sCcXCD5lIuWEqfpoUbVTecjCrpFWrepdV965Wrtfy0ApwCmdwAR5cQR1uoQFNICDhGV7hzXlx3p0P53PeuubkMyewAOfrF+USoJU=</latexit>

⇡

.<latexitsha1_base64="i4zpmuRm7l6oli4lXwkT2OQ8Jsk=">AAACFXicbVC7TgJBFJ31ifhCLW02ggkV2aVQSxIbS0zkkcCGzM7OwoR5bGbuasiGn7Cx0F+xM7bW/omlA2wh4ElucnLOvbn3njDhzIDnfTsbm1vbO7uFveL+weHRcenktG1UqgltEcWV7obYUM4kbQEDTruJpliEnHbC8e3M7zxSbZiSDzBJaCDwULKYEQxW6lb6o0iBqQxKZa/mzeGuEz8nZZSjOSj99CNFUkElEI6N6fleAkGGNTDC6bTYTw1NMBnjIe1ZKrGgJsjm907dS6tEbqy0LQnuXP07kWFhzESEtlNgGJlVbyb+5/VSiG+CjMkkBSrJYlGccheUO3vejZimBPjEEkw0s7e6ZIQ1JmAjWtoSiqUfMpFyYFo9TYs2Kn81mHXSrtf8q5p3Xy83qnloBXSOLlAV+egaNdAdaqIWIoijZ/SK3pwX5935cD4XrRtOPnOGluB8/QImmZ+d</latexit>

..

<latexit sha1_base64="4KzQHU+3d8F0bpLXiRHf1+GMPSk=">AAACF3icbVA9T8MwEL3wWcpXgZHFokViqpIOwFiJhbFI9EM0UeW4TmvVdiLbAVVR/wULA/wVNsTKyD9hxG0z0JYnnfT03p3u7oUJZ9q47reztr6xubVd2Cnu7u0fHJaOjls6ThWhTRLzWHVCrClnkjYNM5x2EkWxCDlth6Obqd9+pEqzWN6bcUIDgQeSRYxgY6WHih+KzE/YpNIrld2qOwNaJV5OypCj0Sv9+P2YpIJKQzjWuuu5iQkyrAwjnE6KfqppgskID2jXUokF1UE2u3iCzq3SR1GsbEmDZurfiQwLrccitJ0Cm6Fe9qbif143NdF1kDGZpIZKMl8UpRyZGE3fR32mKDF8bAkmitlbERlihYmxIS1sCcXCD5lIuWEqfpoUbVTecjCrpFWrepdV965Wrtfy0ApwCmdwAR5cQR1uoQFNICDhGV7hzXlx3p0P53PeuubkMyewAOfrF+USoJU=</latexit>

⇡

[CLASS]

.<latexitsha1_base64="i4zpmuRm7l6oli4lXwkT2OQ8Jsk=">AAACFXicbVC7TgJBFJ31ifhCLW02ggkV2aVQSxIbS0zkkcCGzM7OwoR5bGbuasiGn7Cx0F+xM7bW/omlA2wh4ElucnLOvbn3njDhzIDnfTsbm1vbO7uFveL+weHRcenktG1UqgltEcWV7obYUM4kbQEDTruJpliEnHbC8e3M7zxSbZiSDzBJaCDwULKYEQxW6lb6o0iBqQxKZa/mzeGuEz8nZZSjOSj99CNFUkElEI6N6fleAkGGNTDC6bTYTw1NMBnjIe1ZKrGgJsjm907dS6tEbqy0LQnuXP07kWFhzESEtlNgGJlVbyb+5/VSiG+CjMkkBSrJYlGccheUO3vejZimBPjEEkw0s7e6ZIQ1JmAjWtoSiqUfMpFyYFo9TYs2Kn81mHXSrtf8q5p3Xy83qnloBXSOLlAV+egaNdAdaqIWIoijZ/SK3pwX5935cD4XrRtOPnOGluB8/QImmZ+d</latexit>

..

.

meta token

<latexit sha1_base64="4KzQHU+3d8F0bpLXiRHf1+GMPSk=">AAACF3icbVA9T8MwEL3wWcpXgZHFokViqpIOwFiJhbFI9EM0UeW4TmvVdiLbAVVR/wULA/wVNsTKyD9hxG0z0JYnnfT03p3u7oUJZ9q47reztr6xubVd2Cnu7u0fHJaOjls6ThWhTRLzWHVCrClnkjYNM5x2EkWxCDlth6Obqd9+pEqzWN6bcUIDgQeSRYxgY6WHih+KzE/YpNIrld2qOwNaJV5OypCj0Sv9+P2YpIJKQzjWuuu5iQkyrAwjnE6KfqppgskID2jXUokF1UE2u3iCzq3SR1GsbEmDZurfiQwLrccitJ0Cm6Fe9qbif143NdF1kDGZpIZKMl8UpRyZGE3fR32mKDF8bAkmitlbERlihYmxIS1sCcXCD5lIuWEqfpoUbVTecjCrpFWrepdV965Wrtfy0ApwCmdwAR5cQR1uoQFNICDhGV7hzXlx3p0P53PeuubkMyewAOfrF+USoJU=</latexit>

⇡

Meta-Net

Text Encoder Image Encoder

.<latexitsha1_base64="i4zpmuRm7l6oli4lXwkT2OQ8Jsk=">AAACFXicbVC7TgJBFJ31ifhCLW02ggkV2aVQSxIbS0zkkcCGzM7OwoR5bGbuasiGn7Cx0F+xM7bW/omlA2wh4ElucnLOvbn3njDhzIDnfTsbm1vbO7uFveL+weHRcenktG1UqgltEcWV7obYUM4kbQEDTruJpliEnHbC8e3M7zxSbZiSDzBJaCDwULKYEQxW6lb6o0iBqQxKZa/mzeGuEz8nZZSjOSj99CNFUkElEI6N6fleAkGGNTDC6bTYTw1NMBnjIe1ZKrGgJsjm907dS6tEbqy0LQnuXP07kWFhzESEtlNgGJlVbyb+5/VSiG+CjMkkBSrJYlGccheUO3vejZimBPjEEkw0s7e6ZIQ1JmAjWtoSiqUfMpFyYFo9TYs2Kn81mHXSrtf8q5p3Xy83qnloBXSOLlAV+egaNdAdaqIWIoijZ/SK3pwX5935cD4XrRtOPnOGluB8/QImmZ+d</latexit>

.

.

.<latexitsha1_base64="i4zpmuRm7l6oli4lXwkT2OQ8Jsk=">AAACFXicbVC7TgJBFJ31ifhCLW02ggkV2aVQSxIbS0zkkcCGzM7OwoR5bGbuasiGn7Cx0F+xM7bW/omlA2wh4ElucnLOvbn3njDhzIDnfTsbm1vbO7uFveL+weHRcenktG1UqgltEcWV7obYUM4kbQEDTruJpliEnHbC8e3M7zxSbZiSDzBJaCDwULKYEQxW6lb6o0iBqQxKZa/mzeGuEz8nZZSjOSj99CNFUkElEI6N6fleAkGGNTDC6bTYTw1NMBnjIe1ZKrGgJsjm907dS6tEbqy0LQnuXP07kWFhzESEtlNgGJlVbyb+5/VSiG+CjMkkBSrJYlGccheUO3vejZimBPjEEkw0s7e6ZIQ1JmAjWtoSiqUfMpFyYFo9TYs2Kn81mHXSrtf8q5p3Xy83qnloBXSOLlAV+egaNdAdaqIWIoijZ/SK3pwX5935cD4XrRtOPnOGluB8/QImmZ+d</latexit>

.

.

.<latexitsha1_base64="i4zpmuRm7l6oli4lXwkT2OQ8Jsk=">AAACFXicbVC7TgJBFJ31ifhCLW02ggkV2aVQSxIbS0zkkcCGzM7OwoR5bGbuasiGn7Cx0F+xM7bW/omlA2wh4ElucnLOvbn3njDhzIDnfTsbm1vbO7uFveL+weHRcenktG1UqgltEcWV7obYUM4kbQEDTruJpliEnHbC8e3M7zxSbZiSDzBJaCDwULKYEQxW6lb6o0iBqQxKZa/mzeGuEz8nZZSjOSj99CNFUkElEI6N6fleAkGGNTDC6bTYTw1NMBnjIe1ZKrGgJsjm907dS6tEbqy0LQnuXP07kWFhzESEtlNgGJlVbyb+5/VSiG+CjMkkBSrJYlGccheUO3vejZimBPjEEkw0s7e6ZIQ1JmAjWtoSiqUfMpFyYFo9TYs2Kn81mHXSrtf8q5p3Xy83qnloBXSOLlAV+egaNdAdaqIWIoijZ/SK3pwX5935cD4XrRtOPnOGluB8/QImmZ+d</latexit>

.

.

Figure 2. Our approach, Conditional Context Optimization (CoCoOp), consists of two learnable components: a set of context vectors and a lightweight neural network (Meta-Net) that generates for each image an input-conditional token.

where the main idea is to turn a prompt into a set of continuous vectors that can be end-to-end optimized with respect to an objective function. See Liu et al. [34] for a more comprehensive survey.
In computer vision, prompt learning is a nascent research direction that has only been explored very recently [27, 42, 56,58,62]. Our research is built on top of CoOp [62], which is the earliest work to bring continuous prompt learning to the vision domain for adaptation of pre-trained visionlanguage models. Crucially, our approach solves the weak generalizability problem of CoOp [62], based on a simple idea of conditional prompt learning—which to our knowledge is also novel in the context of NLP and thus could be of interest to the NLP community as well.
Zero-Shot Learning (ZSL) is another relevant research area where the goal is similar to ours, i.e., to recognize novel classes by training only on base classes [3,51,54,57]. Moreover, the generalization problem where a model trained on base classes often fails on novel classes is also linked to the “seen-class bias” issue raised in the ZSL literature [54]. The most common approach to ZSL is to learn a semantic space based on auxiliary information such as attributes [23] or word embeddings [12, 52]. Different from existing ZSL methods, our work addresses the emerging problem of adapting large vision-language models and uses drastically different techniques based on prompting.
3. Methodology
An overview of our approach is shown in Figure 2. Below we first provide brief reviews on CLIP [40], which is the base model used in this paper, and CoOp [62]. Then, we present the technical details of our approach as well as the rationale behind the design. Same as CoOp, our approach is applicable to broader CLIP-like vision-language models.

16818

3.1. Reviews of CLIP and CoOp
Contrastive Language-Image Pre-training known as CLIP [41], has well demonstrated the potential of learning open-set visual concepts. CLIP is built using two encoders, one for image and the other for text, as shown in Figure 2. The image encoder can be either a ResNet [18] or a ViT [9], which is used to transform an image into a feature vector. The text encoder is a Transformer [48], which takes as input a sequence of word tokens and again produces a vectorized representation.
During training, CLIP adopts a contrastive loss to learn a joint embedding space for the two modalities. Specifically, for a mini-batch of image-text pairs, CLIP maximizes for each image the cosine similarity with the matched text while minimizes the cosine similarities with all other unmatched texts, and the loss is computed in a similar fashion for each text too. After training, CLIP can be used for zeroshot image recognition. Let x be image features generated by the image encoder and {wi}Ki=1 a set of weight vectors produced by the text encoder, each representing a category (suppose there are K categories in total). In particular, each wi is derived from a prompt, such as “a photo of a {class}” where the “{class}” token is filled with the i-th class name. The prediction probability is then

  \label {eq:pred_zs} p(y | \bm {x}) = \frac {\exp (\operatorname {sim} (\bm {x}, \bm {w}_y) / \tau )}{\sum _{i=1}^K \exp (\operatorname {sim} (\bm {x}, \bm {w}_i) / \tau )}, 

(1)

where sim(·, ·) denotes cosine similarity and τ is a learned temperature parameter.
Context Optimization (CoOp) aims to overcome the inefficiency problem in prompt engineering for better adapting pre-trained vision-language models to downstream applications [62]. The key idea in CoOp is to model each context token using a continuous vector that can be end-to-end learned from data. Concretely, instead of using “a photo of a” as the context, CoOp introduces M learnable context vectors, {v1, v2, . . . , vM }, each having the same dimension with the word embeddings. The prompt for the i-th class, denoted by ti, now becomes ti = {v1, v2, . . . , vM , ci} where ci is the word embedding(s) for the class name. The context vectors are shared among all classes.3 Let g(·) denote the text encoder, the prediction probability is then

  \label {eq:pred_co p} p(y | \bm {x}) = \frac {\exp (\operatorname {sim} (\bm {x}, g(\bm {t}_y)  / \tau )}{\sum _{i=1}^K \exp (\operatorname {sim} (\bm {x}, g(\bm {t}_i) / \tau )}. 

(2)

To adapt CLIP to a downstream image recognition dataset, a cross-entropy loss can be used as the learning objective. Since the text encoder g(·) is differentiable, gradi-
3CoOp has an alternative version that learns class-specific context, which is not considered here because it is not straightforward to transfer class-specific context to unseen classes.

ents can be propagated all the way back to update the context vectors. Note that the base model of CLIP is frozen in the entire training process (ours too).
3.2. CoCoOp: Conditional Context Optimization
CoOp is a data-efficient approach allowing the context vectors to be trained with only a few labeled images in a downstream dataset. However, as discussed CoOp is not generalizable to wider unseen classes within the same task. We argue that instance-conditional context can generalize better because it shifts the focus away from a specific set of classes—for reducing overfitting—to each input instance, and hence to the entire task.
A straightforward way to implement CoCoOp is to build M neural networks to get M context tokens. However, such a design would require M × the size of a neural network, which is much larger than having M context vectors as in CoOp. Here we propose a parameter-efficient design that works very well in practice. Specifically, on top of the M context vectors, we further learn a lightweight neural network, called Meta-Net, to generate for each input a conditional token (vector), which is then combined with the context vectors. See Figure 2 for a sketch of the architecture.
Let hθ(·) denote the Meta-Net parameterized by θ, each context token is now obtained by vm(x) = vm + π where π = hθ(x) and m ∈ {1, 2, ..., M }. The prompt for the i-th class is thus conditioned on the input, i.e., ti(x) = {v1(x), v2(x), . . . , vM (x), ci}. The prediction probability is computed as
 \label {eq:pred_cocop} p(y | \bm {x}) = \frac {\exp (\operatorname {sim} (\bm {x}, g(\bm {t}_y (\bm {x})) / \tau )}{\sum _{i=1}^K \exp (\operatorname {sim} (\bm {x}, g(\bm {t}_i (\bm {x}) / \tau )}.  (3)
During training, we update the context vectors {vm}M m=1 together with the Meta-Net’s parameters θ. In this work, the Meta-Net is built with a two-layer bottleneck structure (Linear-ReLU-Linear), with the hidden layer reducing the input dimension by 16×. The input to the Meta-Net is simply the output features produced by the image encoder. We leave exploration of more advanced designs for future work.
4. Experiments
Our approach is mainly evaluated in the following three problem settings: 1) generalization from base to new classes within a dataset (Section 4.1); 2) cross-dataset transfer (Section 4.2); 3) domain generalization (Section 4.3). All models used in our experiments are based on the open-source CLIP [40].4 Before discussing the results, we provide the details of the experimental setup below.
Datasets For the first two settings, i.e., base-to-new generalization and cross-dataset transfer, we use the 11 image
4https://github.com/openai/CLIP.

16819

Table 1. Comparison of CLIP, CoOp and CoCoOp in the base-to-new generalization setting. For learning-based methods (CoOp and CoCoOp), their prompts are learned from the base classes (16 shots). The results strongly justify the strong generalizability of conditional prompt learning. H: Harmonic mean (to highlight the generalization trade-off [54]).

(a) Average over 11 datasets.

Base New H

CLIP CoOp CoCoOp

69.34 82.69 80.47

74.22 63.22 71.69

71.70 71.66 75.83

CLIP CoOp CoCoOp

(b) ImageNet.
Base New
72.43 68.14 76.47 67.88 75.98 70.43

H
70.22 71.92 73.10

CLIP CoOp CoCoOp

(c) Caltech101.
Base New
96.84 94.00 98.00 89.81 97.96 93.81

H
95.40 93.73 95.84

CLIP CoOp CoCoOp

(d) OxfordPets.
Base New
91.17 97.26 93.67 95.29 95.20 97.69

H
94.12 94.47 96.43

(e) StanfordCars.

Base New

CLIP

63.37 74.89

CoOp 78.12 60.40

CoCoOp 70.49 73.59

H
68.65 68.13 72.01

CLIP CoOp CoCoOp

(f) Flowers102.
Base New
72.08 77.80 97.60 59.67 94.87 71.75

H
74.83 74.06 81.71

CLIP CoOp CoCoOp

(g) Food101.
Base New
90.10 91.22 88.33 82.26 90.70 91.29

H
90.66 85.19 90.99

(h) FGVCAircraft.

Base New

CLIP

27.19 36.29

CoOp 40.44 22.30

CoCoOp 33.41 23.71

H
31.09 28.75 27.74

CLIP CoOp CoCoOp

(i) SUN397.
Base New
69.36 75.35 80.60 65.89 79.74 76.86

H
72.23 72.51 78.27

CLIP CoOp CoCoOp

(j) DTD.
Base New
53.24 59.90 79.44 41.18 77.01 56.00

H
56.37 54.24 64.85

CLIP CoOp CoCoOp

(k) EuroSAT.
Base New
56.48 64.05 92.19 54.74 87.49 60.04

H
60.03 68.69 71.21

CLIP CoOp CoCoOp

(l) UCF101.
Base New
70.53 77.50 84.69 56.05 82.33 73.45

H
73.85 67.46 77.64

recognition datasets as in Zhou et al. [62], which cover a diverse set of recognition tasks. Specifically, the benchmark includes ImageNet [6] and Caltech101 [11] for classification on generic objects; OxfordPets [38], StanfordCars [28], Flowers102 [36], Food101 [2] and FGVCAircraft [35] for fine-grained classification; SUN397 [55] for scene recognition; UCF101 [46] for action recognition; DTD [5] for texture classification; and finally EuroSAT [19] for satellite imagery recognition. For domain generalization experiments, we use ImageNet as the source dataset and four other variants of ImageNet that contain different types of domain shift as the target datasets, namely ImageNetV2 [43], ImageNetSketch [50], ImageNet-A [22] and ImageNet-R [21].
Following Zhou et al. [62], we randomly sample for each dataset a few-shot training set while using the original test set for testing. We only evaluate the highest shot number studied in Zhou et al. [62], i.e., 16 shots, which is sufficient to justify our approach. For learning-based models, the results are averaged over three runs.
Baselines The direct rival to our approach is CoOp [62], which essentially learns static prompts (in comparison to our dynamic prompts). The zero-shot method, i.e., CLIP [40] is also compared, which is based on manual

prompts. It is worth mentioning that the manual prompt for each dataset was intensively tuned using all classes in the test data [40].
Training Details Our implementation is based on CoOp’s code.5 Throughout the experiments, we use the best available vision backbone in CLIP, i.e., ViT-B/16. Zhou et al. [62] have suggested that a shorter context length and a good initialization can lead to better performance and stronger robustness to domain shift. Therefore, we fix the context length to 4 and initialize the context vectors using the pre-trained word embeddings of “a photo of a” for both CoOp and CoCoOp. Due to the instance-conditional design, our approach is slow to train and consumes much more GPU memory than CoOp. Therefore, to ensure the model can fit into a GPU and meanwhile reduce the training time, we train CoCoOp with batch size of 1 for 10 epochs. Such a limitation is discussed in more detail in Section 5.
4.1. Generalization From Base to New Classes
Solving the weak generalizability problem of CoOp is the main focus in this research. On each of the 11 datasets,
5https://github.com/KaiyangZhou/CoOp.

16820

(a)

(b)

Figure 3. Comprehensive comparisons of CoCoOp and CoOp in the base-to-new generalization setting. (a) CoCoOp is able to gain consistent improvements over CoOp in unseen classes on all datasets. (b) CoCoOp’s declines in base accuracy are mostly under 3%, which are far outweighed by the gains in generalization.

we split the classes equally into two groups, one as base classes and the other as new classes. Learning-based models, i.e., CoOp and CoCoOp, are trained using only the base classes while evaluation is conducted on the base and new classes separately to test generalizability. The detailed results are shown in Table 1.
Failures of CoOp in Unseen Classes The split does not guarantee that the two class groups are equally difficult, as evidenced in CLIP’s bumpy results: the base and new accuracy numbers are dramatically different.6 Nonetheless, CoOp’s new accuracy is consistently much weaker than the base accuracy on nearly all datasets, leaving a huge gap of almost 20% on average (82.69% vs 63.22%). Despite maintaining an advantage over CLIP in terms of average performance, CoOp’s gains in the base classes are nearly zeroed out by the catastrophic failures in the new classes, highlighting the need to improve generalizability for learning-based prompts.
CoCoOp Significantly Narrows Generalization Gap As shown in Table 1(a), CoCoOp improves the accuracy in unseen classes from 63.22% to 71.69%, which largely reduces the gap with manual prompts. The results confirm that instance-conditional prompts are more generalizable. A more detailed breakdown of per-dataset improvement is visualized in Figure 3(a) where we observe more than 10% increases in accuracy on 5 out of 11 datasets. Notably, on the challenging ImageNet dataset, CoCoOp’s surge from 67.88% to 70.43% represents a non-trivial progress (the 70.43% accuracy even surpasses CLIP’s 68.14%).
6For convenience, we refer to base accuracy as the performance in base classes; and similarly for new accuracy.

CoCoOp’s Gains in Generalization Far Outweigh Losses in Base Accuracy In comparison to CoOp, performance drops in the base classes occur for CoCoOp on most datasets (see Figure 3(b)). This is reasonable because CoOp optimizes specifically for base classes whereas CoCoOp optimizes for each instance in order to gain more generalization over an entire task. But it is worth noting that on the 9 datasets where CoCoOp’s base accuracy drops below CoOp’s, most losses are under 3% (precisely on 6 out of 9 datasets), which are far outweighed by the gains in unseen classes shown in Figure 3(a); even for those where CoCoOp suffers the biggest losses, the boosts in generalization are mostly significant enough to turn the averages into positives, e.g., StanfordCars sees the worst base accuracy drop of -7.63% but has the third-highest accuracy gain of +13.19% in the new classes, which together bring a 5.56% positive improvement for CoCoOp.
CoCoOp Is More Compelling Than CLIP When taking into account both the base and new classes, CoCoOp shows a gain of more than 4% over CLIP (75.83% vs 71.70), suggesting that instance-conditional prompts have a better potential in capturing more generalizable elements that are relevant for a recognition task. Theoretically, learning-based prompts have a much higher risk of overfitting base classes than manual prompts. Therefore, CLIP is a strong competitor to beat in unseen classes. Different from CoOp, we obtain promising results for CoCoOp: the new accuracy is even better than CLIP’s on 4 out of 11 datasets (i.e., ImageNet, OxfordPets, Food101 and SUN397) and not too far away from CLIP’s on the rest except FGVCAircraft where the gap between manual and learning-based prompts is generally large. In the ablation study on context length,

16821

Table 2. Comparison of prompt learning methods in the cross-dataset transfer setting. Prompts applied to the 10 target datasets are learned from ImageNet (16 images per class). Clearly, CoCoOp demonstrates better transferability than CoOp. ∆ denotes CoCoOp’s gain over CoOp.

Source

Target

ImageNet Caltech101 OxfordPets StanfordCars Flowers102 Food101 FGVCAircraft SUN397 DTD EuroSAT UCF101 Average

CoOp [62] CoCoOp
∆

71.51 71.02
-0.49

93.70 89.14 64.51 68.71 85.30 18.47 64.15 41.92 46.39 66.55 63.88 94.43 90.14 65.32 71.88 86.06 22.94 67.36 45.73 45.37 68.21 65.74
+0.73 +1.00 +0.81 +3.17 +0.76 +4.47 +3.21 +3.81 -1.02 +1.66 +1.86

Table 3. Comparison of manual and learning-based prompts in domain generalization. CoOp and CoCoOp use as training data 16 images from each of the 1,000 classes on ImageNet. In general, CoCoOp is more domain-generalizable than CoOp.

CLIP [40] CoOp [62] CoCoOp

Learnable?
✓ ✓

Source
ImageNet
66.73 71.51 71.02

ImageNetV2
60.83 64.20 64.07

Target

ImageNet-Sketch ImageNet-A

46.15 47.99 48.75

47.77 49.71 50.63

ImageNet-R
73.96 75.21 76.18

we find that FGVCAircraft benefits from longer context, which is aligned with the findings in Zhou et al. [62]. To close or even overturn the gaps between manual and learning-based prompts in unseen classes, more efforts are required and we hope the insights presented in this research can help the community tackle the generalizability issue in prompt learning.
4.2. Cross-Dataset Transfer
Having demonstrated CoCoOp’s generalizability within a dataset, we further show that CoCoOp has the potential to transfer beyond a single dataset, which is a much more challenging problem because the fundamentals can be totally changed across different datasets (e.g., from object recognition to texture classification). We only consider prompt learning methods in this setting.
We compare CoCoOp with CoOp by transferring context learned from ImageNet, with all 1,000 classes used, to each of the other 10 datasets. The results are detailed in Table 2. On the source dataset, the two models perform similarly. Whereas on the target datasets, CoCoOp mostly outperforms CoOp by a clear margin. Since the ImageNet classes mainly contain objects, as well as a fair amount of dog breeds, it is reasonable to see high accuracy for both models on the relevant target datasets including Caltech101 and OxfordPets.
By comparison, the performance on other datasets with distant—and more fine-grained or specialized—categories

is much lower, such as FGVCAircraft and DTD (containing various textures) where the accuracy numbers are well below 50%. Nonetheless, CoCoOp exhibits much stronger transferability than CoOp on the two mentioned datasets as well as on most other fine-grained or specialized datasets.
4.3. Domain Generalization
Generalization to out-of-distribution data is a capability essential for machine learning models to succeed in practical applications [47, 61]. Zhou et al. [62] have revealed that their learnable prompts are more robust than manual prompts to domain shift. We are also interested to know if instance-conditional prompts still maintain the advantages as in previous experiments.
Following Zhou et al. [62], we evaluate CoCoOp’s domain generalization performance by transferring the context learned from ImageNet to the four specially designed benchmarks. We also include the comparison with CLIP. Table 3 shows the results. Both prompt learning methods clearly beat CLIP on all target datasets. Compared to CoOp, CoCoOp performs slightly worse on ImageNetV2 but better on the other three. The results confirm that instanceconditional prompts are more domain-generalizable.
4.4. Further Analysis
Class-Incremental Test We consider a practical problem scenario where the recognition targets originally composed of base classes are expanded to include completely

16822

Table 4. Recognition accuracy (average over 11 datasets) on a combination of base and new classes. The learnable models only have access to training data from base classes.

CLIP [40] CoOp [62] CoCoOp

Learnable?
✓ ✓

Accuracy
65.22 65.55 69.13

(a) Ablation on initialization.

(b) Ablation on context length.

Figure 4. Ablation studies.

Table 5. CoCoOp (last row) vs a bigger CoOp on ImageNet.

Model

# params Base New H

CoOp (ctx=4)

2,048 76.47 67.88 71.92

CoOp (ctx=60)

30,720 76.16 65.34 70.34

CoOp (ctx=4) + Meta-Net 34,816 75.98 70.43 73.10

new classes. This problem is relevant to the existing continual learning literature [37] but different in that the model here does not have access to any training data from new classes and needs to perform zero-shot recognition on them. We compare CLIP, CoOp and CoCoOp using the 11 datasets. The average results are reported in Table 4. Clearly, CoOp loses competitiveness against CLIP as their performance is similar but the former needs training data. Again, CoCoOp beats the two competitors with a significant margin.
Initialization We compare word embeddings-based initialization with random initialization, which samples from a zero-mean Gaussian distribution with 0.02 standard deviation. Figure 4(a) suggests that a proper initialization is more beneficial to both the base and new classes.
Context Length Following Zhou et al. [62], we study 4, 8 and 16 context tokens. For fair comparison, we use random initialization for all context tokens. Figure 4(b) summarizes the results on the 11 datasets. The differences in the base classes are fairly small whereas in the new classes the models with a longer context length clearly perform better.
CoCoOp vs a Bigger CoOp Since CoCoOp introduces more parameters than CoOp, namely the Meta-Net, one

might question if the improvements simply come from an increased learning capacity. To clear the doubt, we remove the Meta-Net part and increase the number of context tokens in CoOp to the maximum such that CoOp’s and CoCoOp’s sizes are similar. The results in Table 5 show that increasing the parameter size is not the key.
5. Limitations
The first limitation is about training efficiency: CoCoOp is slow to train and would consume a significant amount of GPU memory if the batch size is set larger than one. The reason is because CoCoOp is based on an instanceconditional design that requires for each image an independent forward pass of instance-specific prompts through the text encoder. This is much less efficient than CoOp that only needs a single forward pass of prompts through the text encoder for an entire mini-batch of any size.
The second limitation is that on 7 out of the 11 datasets (see Table 1), CoCoOp’s performance in unseen classes still lags behind CLIP’s, indicating that more efforts are needed from the community to fully close or overturn the gaps between manual and learning-based prompts.
6. Discussion and Conclusion
Our research addresses an important issue that arises with the availability of large pre-trained AI models, i.e., how to adapt them to downstream applications. These models, also called foundation models [1], have received increasing attention from academia and industry in both the vision and NLP communities because they are so powerful in terms of their capabilities for diverse downstream tasks. However, foundation models are costly to pre-train in terms of data scale and compute resources; and typically contain an enormous number of parameters in order to develop sufficient capacity. For instance, the CLIP model [40] based on ViT-B/16 used in our experiments has a whopping 150M parameter size. These factors together highlight the need for research of efficient adaptation methods for democratizing foundation models.
Our studies, which follow the line of parameter-efficient prompt learning [62], provide timely insights into the generalizability issue of static prompts, and more importantly, demonstrate that a simple design based on conditional prompt learning performs superbly in a variety of problem scenarios, including generalization from base to new classes, cross-dataset prompt transfer, and domain generalization.
Acknowledgements This work is supported by NTU NAP, and under the RIE2020 Industry Alignment Fund – Industry Collaboration Projects (IAF-ICP) Funding Initiative, as well as cash and in-kind contribution from the industry partner(s).

16823

References
[1] Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258, 2021. 8
[2] Lukas Bossard, Matthieu Guillaumin, and Luc Van Gool. Food-101–mining discriminative components with random forests. In ECCV, 2014. 5
[3] Wei-Lun Chao, Soravit Changpinyo, Boqing Gong, and Fei Sha. An empirical study and analysis of generalized zeroshot learning for object recognition in the wild. In ECCV, 2016. 3
[4] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. In ICML, 2020. 3
[5] Mircea Cimpoi, Subhransu Maji, Iasonas Kokkinos, Sammy Mohamed, and Andrea Vedaldi. Describing textures in the wild. In CVPR, 2014. 5
[6] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In CVPR, 2009. 5
[7] Karan Desai and Justin Johnson. Virtex: Learning visual representations from textual annotations. In CVPR, 2021. 3
[8] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In NAACL, 2019. 3
[9] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. In ICLR, 2021. 4
[10] Mohamed Elhoseiny, Babak Saleh, and Ahmed Elgammal. Write a classifier: Zero-shot learning using purely textual descriptions. In ICCV, 2013. 3
[11] Li Fei-Fei, Rob Fergus, and Pietro Perona. Learning generative visual models from few training examples: An incremental bayesian approach tested on 101 object categories. In CVPR-W, 2004. 5
[12] Andrea Frome, Greg S Corrado, Jon Shlens, Samy Bengio, Jeff Dean, Marc’Aurelio Ranzato, and Tomas Mikolov. Devise: A deep visual-semantic embedding model. NeurIPS, 2013. 3
[13] Andreas Fu¨rst, Elisabeth Rumetshofer, Viet Tran, Hubert Ramsauer, Fei Tang, Johannes Lehner, David Kreil, Michael Kopp, Gu¨nter Klambauer, Angela Bitto-Nemling, et al. Cloob: Modern hopfield networks with infoloob outperform clip. arXiv preprint arXiv:2110.11316, 2021. 1, 3
[14] Peng Gao, Shijie Geng, Renrui Zhang, Teli Ma, Rongyao Fang, Yongfeng Zhang, Hongsheng Li, and Yu Qiao. Clip-adapter: Better vision-language models with feature adapters. arXiv preprint arXiv:2110.04544, 2021. 1
[15] Tianyu Gao, Adam Fisch, and Danqi Chen. Making pretrained language models better few-shot learners. arXiv preprint arXiv:2012.15723, 2020. 1
[16] Lluis Gomez, Yash Patel, Marc¸al Rusin˜ol, Dimosthenis Karatzas, and CV Jawahar. Self-supervised learning of

visual features through embedding images into text topic spaces. In CVPR, 2017. 3
[17] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning. In CVPR, 2020. 3
[18] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, 2016. 1, 4
[19] Patrick Helber, Benjamin Bischke, Andreas Dengel, and Damian Borth. Eurosat: A novel dataset and deep learning benchmark for land use and land cover classification. IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing, 2019. 5
[20] Olivier J. He´naff, Aravind Srinivas, Jeffrey De Fauw, Ali Razavi, Carl Doersch, S. M. Ali Eslami, and Aa¨ron van den Oord. Data-efficient image recognition with contrastive predictive coding. In ICML, 2020. 3
[21] Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo, Rahul Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, Dawn Song, Jacob Steinhardt, and Justin Gilmer. The many faces of robustness: A critical analysis of out-of-distribution generalization. ICCV, 2021. 5
[22] Dan Hendrycks, Kevin Zhao, Steven Basart, Jacob Steinhardt, and Dawn Song. Natural adversarial examples. In CVPR, 2021. 5
[23] Dat Huynh and Ehsan Elhamifar. Fine-grained generalized zero-shot learning via dense attribute-based attention. In CVPR, 2020. 3
[24] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc V Le, Yunhsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning with noisy text supervision. In ICML, 2021. 1, 2, 3
[25] Zhengbao Jiang, Frank F Xu, Jun Araki, and Graham Neubig. How can we know what language models know? ACL, 2020. 1, 3
[26] Armand Joulin, Laurens Van Der Maaten, Allan Jabri, and Nicolas Vasilache. Learning visual features from large weakly supervised data. In ECCV, 2016. 3
[27] Chen Ju, Tengda Han, Kunhao Zheng, Ya Zhang, and Weidi Xie. Prompting visual-language models for efficient video understanding. arXiv preprint arXiv:2112.04478, 2021. 3
[28] Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 3d object representations for fine-grained categorization. In ICCV-W, 2013. 5
[29] Jimmy Lei Ba, Kevin Swersky, Sanja Fidler, et al. Predicting deep zero-shot convolutional neural networks using textual descriptions. In ICCV, 2015. 3
[30] Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt tuning. arXiv preprint arXiv:2104.08691, 2021. 1, 3
[31] Ang Li, Allan Jabri, Armand Joulin, and Laurens van der Maaten. Learning visual n-grams from web data. In ICCV, 2017. 3
[32] Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation. arXiv preprint arXiv:2101.00190, 2021. 1, 3

16824

[33] Yangguang Li, Feng Liang, Lichen Zhao, Yufeng Cui, Wanli Ouyang, Jing Shao, Fengwei Yu, and Junjie Yan. Supervision exists everywhere: A data efficient contrastive language-image pre-training paradigm. arXiv preprint arXiv:2110.05208, 2021. 1, 3
[34] Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing. arXiv preprint arXiv:2107.13586, 2021. 1, 3
[35] Subhransu Maji, Esa Rahtu, Juho Kannala, Matthew Blaschko, and Andrea Vedaldi. Fine-grained visual classification of aircraft. arXiv preprint arXiv:1306.5151, 2013. 5
[36] Maria-Elena Nilsback and Andrew Zisserman. Automated flower classification over a large number of classes. In ICVGIP, 2008. 5
[37] German I Parisi, Ronald Kemker, Jose L Part, Christopher Kanan, and Stefan Wermter. Continual lifelong learning with neural networks: A review. Neural Networks, 2019. 8
[38] Omkar M Parkhi, Andrea Vedaldi, Andrew Zisserman, and CV Jawahar. Cats and dogs. In CVPR, 2012. 5
[39] Fabio Petroni, Tim Rockta¨schel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, Alexander H Miller, and Sebastian Riedel. Language models as knowledge bases? In EMNLP, 2019. 3
[40] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In ICML, 2021. 1, 2, 3, 4, 5, 7, 8
[41] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 2019. 3, 4
[42] Yongming Rao, Wenliang Zhao, Guangyi Chen, Yansong Tang, Zheng Zhu, Guan Huang, Jie Zhou, and Jiwen Lu. Denseclip: Language-guided dense prediction with contextaware prompting. In CVPR, 2022. 3
[43] Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do imagenet classifiers generalize to imagenet? In ICML, 2019. 5
[44] Taylor Shin, Yasaman Razeghi, Robert L Logan IV, Eric Wallace, and Sameer Singh. Autoprompt: Eliciting knowledge from language models with automatically generated prompts. In EMNLP, 2020. 1, 3
[45] Richard Socher, Milind Ganjoo, Hamsa Sridhar, Osbert Bastani, Christopher D Manning, and Andrew Y Ng. Zero-shot learning through cross-modal transfer. In NeurIPS, 2013. 3
[46] Khurram Soomro, Amir Roshan Zamir, and Mubarak Shah. Ucf101: A dataset of 101 human actions classes from videos in the wild. arXiv preprint arXiv:1212.0402, 2012. 5
[47] Rohan Taori, Achal Dave, Vaishaal Shankar, Nicholas Carlini, Benjamin Recht, and Ludwig Schmidt. Measuring robustness to natural distribution shifts in image classification. In NeurIPS, 2020. 7
[48] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia

Polosukhin. Attention is all you need. In NeurIPS, 2017. 1, 3, 4 [49] Oriol Vinyals, Alexander Toshev, Samy Bengio, and Dumitru Erhan. Show and tell: A neural image caption generator. In CVPR, 2015. 2 [50] Haohan Wang, Songwei Ge, Zachary Lipton, and Eric P Xing. Learning robust global representations by penalizing local predictive power. In NeurIPS, 2019. 5 [51] Wei Wang, Vincent W Zheng, Han Yu, and Chunyan Miao. A survey of zero-shot learning: Settings, methods, and applications. TIST, 2019. 3 [52] Xiaolong Wang, Yufei Ye, and Abhinav Gupta. Zero-shot recognition via semantic embeddings and knowledge graphs. In CVPR, 2018. 3 [53] Mitchell Wortsman, Gabriel Ilharco, Mike Li, Jong Wook Kim, Hannaneh Hajishirzi, Ali Farhadi, Hongseok Namkoong, and Ludwig Schmidt. Robust fine-tuning of zero-shot models. arXiv preprint arXiv:2109.01903, 2021. 1 [54] Yongqin Xian, Bernt Schiele, and Zeynep Akata. Zero-shot learning-the good, the bad and the ugly. In CVPR, 2017. 3, 5 [55] Jianxiong Xiao, James Hays, Krista A Ehinger, Aude Oliva, and Antonio Torralba. Sun database: Large-scale scene recognition from abbey to zoo. In CVPR, 2010. 2, 5 [56] Yuan Yao, Ao Zhang, Zhengyan Zhang, Zhiyuan Liu, TatSeng Chua, and Maosong Sun. Cpt: Colorful prompt tuning for pre-trained vision-language models. arXiv preprint arXiv:2109.11797, 2021. 1, 3 [57] Kai Yi, Xiaoqian Shen, Yunhao Gou, and Mohamed Elhoseiny. Exploring hierarchical graph representation for large-scale zero-shot image classification. arXiv preprint arXiv:2203.01386, 2022. 3 [58] Renrui Zhang, Ziyu Guo, Wei Zhang, Kunchang Li, Xupeng Miao, Bin Cui, Yu Qiao, Peng Gao, and Hongsheng Li. Pointclip: Point cloud understanding by clip. arXiv preprint arXiv:2112.02413, 2021. 3 [59] Yuhao Zhang, Hang Jiang, Yasuhide Miura, Christopher D Manning, and Curtis P Langlotz. Contrastive learning of medical visual representations from paired images and text. arXiv preprint arXiv:2010.00747, 2020. 2 [60] Zexuan Zhong, Dan Friedman, and Danqi Chen. Factual probing is [mask]: Learning vs. learning to recall. In NAACL, 2021. 1, 3 [61] Kaiyang Zhou, Ziwei Liu, Yu Qiao, Tao Xiang, and Chen Change Loy. Domain generalization in vision: A survey. arXiv preprint arXiv:2103.02503, 2021. 7 [62] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. Learning to prompt for vision-language models. arXiv preprint arXiv:2109.01134, 2021. 1, 2, 3, 4, 5, 7, 8

16825

