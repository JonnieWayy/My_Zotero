arXiv:2306.07967v1 [cs.LG] 13 Jun 2023

One-for-All: Generalized LoRA for Parameter-Efficient Fine-tuning
Arnav Chavan∗1,2, Zhuang Liu3, Deepak Gupta2, Eric Xing1,4, Zhiqiang Shen∗1 1MBZUAI 2Transmute AI Lab 3FAIR, Meta 4CMU
{arnav.chavan,eric.xing,zhiqiang.shen}@mbzuai.ac.ae zhuangl@meta.com, guptadeepak2806@gmail.com
Abstract
We present Generalized LoRA (GLoRA), an advanced approach for universal parameter-efficient fine-tuning tasks. Enhancing Low-Rank Adaptation (LoRA), GLoRA employs a generalized prompt module to optimize pre-trained model weights and adjust intermediate activations, providing more flexibility and capability across diverse tasks and datasets. Moreover, GLoRA facilitates efficient parameter adaptation by employing a scalable, modular, layer-wise structure search that learns individual adapter of each layer. Originating from a unified mathematical formulation, GLoRA exhibits strong transfer learning, few-shot learning and domain generalization abilities, as it adjusts to new tasks through additional dimensions on weights and activations. Comprehensive experiments demonstrate that GLoRA outperforms all previous methods in natural, specialized, and structured benchmarks, achieving superior accuracy with fewer parameters and computations on various datasets. Furthermore, our structural re-parameterization design ensures that GLoRA incurs no extra inference cost, rendering it a practical solution for resource-limited applications. Code is publicly available at: GitHub.
1 Introduction
The remarkable achievements of large-scale deep neural networks in recent years have revolutionized the field of artificial intelligence, demonstrating unprecedented performance across various tasks and domains. These highly complex models, often with millions or even billions of parameters, have demonstrated remarkable capabilities in areas such as computer vision [1], natural language understanding [2], and speech recognition [3]. Typically, these colossal models are pre-trained on general and large-scale datasets, such as ImageNet [4], and are subsequently adapted to downstream target scenarios through fine-tuning or transfer learning. Given the immense computational resources required by large pre-trained architectures, many parameter-efficient fine-tuning (PEFT) methods [5, 6, 7, 8, 9] have been proposed. For instance, Low-Rank Adaptation (LoRA) [5] aims to reduce the number of trainable parameters by exclusively learning pairs of rank-decomposition matrices whilst keeping the original model parameter static. Adapter [10] implements bottleneck adapter modules and incorporates a modest number of task-specific parameters into a fixed pre-trained model. Similarly, Visual Prompt Tuning (VPT) [7] introduces a minimal number of learnable parameters to the input of the Transformer, leaving the entire backbone frozen during fine-tuning.
However, distinct downstream datasets often possess unique characteristics, such as natural, specialized, and structured data, which differ significantly in distribution and composition. A static fine-tuning strategy may not sufficiently account for these disparities, thereby hindering its capacity to adapt to diverse datasets. To rectify this, we propose a flexible, parameter-efficient fine-tuning scheme in this work to manage the variations of multiple downstream datasets within a consolidated
∗Equal contribution. Project page: https://sites.google.com/view/generalized-lora.
Preprint. Under review.

formulation. Our approach presents a generalized version of LoRA from a unified parameter-efficient fine-tuning perspective, amplifying LoRA’s capability, scalability, and adaptability by rescaling and shifting intermediate activations, in conjunction with implementing a structural re-parameterization design, etc. It is challenging to devise a unified method that integrates all adjustable dimensions and possibilities when tuning a pre-trained network, especially in the case of transformer architectures, while our proposed approach presents a practicable solution to navigate this complexity.
Our approach exhibits the following advantages: (1) It concurrently takes into account multiple dimensions to enhance capability and flexibility during fine-tuning, encompassing weights, features, and input tokens. (2) It confers a significant edge over prior integration-based NOAH [8] and partial fine-tuning method PFT [6] in that, subsequent to an evolutionary search procedure for the ideal subnet or configuration, there is no requirement to retrain the subnet, in contrast to NOAH and PFT. Consequently, our results are derived directly from the supernet’s model weights without necessitating additional training. (3) It conducts an implicit search devoid of any manual hyperparameter tuning, thus justifying the increased training time. (4) It incurs no additional inference cost thanks to our structural re-parameterization architecture, whereby the extra fine-tuning parameters will be fused to the proximate projection weights post-training.
We conduct comprehensive experiments on VTAB-1K [11], ImageNet [4], and its variants [12, 13, 14, 15]. The VTAB-1K dataset comprises 19 heterogeneous vision datasets, enveloping a broad spectrum of visual domains that include natural objects and scenes, textures and shapes, satellite imagery, among others. GLoRA surpasses all previous state-of-the-art PEFT methods by a substantial margin in terms of average accuracy. Additionally, we evaluate the model’s few-shot learning capacity on five fine-grained visual recognition datasets, akin to prior works [8, 7], along with its ability for domain generalization and robustness on ImageNet-V2 [12], ImageNet-Sketch [13], ImageNet-A [14], and ImageNet-R [15] datasets. GLoRA significantly outperforms previous methods across all these benchmarks, without incurring any extra computational overhead during the inference phase.
Our contributions:
• We propose Generalized LoRA (GLoRA), a novel parameter-efficient fine-tuning framework. GLoRA enhances the low-rank adaptation approach with a more generalized prompt module design per layer, offering enhanced capability and flexibility in finetuning.
• We present a unified perspective of formulation for the parameter-efficient fine-tuning problem through investigating various previous approaches to this task. Our proposed GLoRA framework is constructed based on this formulation that can achieve all fine-tuning paradigms from a single formulation, i.e., a One-for-All 2 fine-tuning architecture.
• To evaluate the effectiveness of our proposed approach, we conduct comprehensive experiments on downstream fine-tuning, few-shot learning, and domain generalization and robustness using diverse datasets. Our experimental results demonstrate that GLoRA outperforms all previous methods on these benchmarks while requiring only a small number of extra parameters and no additional inference cost.
2 GLoRA
In this section, we start from providing a mathematical overview of existing state-of-the-art PEFT methods and discuss the advantages and disadvantages for them. Then, we introduce a unified formulation of integrating all existing SOTA PEFT methods and elaborate our proposed generalized LoRA in detail following this unified formulation perspective. After that, a structural re-parameterization design is presented to show the inference efficiency without additional cost. An evolutionary search for optimal layer-wise configurations is also introduced to achieve the goal of generalized LoRA. We further give the theoretical analysis and discussions on the higher capability of the proposed method.
2.1 Previous Solutions with Limitations
Visual Prompt Tuning [7]: VPT introduces a small amount of task-specific learnable parameters into the input space while freezing the entire pre-trained Transformer backbone during downstream
2One-for-All represents that one formulation can be transformed into various shapes of PEFT paradigms.
2

fine-tuning. It proposes two strategies: VPT-Shallow, i.e., only input space has the trainable prompt:

[x1, Z1, E1] = L1 ([x0, P, E0]) [xi, Zi, Ei] = Li ([xi−1, Zi−1, Ei−1])

(1)

where P is prompt, it is trainable and others are frozen. x is the [CLS] token, E are the image patches. Prompts use <1% trainable parameters as compared to the original model.

VPT-Deep, i.e., every layer has the trainable prompt. The formulation is:

[xi, . . . , Ei] = Li ([xi−1, Pi−1, Ei−1])

(2)

VTP-Deep outperforms full fine-tuning on majority of vision tasks and also has better accuracy in low data regime. VTP observes that when prompt append is larger than prompt add, adding prompts to the shallow layers is better than deep. However, VPT increases cost in the inference stage.

AdaptFormer [16]: AdaptFormer introduces a parallel learnable branch of two linear layers and ReLU over the MLP block and learns only this path while freezing other parts.

x˜ℓ = ReLU (LN (x′ℓ) · Wdown) · Wup

(3)

xℓ = MLP (LN (x′ℓ)) + s · x˜ℓ + x′ℓ

(4)

where x′ℓ are the tokens after MHSA at the ℓ-th layer. Wdown and Wup are a down-projection layer and an up-projection layer from the parallel branch, respectively. s is a scale factor.

LoRA [5]: LoRA proposes to freeze the pre-trained model weights and injects trainable low-rank decomposition matrices into each layer. It learns only the residual from pre-trained weight. SOTA performance is achieved if comparing to prompt learning, adapters, etc., on the GPT-2 model family. Assuming W0, b0, x are pre-trained weights, bias and input, let f be a linear layer, thus f (x) = W0x + b0. During fine-tuning, W0 and b0 are frozen, the learning process can be:

f (x) = W0x + ∆Wx + b0 = WLoRAx + b0

(5)

where ∆W is the low-rank decomposition weights that are learnable.

Scaling & Shifting Features (SSF) [17]: SSF modul scales and shifts features after every MLP,

MHSA, Layernorm module during training, and performs re-parameterization during inference as it

is a linear structure.

y = γ ⊙x+β

(6)

where y is the output features. γ and β are the scale and shift factors, ⊙ is the dot product. This method has no increase in inference but the capability is limited to feature adaptation.

FacT [18]: FacT proposes to use a tensorization-decomposition method to store the additional weight, the weights of the model are tensorized into a single 3D tensor, and their additions are then decomposed into lightweight factors. In fine-tuning, only the factors will be updated and stored.

f (x) = W0x + b0 + UΣVx = (W0 + UΣV) x + b0

(7)

where ∆W in LoRA is decomposed into U, V and Σ. This is Tensor-Train in FacT.

f (x) = W0x + b0 + UCPVx = (W0 + UCPV) x + b0

(8)

where ∆W in LoRA is decomposed into U, C, P and V. This is Tucker in FacT.

RepAdapter [9]: RepAdapter inserts lightweight networks into the pre-trained models, and the additional parameters will be re-parameterized to the nearby projection weights after training. Adding sequential (not parallel) adapter to both MHA and MLP, adapter is linear thus can be re-parameterized, and has two layers: downsampling dense FC layer to downsample inputs; upsampling downsampled features that are divided into group, and each group has an upsampling layer. The group of upsampling layers can be merged into a single sparse upsampling layer and can be re-parameterized directly into the original MLP/MHSA. The formulation can be:

f (x) = W0 (x + Wu (Wdx + bd) + bu) + b0 = (W0 + W0WuWd) x + W0Wubd + W0bu + b0

(9)

where Wu, Wd, bu and bb are learnable weights and biases, respectively.

3

Limitations: In general, many existing PETL methods such as (VPT, Adapter) increase the inference time since the proposed structure cannot be re-parameterized. Direct prompt tuning is also hard to design as it brings in computational burden and requires hyper-parameter tuning i.e., how and where to place prompts. LoRA can be re-parameterized at inference but it doesn’t scale up for larger matrices and the adaptation ability is constrained on weight space. SSF / Repadaptor cannot learn the wieght change i.e., ∆W in weight space, whereas LoRA / FacT cannot efficiently learn the scaling and shifting of feature change i.e., ∆H in features space. Both feature and weight space need flexibility while performing transfer learning from a large model. Our proposed idea in this work attempts at: ∆W tuning, ∆H tuning, along with W and H scale and shift learning.

2.2 A Unified Formulation of One-for-All

For model fine-tuning, we propose a unified formulation that encompasses all tunable dimensions, including but not limited to the weight space and feature space. Additionally, we adopt

+

'(&) Output
+ +

a re-parameterization strategy to incorporate auxiliary parameters into the adjacent projec-

×

!!

×

+

+

"!

+

tion weights during the inference stage. Broadly speaking, our method serves as a superset of

×

!

"

#

$×

%

all prior solutions, i.e., one-for-all mechanism. ×

By setting different support tensors to zero, our

&

×

×

*

GLoRA can be reduced to any of these predeces-

Input

sor methods. Unlike NOAH [8], our architecture Figure 1: Schematic representation of a linear layer can be succinctly articulated as a unified mathe- adapted with GLoRA. matical equation. The consolidated formulation

to represent all tunable spaces can be represented as follows:

f (x) = (W0 + W0A + B) x + CW0 + Db0 + E + b0

(10)

where A, B, C, D, E are the trainable support tensors for downstream tasks in our GLoRA, W0 and b0 are frozen during whole fine-tuning. A is utilized to scale the weight. B has the role to scale the input and shift the weight. C is the layer-wise prompt serving a similar function of VPT-Deep, D
and E are used to scale and shift the bias, respectively. A detailed illustration is shown in Figure 1.

2.3 Prompt Modules

In this subsection, we delineate the methodology for designing layer-wise adaptors or prompt modules for A, B, C, D, E. In a broad sense, these can take the form of scalars, vectors, low-rank decompositions, or none. Based on the role of these trainable support tensors, they can be classified as follows:

A = {LoRA, vector, scalar, none}

B = {LoRA, vector, scalar, none}

C = {LoRA, vector, none}

(11)

D = {vector, scalar, none}

E = {vector, scalar, none}

where none indicates zero, if all the trainable support tensors are zero, the model will be degraded to the original formulation and training recipe. In particular, suppose W0 ∈ Rd×d, we define Ad ∈ Rd×r, Au ∈ Rr×d, Bd ∈ Rd×r, Bu ∈ Rr×d, Cd ∈ Rd×r, Cu ∈ Rr×1, D ∈ Rd×1 and E ∈ Rd×1. Depending upon the current subnet configuration, in case of LoRA with rank r1 < r, Ard1 ∈ Rd×r1 , Aru1 ∈ Rr1×d is indexed from Ad and Au respectively; and A = Ard1 × Aru1 is used as the final tensor, in case of vector A ∈ Rd×1 is indexed from Ad and in case of scalar A ∈ R1×1 is indexed from Ad. A similar strategy is followed for all other support tensors depending upon the current sampled configuration in the subnet. This weight entanglement strategy helps to increase
the search space without increasing the number of parameters substantially and also shows faster
convergence due to weight sharing in different subnets.

4

2.4 Structural Re-parameterization Design and Inference Efficiency Analysis

The fundamental aspect enabling re-parameterization is the elimination of non-linearity amidst adjacent transformations, thereby permitting the absorption of supplementary parameters into the preceding ones. As mentioned in RepAdapter [9], the removal of such non-linear layers does not detrimentally impact the performance of the networks. The precise concept of GLoRA reparameterization is explicated as follows:

f (x) = Wunix + buni

(12)

where Wuni and buni are our final unified trained weight and bias in GLoRA. They are reparameterized according to Eq 10:

Wuni = W0 + W0A + B

(13)

buni = CW0 + Db0 + E + b0

(14)

As a result, the re-parameterization strategy we employ, which integrates learnable parameters into the neighboring projection weights, can be advantageous as it incurs no additional computational cost during the inference phase.

2.5 Evolutionary Search for Optimal Layer-wise Configurations
Our design for a unified adaptor is implemented on a per-layer basis, thus allowing for heterogeneity across different layers. To identify the optimal configuration for each layer, we employ the evolutionary search method [8, 6], which offers a balance of efficiency and effectiveness. Although the training time may increase due to this search process, it is important to note that existing work [8] necessitate an extensive hyperparameter search (such as low-rank in LoRA and FacT, as well as position and size of adapter modules in Adapter [10], dimension and structure configuration in RepAdapter [9], among others). In particular, we make use of a weight sharing strategy where a single large matrix is defined for each support tensor, and depending upon the component (LoRA, vector, scalar, or none), a submatrix is indexed and applied for the current training iteration. This allows better parameter efficiency since the maximal weight sharing is done in the subnets. Contrarily, our unified prompt module design conducts an implicit search that eliminates the need for manual hyperparameter tuning. Therefore, any augmentation in training time is reasonable and well-justified.

2.6 GLoRA with Higher Capacity
Model capacity refers to the capability of a model to approximate a diverse range of functions. A method for regulating the capacity of a learning algorithm involves selecting an appropriate hypothesis space, essentially a set of functions that the learning algorithm is permitted to consider as potential solutions. The Vapnik-Chervonenkis Dimension (VC Dimension) [19], a measure of the capacity and complexity of a statistical algorithm, can be leveraged to provide a formal evidence for this assertion.
Theorem 1 Suppose dvc(H) is the VC dimension of the hypothesis H. If Hi ⊆ Huni,
dvc(Huni) − dvc(Hi) ≥ ϵ s.t. ϵ ≥ 0
The validity of this theorem stems from the inherent property of our problem context, where the hypothesis space Hi is a subset of Huni in our context. Huni encompasses all possible shattered scenarios of Hi. For the extreme case where the VC dimension dvc(Ho) (Ho is the difference set of Huni and Hi) is 0, the error ϵ will be zero. As per learning theory, a higher VC dimension implies greater model flexibility and capability of our approach.

3 Experiments
Datasets. We thoroughly evaluate GLoRA on VTAB-1K [11] benchmark at different parameter counts. VTAB-1K comprises of 19 image classification tasks. Tasks are clustered into three domains: i) Natural images ii) Specialized tasks consisting of remote sensing and medical datasets; and iii) Structured tasks focusing on scene structure understanding, like depth prediction and orientation prediction, among others. To test the few-shot fine-tuning performance, we evaluate GLoRA on five

5

Table 1: Full results on VTAB-1K benchmark. “# params” specifies the number of trainable parameters in backbones. Average accuracy and # params are averaged over group-wise mean values.

Natural

Specialized

Structured

# param (M) Inference Cost
Cifar100 Caltech101
DTD Flower102
Pets SVHN Sun397 Camelyon EuroSAT Resisc45 Retinopathy Clevr-Count Clevr-Dist DMLab KITTI-Dist dSpr-Loc dSpr-Ori sNORB-Azim sNORB-Ele Average

Traditional Finetuning

Full Linear

85.8 0-

PETL methods

BitFit [28] 0.10 -

VPT-Shall. [7] 0.06 ↑

VPT-Deep [7] 0.53 ↑

Adapter [10] 0.16 ↑

AdaptForm. [16] 0.16 ↑

LoRA [5]

0.29 -

NOAH [8] 0.36 ↑

FacT [18]

0.07 -

SSF [17]

0.24 -

RepAdapter [9] 0.22 -

GLoRA GLoRA GLoRA

0.86 0.44 0.29 -

68.9 87.7 64.3 97.2 86.9 87.4 38.8 79.7 95.7 84.2 73.9 56.3 58.6 41.7 65.5 57.5 46.7 25.7 29.1 68.9 64.4 85.0 63.2 97.0 86.3 36.6 51.0 78.5 87.5 68.5 74.0 34.3 30.6 33.2 55.4 12.5 20.0 9.6 19.2 57.6
72.8 87.0 59.2 97.5 85.3 59.9 51.4 78.7 91.6 72.9 69.8 61.5 55.6 32.4 55.9 66.6 40.0 15.7 25.1 65.2 77.7 86.9 62.6 97.5 87.3 74.5 51.2 78.2 92.0 75.6 72.9 50.5 58.6 40.5 67.1 68.7 36.1 20.2 34.1 67.8 78.8 90.8 65.8 98.0 88.3 78.1 49.6 81.8 96.1 83.4 68.4 68.5 60.0 46.5 72.8 73.6 47.9 32.9 37.8 72.0 69.2 90.1 68.0 98.8 89.9 82.8 54.3 84.0 94.9 81.9 75.5 80.9 65.3 48.6 78.3 74.8 48.5 29.9 41.6 73.9 70.8 91.2 70.5 99.1 90.9 86.6 54.8 83.0 95.8 84.4 76.3 81.9 64.3 49.3 80.3 76.3 45.7 31.7 41.1 74.7 67.1 91.4 69.4 98.8 90.4 85.3 54.0 84.9 95.3 84.4 73.6 82.9 69.2 49.8 78.5 75.7 47.1 31.0 44.0 74.5 69.6 92.7 70.2 99.1 90.4 86.1 53.7 84.4 95.4 83.9 75.8 82.8 68.9 49.9 81.7 81.8 48.3 32.8 44.2 75.5 70.6 90.6 70.8 99.1 90.7 88.6 54.1 84.8 96.2 84.5 75.7 82.6 68.2 49.8 80.7 80.8 47.4 33.2 43.0 75.6 69.0 92.6 75.1 99.4 91.8 90.2 52.9 87.4 95.9 87.4 75.5 75.9 62.3 53.3 80.6 77.3 54.9 29.5 37.9 75.7 72.4 91.6 71.0 99.2 91.4 90.7 55.1 85.3 95.9 84.6 75.9 82.3 68.0 50.4 79.9 80.4 49.2 38.6 41.0 76.1
76.4 92.9 74.6 99.6 92.5 91.5 57.8 87.3 96.8 88.0 76.0 83.1 67.3 54.5 86.2 83.8 52.9 37.0 41.4 78.0 76.5 92.3 75.2 99.6 92.3 91.2 57.5 87.3 96.7 88.1 76.1 80.6 67.2 53.4 84.5 83.5 52.8 35.2 40.8 77.6 76.1 92.7 75.3 99.6 92.4 90.5 57.2 87.5 96.7 88.1 76.1 81.0 66.2 52.4 84.9 81.8 53.3 33.3 39.8 77.3

fine-grained visual recognition few-shot datasets: Food101 [20], OxfordFlowers102 [21], StandfordCars [22], OxfordPets [23], and FGVCAircraft [24]. Following previous work [18], we evaluate 1, 2, 4, 8, and 16 shot settings. Finally, to show the domain generalization capabilities of GLoRA, we train GLoRA on ImageNet [25] under a 16-shot setting and test on ImageNetV2 [12], ImageNet-Sketch [13], ImageNet-A [14], and ImageNet-R [15].
Network Architecture and Implementation Details. For all our experiments, we employ ViT-B [1] pre-trained on ImageNet-21K as the base model. ViT-B is a 12-block deep transformer model with Multi Head Self-Attention (MHSA) and Multi-Layer Perceptron (MLP) modules present in each block. MHSA module has two linear layers; Q-K-V linear transformation layer and Output Projection layer, similarly, the MLP module has two linear layers. Thus each block has a total of four linear layers. For optimal flexibility, we implement GLoRA on all 4 layers across the 12 blocks, accounting for 48 GLoRA-adapted linear layers in the final model. It is important to note that GLoRA can be added to any linear layer in any network architecture.
Our method requires minimal hyperparameter tuning (except optimizer hyperparameters) due to the inherent search mechanism. Based on the previous works [26, 16, 8], we use AdamW [27] optimizer in all our experiments, weight decay is fixed at 1e-4 and the learning rate is either 1e-4 for larger datasets or 5e-4 for smaller datasets. Dataset-specific learning rates are provided in the supplementary material. We train our supernets for 500 epochs with a fixed batch size of 64 and a cosine learning rate scheduler. It is important to note that irrespective of the dataset, this exact policy works well in all settings. Also, we don’t re-train the weights after the search process and directly inherit the supernet weights for evaluation. Thus all supernets of various datasets are only trained for 500 epochs in total. The comparison of search / training time cost with other search-based methods is provided in the appendix.
3.1 VTAB-1K Dataset
We train three different GLoRA supernet configurations to vary the number of trainable parameters. The difference in these is only the LoRA dimensions in the search space which varies from 8 and 4 in the largest model, 4 and 2 in the intermediate model, and 2 in the smallest model. This added parameter flexibility in our method allows for user-defined trainable parameter count in the final models. Results on the VTAB-1k benchmark are shown in Table 1. We push the state-of-the-art in parameter-efficient transfer learning by up to 2.9%, Even our smallest model outperforms all existing methods by a substantial margin. It is worth noting that GLoRA performs competitively across datasets in contrast to all existing works which fail on at least one dataset; proving GLoRA’s high generalization capabilities. GLoRA pushes the state of the art in as many as 14 out of 19 datasets in the VTAB-1k benchmark while performing competitively on the remaining datasets too.
6

Accuracy (%)

Accuracy (%)

80

Average

70

60

50

GLoRA(0.28M)

40

NOAH(0.36M) VPT(0.64M)

LoRA(0.29M)

30

Adapter(0.16M)

1 N2umbe4r of trainin8g samples per class16

OxfordPets

90

85

80

75

70

GLoRA(0.29M)

65

NOAH(0.36M) VPT(0.64M)

60

LoRA(0.29M) Adapter(0.16M)

1 N2umbe4r of trainin8g samples per class16

Accuracy (%)

Accuracy (%)

StanfordCars

70

60

50

40

30

GLoRA(0.27M)

20

NOAH(0.36M) VPT(0.64M)

10

LoRA(0.29M) Adapter(0.16M)

1 N2umbe4r of trainin8g samples per class16

Flowers102 100

95

90

85

80

75 70

GLoRA(0.27M) NOAH(0.36M)

65

VPT(0.64M) LoRA(0.29M)

60

Adapter(0.16M)

1 N2umbe4r of trainin8g samples per class16

Accuracy (%)

Accuracy (%)

FGVCAircraft 50

40

30

20

GLoRA(0.27M) NOAH(0.36M)

VPT(0.64M)

10

LoRA(0.29M) Adapter(0.16M)

1 N2umbe4r of trainin8g samples per class16

Food101

70 60

50

40

GLoRA(0.28M) NOAH(0.36M)

30

VPT(0.64M) LoRA(0.29M)

Adapter(0.16M)

1 N2umbe4r of trainin8g samples per class16

Figure 2: Results on few-shot learning datasets. The baseline methods include Adapter, LoRA, VPT, NOAH. GLoRA consistently performs better across five datasets and a varying number of training examples per class.

Table 2: Results on domain generalization. GLoRA is significantly better than the existing methods.

Source

Target

ImageNet -Sketch -V2 -A -R

Adapter [10]

70.5

VPT [7]

70.5

LoRA [5]

70.8

NOAH [8]

71.5

GLoRA (0.29M) 78.3

16.4 59.1 5.5 22.1 18.3 58.0 4.6 23.2 20.0 59.3 6.9 23.3 24.8 66.1 11.9 28.5 30.6 67.5 13.3 31.0

3.2 Few-shot Learning
To extend the evaluation of GLoRA under conditions of limited data availability, we present the performance of GLoRA on fine-grained visual recognition datasets as the few-show learning, comparing it with LoRA, Adapter, VPT, and NOAH. The results at 1, 2, 4, 8, and 16 shots are illustrated in Figure 5. GLoRA demonstrates superior efficacy across the majority of the few-shot learning datasets, consistently outperforming the performance of existing methods by a large margin with similar parameter counts. Interestingly, on the Flowers102 dataset, all methods yield similar accuracy levels, attributable to the already exceptional overall performance. On the Food101 dataset, the average accuracy of GLoRA is on par with NOAH. From the first subgifure we can observe, the average performance boost becomes more pronounced at higher shot scenarios, nevertheless, even at lower shot settings, the gains remain significant.
3.3 Domain Generalization
The capacity of out-of-domain generalization holds significant value for large-scale neural networks [29]. Models fine-tuned via PETL methods should exhibit enhanced domain generalization aptitude, thereby making them more applicable in real-world scenarios. We demonstrate the out-of-domain generalization capabilities of GLoRA in Table 2, where a single ImageNet-1K [25] fine-tuned
7

Number of Parameters(M)

0.30

Natural

0.25

Specialized Structured

0.20

0.15

0.10

0.05

0.00 Q-K-V ProjecLtiaoyner TypeFC1

FC2

Figure 3: Distribution of GLoRA (0.86M) parameters across layer types on VTAB-1K dataset. Q-K-V and Projection are linear layers in MHSA module and FC1 and FC2 are linear layers in MLP module.

Table 3: Inference efficiency comparison of GLoRA with existing methods.

Method
Full tuning VPT [7]
Adapter [10] AdaptFormer [16]
NOAH [8] LoRA [5] GLoRA

↑ #Param(M)
0 0.55 0.16 0.16 0.12
0

↑ FLOPs(G)
0 5.60 0.03 0.03 0.02
0

Latency (imgs/sec) bs = 1 bs = 4 bs = 16 91.5 375.7 539.5 86.1 283.5 381.5 70.9 306.6 504.7 71.4 309.9 508.1 72.1 312.7 492.9
91.5 375.7 539.6

GLoRA model is subjected to testing on out-of-domain datasets. Aligning with preceding research, we limit the number of training examples per class to 16 for this experiment. It’s noteworthy that the performance for the fully-scaled ImageNet-1K fine-tuned model stands at 83.97% [1], and our approach manages to narrow this performance gap, even within a 16-shot setting (78.3%), thereby exhibiting superior few-shot learning on ImageNet-level datasets. Furthermore, the out-of-domain performance also witnesses a substantial boost in comparison to existing methods. When compared with LoRA, GLoRA enhances out-of-domain performance by as much as 100% (ImageNet-A) and 50% (ImageNet-Sketch).
3.4 Analysis and Discussion
Computational Cost. We show the final inference cost of various PEFT methods in Table 3, computed on an NVIDIA 3090 GPU. It is in this context that GLoRA significantly outperforms other methods, as GLoRA benefits from zero parameter or FLOPs overhead during the inference process. An ancillary advantage is the expedited adaptability in real-world scenarios where previous models are already deployed. The weights of GLoRA can be directly loaded without necessitating any manual system modifications. As previously mentioned, GLoRA supports VPT-Deep level prompts via the support tensor C, however, it does not impose any computational overhead due to its complete structural re-parameterization design.
Visualizations of searched fine-tuning strategy for each layer. Figure 3 visually shows the distribution of trainable parameters across the four types of linear layers embodied in ViT-B. Notably, the projection layer possesses the minimum quantity of trainable parameters spanning across VTAB1K categories. Generally, the MLP module hosts a substantially higher number of parameters compared to the MHSA. As anticipated, the structured group necessitates a greater number of parameters for adaptation due to a pronounced domain drift relative to ImageNet-1K [25]. Figure 4 illustrates the layerwise configuration of the support tensors as searched by the GLoRA algorithm. Each support tensor at every layer can potentially undergo 72 distinct adaptations across datasets. Support tensors D and E exhibit relatively low adaptation due to the prevalence of none adaptations,
8

Number of Occurences (Total=72)

70

LoRA

60

vector constant

50

40

30

20

10

0

A B CDE A B CDE A B CDE A B CDE A B CDLEayAerBDCeDpEth A(TBoCtaDl=E12A)B CDE A B CDE A B CDE A B CDE A B CDE

Figure 4: Layerwise configuration of support tensors of GLoRA (0.86M) on VTAB-1K dataset.

whereas A and B demonstrate a higher number of adaptations, though without a discernible pattern regarding the type of adaptation. It’s important to underscore that even a basic scalar can function effectively as a support tensor, enabling GLoRA to maintain superior parameter efficiency despite adapting every linear layer.
Our Limitations. A primary limitation of GLoRA is the evolutionary search procedure, which increases the total training duration. However, it is fairly justified since GLoRA requires negligible hyperparameter tuning and no manual architecture design. Furthermore, owing to its inherent mechanism, GLoRA can be effectively deployed for downstream tasks in both computer vision and natural language processing domains, although these applications are not investigated in this paper. Another facet of GLoRA is its adaptability to convolutional layers, an aspect not detailed in this paper. However, this should be straightforward to implement, following the principle of Equation 10.
4 Related Work
Due to the exponential growth in model size, the field of NLP proposed many methods for parameter efficient fine-tuning (PEFT) of large language models (LLMs) [30, 31, 32, 33, 34, 35, 28, 10]. The effectiveness of parameter-efficient fine-tuning has been proven in a wide range of natural language processing tasks [36]. With the advent growth in the size of vision models [26, 37], methods specifically focused on image models have also been put forward [18, 17, 16, 9, 8, 7, 38]. Among these methods, LoRA [32] has proven to transfer well across modalities and tasks. This is partly due to the simplistic design strategy of LoRA which directly works over weight tensors, irrespective of model type or configuration. Additionally, unlike Adapters [10, 16] and Prompt tuning [7], LoRA doesn’t add any additional inference parameters or latency due to structural re-parameterization (SR) design. RepAdapter [9] and SSF [17] also propose a SR design for PEFT, however RepAdapter is specific to model architectures and required manual designing for different layer configurations; SSF provides a very simple baseline but suffers from low flexibility and capability due to adaptation limited in the activation space. FacT [18] further decomposes LoRA matrices for better parameter efficiency, but we argue that <1M parameter scale is fairly efficient for fine-tuning on a single GPU. Thus, due to the advantages of LoRA over other related works, it is of importance to increase the flexibility, scalabilty and adaptabilty of LoRA.
5 Conclusion
We have presented GLoRA, a generalized parameter-efficient fine-tuning approach that has successfully demonstrated the effectiveness in enhancing the fine-tuning and transfer learning ability for the large-scale pre-trained models. By adopting a generalized low-rank adaptation and re-parameterization framework, GLoRA significantly reduces the number of parameters and computation required for fine-tuning, making it a more resource-efficient and practical method for real-world applications. The experiments conducted on a diverse range of tasks and datasets have substantiated the superiority of GLoRA over existing PEFT techniques, showcasing its scalability and adaptability.
9

Moreover, the ablation studies have provided valuable insights into the inner workings and the relative importance of different GLoRA components. This work not only contributes to the improvement of the fine-tuning process for large-scale pre-trained models but also opens up new avenues for future work, including further exploration of generalized low-rank adaptation techniques, the development of hybrid approaches, and the refinement of search and optimization algorithms. These areas of research may continue to expand the accessibility and efficiency of transfer learning across a broader range of applications.
References
[1] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations, 2021.
[2] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.
[3] Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever. Robust speech recognition via large-scale weak supervision. arXiv preprint arXiv:2212.04356, 2022.
[4] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A largescale hierarchical image database. In 2009 IEEE Conference on Computer Vision and Pattern Recognition, 2009.
[5] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021.
[6] Zhiqiang Shen, Zechun Liu, Jie Qin, Marios Savvides, and Kwang-Ting Cheng. Partial is better than all: revisiting fine-tuning strategy for few-shot learning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 9594–9602, 2021.
[7] Menglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie, Serge Belongie, Bharath Hariharan, and Ser-Nam Lim. Visual prompt tuning. In Computer Vision–ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23–27, 2022, Proceedings, Part XXXIII, pages 709–727. Springer, 2022.
[8] Yuanhan Zhang, Kaiyang Zhou, and Ziwei Liu. Neural prompt search. arXiv preprint arXiv:2206.04673, 2022.
[9] Gen Luo, Minglang Huang, Yiyi Zhou, Xiaoshuai Sun, Guannan Jiang, Zhiyu Wang, and Rongrong Ji. Towards efficient visual adaption via structural re-parameterization. arXiv preprint arXiv:2302.08106, 2023.
[10] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning for nlp. In International Conference on Machine Learning, pages 2790–2799. PMLR, 2019.
[11] Xiaohua Zhai, Joan Puigcerver, Alexander Kolesnikov, Pierre Ruyssen, Carlos Riquelme, Mario Lucic, Josip Djolonga, Andre Susano Pinto, Maxim Neumann, Alexey Dosovitskiy, Lucas Beyer, Olivier Bachem, Michael Tschannen, Marcin Michalski, Olivier Bousquet, Sylvain Gelly, and Neil Houlsby. The visual task adaptation benchmark, 2020.
[12] Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do imagenet classifiers generalize to imagenet? In International Conference on Machine Learning (ICML), pages 5389–5400. PMLR, 2019.
[13] Haohan Wang, Songwei Ge, Zachary Lipton, and Eric P Xing. Learning robust global representations by penalizing local predictive power. Advances in Neural Information Processing Systems (NeuIPS), 32, 2019.
[14] Dan Hendrycks, Kevin Zhao, Steven Basart, Jacob Steinhardt, and Dawn Song. Natural adversarial examples. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 15262–15271, 2021.
10

[15] Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo, Rahul Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, et al. The many faces of robustness: A critical analysis of out-of-distribution generalization. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 8340–8349, 2021.
[16] Shoufa Chen, Chongjian Ge, Zhan Tong, Jiangliu Wang, Yibing Song, Jue Wang, and Ping Luo. Adaptformer: Adapting vision transformers for scalable visual recognition. arXiv preprint arXiv:2205.13535, 2022.
[17] Dongze Lian, Daquan Zhou, Jiashi Feng, and Xinchao Wang. Scaling & shifting your features: A new baseline for efficient model tuning. arXiv preprint arXiv:2210.08823, 2022.
[18] Shibo Jie and Zhi-Hong Deng. Fact: Factor-tuning for lightweight adaptation on vision transformer. arXiv preprint arXiv:2212.03145, 2022.
[19] Vladimir N Vapnik and A Ya Chervonenkis. On the uniform convergence of relative frequencies of events to their probabilities. Measures of complexity: festschrift for alexey chervonenkis, pages 11–30, 2015.
[20] Lukas Bossard, Matthieu Guillaumin, and Luc Van Gool. Food-101 – mining discriminative components with random forests. In European Conference on Computer Vision, 2014.
[21] M-E Nilsback and Andrew Zisserman. A visual vocabulary for flower classification. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), volume 2, pages 1447–1454. IEEE, 2006.
[22] Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 3d object representations for finegrained categorization. In Proceedings of the IEEE international conference on computer vision workshops, pages 554–561, 2013.
[23] Omkar M Parkhi, Andrea Vedaldi, Andrew Zisserman, and CV Jawahar. Cats and dogs. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 3498–3505. IEEE, 2012.
[24] Subhransu Maji, Esa Rahtu, Juho Kannala, Matthew Blaschko, and Andrea Vedaldi. Finegrained visual classification of aircraft. arXiv preprint arXiv:1306.5151, 2013.
[25] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 248–255. Ieee, 2009.
[26] Mostafa Dehghani, Josip Djolonga, Basil Mustafa, Piotr Padlewski, Jonathan Heek, Justin Gilmer, Andreas Steiner, Mathilde Caron, Robert Geirhos, Ibrahim Alabdulmohsin, et al. Scaling vision transformers to 22 billion parameters. arXiv preprint arXiv:2302.05442, 2023.
[27] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Conference on Learning Representations.
[28] Elad Ben Zaken, Yoav Goldberg, and Shauli Ravfogel. Bitfit: Simple parameter-efficient fine-tuning for transformer-based masked language-models. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 1–9, 2022.
[29] Kaiyang Zhou, Ziwei Liu, Yu Qiao, Tao Xiang, and Chen Change Loy. Domain generalization in vision: A survey. arXiv preprint arXiv:2103.02503, 2021.
[30] Xiao Liu, Kaixuan Ji, Yicheng Fu, Zhengxiao Du, Zhilin Yang, and Jie Tang. P-tuning v2: Prompt tuning can be comparable to fine-tuning universally across scales and tasks. CoRR, abs/2110.07602, 2021.
[31] Qingru Zhang, Minshuo Chen, Alexander Bukharin, Pengcheng He, Yu Cheng, Weizhu Chen, and Tuo Zhao. Adaptive budget allocation for parameter-efficient fine-tuning. In The Eleventh International Conference on Learning Representations.
[32] Edward J Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. In International Conference on Learning Representations.
[33] Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yujie Qian, Zhilin Yang, and Jie Tang. Gpt understands, too, 2021.
11

[34] Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 4582–4597, Online, August 2021. Association for Computational Linguistics.
[35] Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt tuning. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 3045–3059, 2021.
[36] Zihao Fu, Haoran Yang, Anthony Man-Cho So, Wai Lam, Lidong Bing, and Nigel Collier. On the effectiveness of parameter-efficient fine-tuning. arXiv preprint arXiv:2211.15583, 2022.
[37] Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan Puigcerver, Jessica Yung, Sylvain Gelly, and Neil Houlsby. Big transfer (bit): General visual representation learning. In Computer Vision– ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part V 16, pages 491–507. Springer, 2020.
[38] Haoyu He, Jianfei Cai, Jing Zhang, Dacheng Tao, and Bohan Zhuang. Sensitivity-aware visual parameter-efficient tuning, 2023.
[39] Minghao Chen, Houwen Peng, Jianlong Fu, and Haibin Ling. Autoformer: Searching transformers for visual recognition. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 12270–12280, October 2021.
[40] Laurens Van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of machine learning research, 9(11), 2008.
Appendix
A Hyperparameters
For the hyperparameter search, we primarily concentrate on the exploration of the learning rate for supernet training, limiting our search scope to two potential alternatives: 1e-4 and 5e-4. For a detailed account of dataset-specific learning rates, please refer to Table 4. All other training particulars strictly adhere to the exact training policy delineated in the works of [18, 9].
Table 4: Dataset specific supernet training learning rate for VTAB-1K datastet.

Cifar100 Caltech101 DTD Flowers102 Pets SVHN Sun397 Camelyon EuroSAT Resisc45 Retinopathy Clevr-Count Clevr-Dist DMLab KITTI-Dist dSpr-Loc dSpr-Ori sNORB-Azim sNORB-Ele

Dataset

LR

5e-4 5e-4 5e-4 5e-4 5e-4 5e-4 5e-4 5e-4 5e-4 5e-4 1e-4 1e-4 1e-4 5e-4 5e-4 5e-4 5e-4 5e-4 1e-4

B Hierarchical Transformer
We show the performance of GLoRA on the Swin-B backbone in Table 5. We follow a dataset-specific learning rate search similar to ViT-B and also add GLoRA to the reduction linear layer in Swin architecture to maintain uniformity and avoid architecture-specific tuning. GLoRA can adapt to any layer irrespective of architecture configuration and perform well across tasks and datasets which can be clearly seen in Table 5 where GLoRA outperforms all existing works by a fair margin.
C Training Time
Our GLoRA, being a search-based approach for PEFT, naturally incurs increased training time due to the requirements of supernet training and evolutionary search. It is, however, critical to underscore that all current methods necessitate a manual search for design choices, as evidenced in Table 6. This necessity significantly inflates the total training time for a specific dataset, due to the broad search within these design choices. GLoRA streamlines this process through an automated evolutionary search mechanism, thus leveraging the benefit of an expansive search space.
12

Table 5: Performance on VTAB-1K benchmark with Swin-B pre-trained on ImageNet-21K as the backbone.

Method
Full Linear BitFit VPT FacT RepAdapter GLoRA

Natural
79.2 73.5 74.2 76.8 82.7 83.1 83.7

Specialized
86.2 80.8 80.1 84.5 87.5 86.9 88.7

Structured
59.7 33.5 42.4 53.4 62.0 62.1 61.9

Average
75.0 62.6 65.6 71.6 77.4 77.4 78.1

Table 6: Manual design choices in existing approaches.

Method
VPT AdaptFormer
NOAH RepAdapter
FacT GLoRA

Design Choices/Hyperparameters
Prompt Length, Prompt Location, Prompt Depth Adapter Location, Scaling Factor, Hidden dimension, Insertion Form
VPT choices, Adapter choices, LoRA rank Adapter Location, Number of groups, Hidden dimension, Adapter variants
Decomposition method, Scaling factor, Decomposition Rank LoRA ranks in search space

D Search Space
In this section, we undertake the computation of the possible number of subnets within our GLoRA adapted supernet. Each layer offers 4, 4, 3, 3, and 3 options for the support tensor A, B, C, D, and E, respectively. This results in 432 possible configurations for a single linear layer. In our implementation, we incorporate 48 such layers within ViT-B, yielding a total of 432 × 48 = 20, 736 subnets being explored within GLoRA. This figure can escalate if multiple LoRA ranks coexist within the same search space. For instance, we allow ranks 8 and 4 in our largest GLoRA models, leading to 82, 944 distinct subnets. Furthermore, owing to the phenomenon of weight entanglement as per [39], comparable performance is maintained across all subnets, even if they aren’t all explored during the training of the supernet.

Figure 5: Visualization of features from SVHN dataset by t-SNE [40]. 13

E Support Tensor
In this section, we justify the choices of support tensors in our framework. Consider a linear layer that facilitates the transformation of inputs from a d1 dimensional space to a d2 dimensional space, with a corresponding weight matrix W0 ∈ Rd2×d1 . Given that A is tasked with scaling W0, A could feasibly belong to Rd2×d1 , Rd2×1, or R1×1. These matrix dimensions are respectively indicative of LoRA, vector, and scalar operations. It’s pertinent to note that in scenarios where A ∈ Rd2×d1 , LoRA is realized via corresponding matrices Ad ∈ Rd2×r and Au ∈ Rr×d1 . A parallel scrutiny of other support tensors would result in determining the appropriate support tensor choice, as elaborated in Section 2.3 of the main paper.
F Fine-tuned Embedding Visualization
We present feature visualizations of the ViT-B model adapted via GLoRA and FacT [18] methods applied to the SVHN dataset, as shown in Figure 5. We selected FacT as opposed to LoRA [32], given that FacT constitutes a direct mathematical enhancement over LoRA and presently represents the state-of-the-art. A clear distinction can be discerned whereby GLoRA exhibits superiorly segregated clusters in comparison to FacT. Further, the delineations are broader, and the clusters demonstrate a higher degree of concentration, signaling the heightened discriminative capacity of the GLoRAadapted model features.
14

