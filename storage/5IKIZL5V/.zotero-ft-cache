
Skip to main content
Cornell University
We gratefully acknowledge support from the Simons Foundation, member institutions , and all contributors. Donate
arxiv logo > cs > arXiv:1810.11748

Help | Advanced Search
Search
Computer Science > Human-Computer Interaction
(cs)
[Submitted on 28 Oct 2018]
Title: DQN-TAMER: Human-in-the-Loop Reinforcement Learning with Intractable Feedback
Authors: Riku Arakawa , Sosuke Kobayashi , Yuya Unno , Yuta Tsuboi , Shin-ichi Maeda
Download a PDF of the paper titled DQN-TAMER: Human-in-the-Loop Reinforcement Learning with Intractable Feedback, by Riku Arakawa and Sosuke Kobayashi and Yuya Unno and Yuta Tsuboi and Shin-ichi Maeda
Download PDF

    Abstract: Exploration has been one of the greatest challenges in reinforcement learning (RL), which is a large obstacle in the application of RL to robotics. Even with state-of-the-art RL algorithms, building a well-learned agent often requires too many trials, mainly due to the difficulty of matching its actions with rewards in the distant future. A remedy for this is to train an agent with real-time feedback from a human observer who immediately gives rewards for some actions. This study tackles a series of challenges for introducing such a human-in-the-loop RL scheme. The first contribution of this work is our experiments with a precisely modeled human observer: binary, delay, stochasticity, unsustainability, and natural reaction. We also propose an RL method called DQN-TAMER, which efficiently uses both human feedback and distant rewards. We find that DQN-TAMER agents outperform their baselines in Maze and Taxi simulated environments. Furthermore, we demonstrate a real-world human-in-the-loop RL application where a camera automatically recognizes a user's facial expressions as feedback to the agent while the agent explores a maze. 

Subjects: 	Human-Computer Interaction (cs.HC) ; Machine Learning (cs.LG)
Cite as: 	arXiv:1810.11748 [cs.HC]
  	(or arXiv:1810.11748v1 [cs.HC] for this version)
  	https://doi.org/10.48550/arXiv.1810.11748
Focus to learn more
arXiv-issued DOI via DataCite
Submission history
From: Sosuke Kobayashi [ view email ]
[v1] Sun, 28 Oct 2018 02:18:40 UTC (2,339 KB)
Full-text links:
Download:

    Download a PDF of the paper titled DQN-TAMER: Human-in-the-Loop Reinforcement Learning with Intractable Feedback, by Riku Arakawa and Sosuke Kobayashi and Yuya Unno and Yuta Tsuboi and Shin-ichi Maeda
    PDF
    Other formats 

( license )
Current browse context:
cs.HC
< prev   |   next >
new | recent | 1810
Change to browse by:
cs
cs.LG
References & Citations

    NASA ADS
    Google Scholar
    Semantic Scholar

DBLP - CS Bibliography
listing | bibtex
Riku Arakawa
Sosuke Kobayashi
Yuya Unno
Yuta Tsuboi
Shin-ichi Maeda
a export BibTeX citation Loading...
Bookmark
BibSonomy logo Reddit logo
Bibliographic Tools
Bibliographic and Citation Tools
Bibliographic Explorer Toggle
Bibliographic Explorer ( What is the Explorer? )
Litmaps Toggle
Litmaps ( What is Litmaps? )
scite.ai Toggle
scite Smart Citations ( What are Smart Citations? )
Code, Data, Media
Demos
Related Papers
About arXivLabs
Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? )

    About
    Help

    contact arXiv Click here to contact arXiv Contact
    subscribe to arXiv mailings Click here to subscribe Subscribe

    Copyright
    Privacy Policy

    Web Accessibility Assistance

    arXiv Operational Status
    Get status notifications via email or slack

