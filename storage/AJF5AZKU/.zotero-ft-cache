arXiv:2305.18729v2 [cs.CV] 15 Jul 2023

Real-World Image Variation by Aligning Diffusion Inversion Chain
Yuechen Zhang1 Jinbo Xing1 Eric Lo1 Jiaya Jia1, 2 1The Chinese University of Hong Kong 2SmartMore
{yczhang21, jbxing, ericlo, leojia}@cse.cuhk.edu.hk
Abstract
Recent diffusion model advancements have enabled high-fidelity images to be generated using text prompts. However, a domain gap exists between generated images and real-world images, which poses a challenge in generating high-quality variations of real-world images. Our investigation uncovers that this domain gap originates from a latents’ distribution gap in different diffusion processes. To address this issue, we propose a novel inference pipeline called Real-world Image Variation by ALignment (RIVAL) that utilizes diffusion models to generate image variations from a single image exemplar. Our pipeline enhances the generation quality of image variations by aligning the image generation process to the source image’s inversion chain. Specifically, we demonstrate that step-wise latent distribution alignment is essential for generating high-quality variations. To attain this, we design a cross-image self-attention injection for feature interaction and a step-wise distribution normalization to align the latent features. Incorporating these alignment processes into a diffusion model allows RIVAL to generate high-quality image variations without further parameter optimization. Our experimental results demonstrate that our proposed approach outperforms existing methods with respect to semantic-condition similarity and perceptual quality. Furthermore, this generalized inference pipeline can be easily applied to other diffusion-based generation tasks, such as image-conditioned text-to-image generation and example-based image inpainting. Project page: https://rival-diff.github.io
1 Introduction
Generating real-world image variation is a crucial area of research in computer vision and machine learning, owing to its practical applications in image editing, image synthesis, and data augmentation [21, 11]. This task involves the generation of diverse variations of a given real-world image while preserving its semantic content and visual quality. Early methods for generating image variations included texture synthesis, neural style transfer, and generative models [6, 8, 9, 10, 18, 43]. However, these methods are limited in generating realistic and diverse variations from real-world images and are only suited for generating variations of textures or artistic images.
Denoising Diffusion Probabilistic Models (DDPMs) have resulted in significant progress in textdriven image generation [13, 38, 36]. However, generating images that maintain the style and semantic content of the reference remains a significant challenge. Although advanced training-based methods [37, 7, 23, 5] can generate images with novel concepts and styles with given images, they require additional training stages and data. Directly incorporating image as the input condition [36, 34] results in suboptimal visual quality and content diversity compared to the reference input. Besides, they do not support input with text descriptions. A plug-and-play method has not been proposed to generate high-quality, real-world image variations without extra optimization.
Preprint. Under review.

!$" Standard Gaussian
!!" Real-image inversion space

Vanilla denoising chain Distribution Gap
Real-image inversion chain Denoising step

!$#
Domain Gap
!!#

distribution

"(0, 1) Inverted latent !!"
value Image exemplar (

Figure 1: Conceptual illustration of the challenges in the real-world image variation. Left: Despite using the same text prompt "a lynx sitting in the grass", a bias exists between the latent distribution in the vanilla generation chain and the image inversion chain, resulting in a significant domain gap between the denoised images. Right: Visualization of the real-image inverted four-channel latent. The distribution of the latent is biased in comparison to a standard Gaussian distribution.

Observing the powerful denoising ability of pre-trained DDPMs in recovering the original image from the inverted latent space [39, 28], we aim to overcome the primary challenge by modifying the vanilla latent denoising chain of the DDPM to fit the real-image inversion chain [39, 28, 45]. Despite generating images with the same text condition, a significant domain gap persists between the generated and source images, as depicted in Fig. 1. We identify distribution misalignment as the primary factor that impedes the diffusion model from capturing certain image features from latents in the inversion chain. As illustrated in the right portion of Fig. 1, an inverted latent may differ significantly from the standard Gaussian distribution. This misalignment accumulates during the denoising process, resulting in a domain gap between the generated image and its reference exemplar.
To address this distribution gap problem for generating image variations, we propose an inference pipeline called Real-world Image Variation by Alignment (RIVAL). RIVAL is a tunning-free approach that reduces the domain gap between the generated and real-world images by aligning the denoising chain with the real-image inversion chain. Our method comprises two key components: (i) a crossimage self-attention injection that enables cross-image feature interaction in the variation denoising chain, guided by the hidden states from the inversion chain, and (ii) a step-wise latent normalization that aligns the latent distribution with the inverted latent in early denoising steps. Notably, this modified inference process requires no training and is suitable for arbitrary image input.
Our proposed approach produces visually appealing image variations while maintaining semantic and style consistency with a given image exemplar. RIVAL remarkably improves the quality of image variation generation qualitatively and quantitatively compared to existing methods [36, 34, 46]. Furthermore, we have demonstrated that RIVAL’s alignment process can be applied to other textto-image tasks, such as text-driven image generation with real-image condition [2, 12, 3] and example-based inpainting [26, 50].
This paper makes three main contributions. (1) Using a real-world image exemplar, we propose a novel approach to generate high-quality image variations. (2) We introduce an latent alignment process to enhance the quality of the generated variations. (3) Our proposed method offers a promising denoising pipeline that can be applied across various applications.

2 Related Works
Diffusion models with text control represent advanced techniques for controllable image generation with text prompts. With the increasing popularity of text-to-image diffusion models [36, 34, 35, 38], high-quality 2D images can be generated through text prompt input. However, such methods cannot guarantee to generate images with the same low-level textures and tone mapping as the given image reference, which is difficult to describe using text prompts. Another line of works aims for diffusionbased concept customization, which requires fine-tuning the model and transferring image semantic contents or styles into text space for new concept learning [37, 7, 23, 5, 16]. However, this involves training images and extra tuning, making it unsuitable for plug-and-play inference.
Real-world image inversion is a commonly employed technique in Generative Adversarial Networks (GANs) for image editing and attribute manipulation [10, 48, 49, 54, 20]. It involves inverting images to the latent space for reconstructions. In diffusion models, the DDIM sampling technique [39] provides a deterministic and approximated invertible diffusion process. Recently developed inversion methods [28, 45] guarantee high-quality reconstruction with step-wise latent alignments. With

2

" 0, %

Real-image inversion

…

…

!$"

Noise align

!$%('

!$%
injection

!$%&'

!$#
Image exemplar !

Latent Alignment

… !!%

VV T2I model / / Q

…

KK

!!%&'

!!#

!!"

Cross-image self-attention

Image variation "

Figure 2: High-level framework of RIVAL. Input exemplar R is inverted to a noisy latent XRT . An image variation G is generated from random noise following the same distribution as XRT . For each denoising step t, we interact XRt and XGt by self-attention injection and latent alignment.

diffusion inversion, real-image text-driven manipulations can be performed in image and video editing methods [25, 42, 3, 12, 30, 27, 32]. However, these editing methods heavily rely on the original structure of the input. They cannot generate free-form variations with the same content as the reference image from the perspective of image generation.
Image variation generation involves generating diverse variations of a given image exemplar while preserving its semantic content and visual quality. Several methods have been proposed for this problem, including neural style transfer [9, 17, 19, 22], novel view synthesis [51, 53, 29], and GANbased methods [18, 55, 20]. However, these methods were limited to generating variations of artistic images or structure-preserved ones and were unsuitable for generating realistic and diverse variations of real-world images. Diffusion-based methods [34, 36, 46] can generate inconsistent image variants by analyzing the semantic contents from the reference while lacking low-frequency details. Another concurrent approach, MasaCtrl [3], adopts self-attention injection as guidance for the denoising process. Yet, it can only generate variations with a generated image and fails on real image inputs. In contrast, RIVAL leverages the strengths of diffusion chain alignment to generate variations of real-world images while maintaining their characteristics.

3 Real-World Image Variation by Alignment

In this work, we define the image variation as the construction of a diffusion process F that satisfies
D(R, C) ≈ D(FC(X, R), C), where D(·, C) is a data distribution function with a given semantic content condition C. The diffusion process FC(X, R) = G generates the image variation G based on a sampled latent feature X, condition C, and the exemplar image R.

The framework of RIVAL is illustrated in Fig. 2. RIVAL generates an inverted latent feature chain
{XR0 , ..., XRT } by inverting a reference exemplar image R using DDIM inversion [39]. Then we obtain the initial latent XRT of the inversion chain. Next, a random latent feature XGT is sampled as the initial latent of the image generation (denoising) chain. In this multi-step denoising chain
{XGT , ..., XG0 } for generation, we align the latent features to latents from the inversion chain to obtain perceptually similar generations. The modified step-wise denoising function f at step t can be

represented abstractly as:

XGt = ft(XGt+1, XRt+1, C).

(1)

This function is achieved by performing adaptive cross-image attention in self-attention blocks for

feature interaction and performing latent distribution alignment in the denoising steps. The denoising

chain can produce similar variations by leveraging latents in the inversion chain as guidance.

3.1 Cross-Image Self-Attention Injection
To generate image variations from an exemplar image R, a feature interaction between the inversion chain and the generation chain is crucial. Previous works [47, 12] have shown that self-attention can efficiently facilitate feature interactions. Similar to its applications [25, 3, 32] in text-driven generation and editing tasks, we utilize intermediate hidden states vR obtained from the inversion chain to modify the self-attention in the generation chain. Specifically, our modified features for

3

Generated exemplar

+ cross-image attention

Real exemplar

+ cross-image attention

+ latent aligned attention

Figure 3: Exemplar images and generation results are obtained using the prompt "backpack in the wild". Cross-image self-attention can generate faithful outputs when a vanilla generation chain is employed as guidance. However, in the case of real-world images, the generation of faithful variations is dependent on the alignment of the latent.

cross-image self-attention for one diffusion step t in the generation chain are defined as follows:

Q = W Q(vG), K = W K (vG′ ), V = W V (vG′ ) , where

(2)

vG′ =

vG ⊕ vR vR

if t ≤ talign . otherwise

(3)

We denote W (·) as the pre-trained linear projections in the self-attention block and use ⊕ to represent concatenation in the spatial dimension. In early steps (t > talign), we directly use the corresponding hidden states vR as a reference feature in the inversion chain to obtain the modified feature vG′ . In subsequent steps, we concatenate vR and vG to obtain the modified key-value features. The proposed adaptive cross-image attention mechanism explicitly introduces feature interactions in the denoising process of the generation chain. Moreover, direct inversion feature injection in the early steps aligns the content distribution of the latent features in two denoising chains. Building upon the self-attention mechanism [44], we obtain the updated hidden state output as

vG∗ = softmax

QK ⊤ √
dk

V,

(4)

where dk is the dimensionality of K. For simplicity, we only present the attention mechanism for a single head in the paper. Cross-image self-attention and feature injection can facilitate the interaction between hidden features in the generation chain and the inversion chain. It should be noted that the inversion chain is deterministic throughout the inference process, which results in reference feature vR remaining independent of the generation chain.

3.2 Inverted Latent Chain Alignment

The cross-image self-attention injection is a potent method for generating image variations from a vanilla denoising process originating from a standard Gaussian distribution and is demonstrated in [3]. However, as depicted in Fig. 3, direct adaptation to real-world image input is not feasible due to a domain gap between real-world inverted latent chains and the vanilla generation chains. This leads to the attenuation of attention correlation during the calculation of Eq. (4). We visualize this problem in Fig. 7. To facilitate the generation of real-world image variations, the pseudo-generation

chain (inversion chain) of the reference exemplar can be estimated using the DDIM inversion [39]. Generating an image from latent features XT using a small number of denoising steps is possible
with the use of deterministic DDIM sampling:

Xt−1 =

αt−1/αt

·

Xt

+

√ αt−1(βt−1

−

βt)

·

εθ

Xt, t, C

,

(5)

where step-wise coefficient is set to βt = 1/αt − 1 and εθ(Xt, t, C) is the pre-trained noise prediction function in one timestep. Please refer to Appendix A for the definition of αt and DDIM sampling. With the assumption that noise predictions are similar in latents with adjacent time steps, DDIM sampling can be reversed In a small number of steps t ∈ [0, T ], using the equation:

Xt+1 =

αt+1/αt

·

Xt

+

√ αt+1(βt+1

−

βt)

·

εθ

Xt, t, C

,

(6)

which is known as DDIM inversion [39]. To acquire the latent representation XRT of a real image R, we apply DDIM. Nevertheless, the deterministic noise predictions in DDIM cause uncertainty

4

in maintaining a normal distribution N (0, I) of the inverted latent XRT . This deviation from the target distribution causes the attention-attenuation problem [4] between generation chains of R and
G, leading to the generation of images that diverge from the reference. To tackle this challenge, we find a straightforward distribution alignment on the initial latent XGT useful, that is,

xTG ∈ XGT ∼ N µ XRT , σ2 XRT , or

(7)

XGT = shuffle XRT .

(8)

In the alignment process, feature elements xTG ∈ XGT in the spatial dimension can be sampled from one of the two sources: (1) an adaptively normalized Gaussian distribution or (2) a permutated sample from the inverted reference latent, shuffle(XRT ). Our experiments show that both types of alignment yield comparable performance. This latent alignment strengthens the association between the variation
latents and the exemplar’s latents, increasing relevance within the self-attention mechanism and thus
increasing the efficacy of the proposed cross-image attention.

In the context of initializing the latent variable XGT , further alignment steps within the denoising chain are indispensable. This necessity arises due to the limitations of the Classifier-Free Guidance (CFG) inference proposed in [15]. While effective for improving text prompt guidance in the generation chain, CFG cannot be directly utilized to reconstruct high-quality images through DDIM inversion. On the other hand, the noise scaling of CFG will affect the noise distribution and lead to a shift between latents in two chains. To avoid this misalignment while attaining the advantage of the text guidance in CFG, we decouple two inference chains and rescale the noise prediction during denoising inference, formulated as an adaptive normalization:

ϵθ XGt , t, C = AdaIN(εcθfg XGt , t, C , εθ XRt , t, C ), where

(9)

a − µ(a)

AdaIN(a, b) = σ(b) σ(a) + µ(b),

(10)

εcθfg XGt , t, C = mεθ XGt , t, C + (1 − m)εθ XGt , t, C∗ , and t > tearly.

(11)

εcθfg(XGt , t, C) is the noise prediction using guidance scale m and unconditional null-text C∗. We incorporate this noise alignment approach during the initial stages (t > tearly). After that, a vanilla CFG scaling is applied as the content distribution in G may not be consistent with R. Aligning the

distributions at this early stage proves advantageous for aligning the overall denoising process.

4 Experiments
We evaluate our proposed pipeline through three primary tasks in Sec. 4.2: image variation, image generation with text control, and example-based inpainting. Additionally, we provide quantitative assessments and user study results to evaluate generation quality in Sec. 4.3. A series of ablation studies are conducted in Sec. 4.4 to assess the effectiveness of individual modules within RIVAL.
4.1 Implementation Details
Our study obtained a high-quality test set of reference images from the Internet and DreamBooth [37] to ensure a diverse image dataset. To generate corresponding text prompts C, we utilized BLIP2 [24]. Our baseline model is Stable-Diffusion V1.5. During the image inversion and generation, we employed DDIM sample steps T = 50 for each image and set the classifier-free guidance scale m = 7 in Eq. (11). We split two stages at talign = tearly = 30 for attention alignment in Eq. (3) and latent alignment in Eq. (11). In addition, we employ the shuffle strategy described in Eq. (8) to initialize the starting latent XGT . All inference experiments run on a single NVIDIA 4090 GPU with 8 seconds to generate image variation with batch size 1.
4.2 Applications and Comparisons
Image variation. As depicted in Fig. 4, real-world image variations exhibit diverse characteristics to evaluate perceptual quality. Text-driven image generation using the basic Stable Diffusion [36] cannot use image exemplars explicitly, thus failing to get faithful image variations. While recent image-conditioned generators such as [46, 31] have achieved significant progress for image input, their reliance on encoding images into text space limits the representation of certain image features,

5

“An old black and white drawing of a city.”

“A group of people standing around a statue.”

“A photo of backpack.”

“A girl in a sailor uniform posing for a photo.”

“A photo of a dog lying on a sofa.”

Source exemplar

Stable Diffusion

SD variation

ELITE

RIVAL (Ours)

Figure 4: Real-world image variation. We compare RIVAL with recent competitive tuning-free methods [36, 31, 46] conditioned on the same text prompt (results in the 2nd column) or exemplar image (results in the 3rd and 4th column).

including texture, color tone, lighting environment, and image style. In contrast, RIVAL generates image variations based on text descriptions and reference images, harnessing the real-image inversion chain to facilitate latent distribution alignments. Consequently, RIVAL ensures high visual similarity in both semantic content and low-level features. For instance, our approach is the sole technique that generates images with exceedingly bright or dark tones (2nd-row in Fig. 4). More visual comparisons (including comparison with DALL·E 2 [34]) and experimental settings can be found in Appendix C.
Text-driven image generation. In addition to its ability to generate images corresponding to the exemplar image and text prompts, we have also discovered that RIVAL has a strong ability to transfer styles and semantic concepts in the exemplar for a casual text-driven image generation. When an inversion with the original text prompt is performed, the alignment on denoising processes can still transfer high-level reference style to the generated images with modified prompts. This process could be directly used in structure-preserved editing. This could be regarded as the same task of MasaCtrl and Plug-and-Play [3, 42], which utilizes an initialized inverted latent representation to

6

“A {} is sitting in the grass”

“lynx”*

w/o RIVAL*

“A {n.} {adj.} is posing for a portrait”

“dog”*

“dog”

“cat”

{n.}: “women”, {adj.}: “” *

{n.}: “robot”*

{n.}: “robot”

{adj.}: “in a sailor uniform”

{n.}: “girl”

{n.}: “Elon Musk” {adj.}: “”

Figure 5: Text-driven image generation results using RIVAL, with the leftmost image as the source
exemplar in each row. We can generate images with diverse text inputs with RIVAL while preserving
the reference style. Images noted with * indicate structure-preserved editing, which starts with the same initialized inverted reference latent XRT as the exemplar.

source exemplar

Paint-by-example

RIVAL(Ours)

source exemplar

Paint-by-example

RIVAL(Ours)

Figure 6: RIVAL extended to self-example image inpainting. We use the same image as the example to fill the masked area. Results are compared with a well-trained inpainting SOTA method [50].

drive text-guided image editing. As shown in Fig. 5 with examples marked with *, RIVAL can do local editing by editing text descriptions.
Furthermore, in the absence of a structure-preserving prior, it is possible for a user-defined text prompt to govern the semantic content of a freely generated image G. This novel capability is showcased in Fig. 5, where RIVAL successfully generates images driven by text input while maintaining the original style of the reference image. With RIVAL, we can easily get a style-specific text-to-image generation. For instance, it can produce a portrait painting of a robot adorned in a sailor uniform while faithfully preserving the stylistic characteristics inherent in the provided oil painting.
Example-based image inpainting. When abstracting RIVAL as a novel paradigm of image-based diffusion inference, we can extend this framework to enable it to encompass other image editing tasks, such as inpainting. By incorporating a coarse mask M into the generation chain, we obtain the inpainted image G. Specifically, we only permute the inverted latent variables within the mask to initialize the latent variable XGT . During denoising, we directly replace latents in the unmasked regions with latents in the inversion chain for each step. As shown in Fig. 6, we present a visual comparison between our approach and Paint by Example [50]. RIVAL produces a reasonable and visually harmonious inpainted result within the coarse mask.

4.3 Quantitative Evaluation
We compare RIVAL with several state-of-the-art methods, employing the widely used CLIP-score [33] evaluation metric for alignments with text and the image exemplar. The evaluation samples are from two datasets: the DreamBooth dataset [37] and our collected images. In addition, we cluster a 10-color palette using k-means and calculate the minimum bipartite distance to assess the similarity in low-

7

Metric

SD [36]

ImgVar [31] ELITE [46] DALL·E 2 [34] RIVAL

Text Alignment ↑ Image Alignment ↑ Palette Distance ↓

0.255 ± 0.04 0.748 ± 0.08 3.650 ± 1.30

0.223 ± 0.04 0.832 ± 0.07 3.005 ± 0.86

0.209 ± 0.05 0.736 ± 0.09 2.885 ± 0.81

0.253 ± 0.04 0.897 ± 0.05 2.102 ± 0.83

0.275 ± 0.03 0.840 ± 0.07 1.674 ± 0.63

Real-image Rank ↓ Preference Rank ↓ -

2.982 (4.7%) 3.146 (7.3%)

3.526 (9.6%) 3.353 (5.6%)

1.961 (28.0%) 1.897 (30.6%)

1.530 (61.6%) 1.603 (56.4%)

Table 1: Quantitative comparisons. We evaluate the quality of image variation regarding feature matching within different levels (color palette, text feature, image feature), highlighted with best and second best results. We also report user preference rankings and the first ranked rate.

% of attention score

Source exemplar

- cross-image attention

- latent alignment

- early alignment

Full pipeline

steps

Figure 7: Left: a visual example for a module-wise ablation study. Right: the source feature’s contribution to the cross-image self-attention score in the bottleneck block in the generation chain.

(50, 50)

(40, 40)

(30, 30)

(0, 0)

(50, 0)

(0, 50)

Figure 8: Ablation study for different early-alignment strategies with the exemplar in Fig. 4. We list (talign, tearly) pairs for each image. All images are generated from the same fixed latent.

level color tones. In general, the results in Tab. 1 demonstrate that RIVAL significantly outperforms other methods regarding semantic and low-level feature matching. Notably, our method achieves better text alignment with the real-image condition than the vanilla text-to-image method using the same model. Furthermore, our alignment results are on par with those attained by DALL·E 2 [34], which directly utilizes the image CLIP feature as the condition.
We conducted a user study to evaluate further the generated images’ perceptual quality. This study utilized results generated by four shuffled methods based on the same source. Besides, the user study includes a test for image authenticity, which lets users identify the "real" image among generated results. We collected responses of ranking results for each case from 41 participants regarding visual quality. The findings indicate a clear preference among human evaluators for our proposed approach over existing methods [31, 46, 34]. The detailed ranking results are presented in Tab. 1.
4.4 Ablation Studies
We perform ablation studies to evaluate the efficacy of each module in our design. Additional visual results can be found in Appendix D.
Cross-image self-attention injection plays a crucial role in aligning the feature space of the source image and the generated image. We eliminated the cross-image self-attention module and generated image variations to verify this. The results in Fig. 7 demonstrate that RIVAL fails to align the feature space without attention injection, generating images that do not correspond to the exemplar.
Latent alignment. To investigate the impact of step-wise latent distribution alignment, we remove latent alignments and sampling XGT ∼ N (0, I). An example is shown in Fig. 3 and Fig. 7. Without latent alignment, RIVAL produces inconsistent variations of images, exhibiting biased tone and semantics compared to the source image. Additionally, we perform ablations on latent replacement in Eq. (3) and noise alignment in Eq. (11) during the early steps. Clear color and semantic bias emerge when the early alignment is absent, as shown in Fig. 8. Furthermore, if the values of tearly or

8

“A {} of <sks> {}.”

Vanilla Inference

RIVAL Inference

“photo + toy” source exemplar

“photo + bag”

“painting + toy girl, in the style of Vermeer”

“photo + bag”

“painting + toy girl”

Figure 9: RIVAL enables seamless customization of optimized novel concepts through text prompt control (<sks>). With various text prompt inputs, we can still generate images preserving the tones, style, and contents of the provided source exemplar (depicted on the left).

talign are excessively small, the latent alignment stage becomes protracted, resulting in the problem of over-alignment. Over-alignment gives rise to unwanted artifacts and generates blurry outputs.
To explicitly demonstrate the impact of latent alignment designs on cross-image attention, we visualize the proportion of the attention score with features provided by the inversion chain in the bottleneck block. As illustrated in the right section of Fig. 7, latent initialization (orange) and with early alignments green) play critical roles in ensuring a substantial contribution of the source feature in self-attention. A higher attention contribution helps in resolving the attention attenuation problem (purple) in the generation process.

4.5 Discussions
Integration with concept customization. In addition to its ability to generate image variations from a single source image using a text prompt input for semantic alignment, RIVAL can be effectively combined with optimization-based concept customization techniques, such as DreamBooth [37], to enable novel concept customization. As illustrated in Fig. 9, an optimized concept can efficiently explore the space of potential image variations by leveraging RIVAL’s proficiency in real-world image inversion alignment.
Limitations and Future Directions. Despite introducing an innovative technique for crafting highquality inconsistent image variations, our method is contingent upon a text prompt input, potentially infusing semantic biases affecting image quality. The existing diffusion model’s generation capacity also restricts its effectiveness with intricate scenes or novel concepts, as evidenced in Fig. 10. Future studies might concentrate on refining diffusion models and exploring novel input avenues besides text prompts to mitigate these constraints.

“Pokémon”
(a) Semantic bias

(b) Complex scene

(c) Hard concept

Figure 10: Fail cases of RIVAL. The exemplar image is on the left for each case, with its corresponding generated variation on the right.

5 Conclusion

This paper presents a novel pipeline for generating diverse and high-quality variations of real-world images while maintaining their semantic content and style. Our proposed approach addresses existing methods’ limitations by modifying the diffusion model’s denoising inference chain to align with the real-image inversion chain. We introduce a cross-image self-attention injection and a step-wise latent alignment technique to facilitate alignment between two denoising chains. Our method exhibits significant improvements in the quality of image variation generation compared to state-of-the-art methods, as demonstrated through qualitative and quantitative evaluations. Moreover, with the novel paradigm of hybrid text-image conditions, our approach can be easily extended to text-to-image tasks, including text-driven image generation and example-based image inpainting.

9

Appendix

A Basic Background of Diffusion Models

This section uses a modified background description provided in [28]. We only consider the condition-

free case for the diffusion model here. Diffusion Denoising Probabilistic Models (DDPMs) [14] are

generative latent variable models designed to approximate the data distribution q(x0). The diffusion

operation starts from the latent x0, adding step-wise noise to diffuse data into pure noise xT . It’s

important to note that this process can be viewed as a Markov chain starting from x0, where noise is

gradually added to the data to generate the latent variables x1, . . . , xT ∈ X. The sequence of latent

variables follows the conditional distribution q(x1, . . . , xt | x0) =

t i=1

q(xt

|

x√t−1).

Each

step

in

the forward process is defined by a Gaussian transition q(xt | xt−1) := N (xt; 1 − ktxt−1, ktI),

which is parameterized by a schedule k0, . . . , kT ∈ (0, 1). As T becomes sufficiently large, the final noise vector xT approximates an isotropic Gaussian distribution.

The forward process allows us to express the latent variable xt directly as a linear combination of

noise

and

x0,

without

the

need

to

sample √

intermediate √

latent

vectors.

xt = αtx0 + 1 − αtw, w ∼ N (0, I),

(12)

where αt :=

t i=1

(1

−

ki).

To

sample

from

the

distribution

q(x0),

a

reversed

denoising

process

is

defined by sampling the posteriors q(xt−1 | xt), which connects isotropic Gaussian noise xT to the

actual data. However, the reverse process is computationally challenging due to its dependence on the

unknown data distribution q(x0). To overcome this obstacle, an approximation of the reverse process with a parameterized Gaussian transition network denoted as pθ(xt−1 | xt), where pθ(xt−1 | xt) follows a normal distribution with mean µθ(xt, t) and covariance Σθ(xt, t). As an alternative approach, the prediction of the noise ϵθ(xt, t) added to x0, which is obtained using equation 12, can replace the use of µθ(xt, t) as suggested in [14]. Bayes’ theorem could be applied to approximate

1

µθ(xt, t)

=

√ αt

xt

−

√ kt 1−

αt

ϵθ (xt ,

t)

.

(13)

Once we have a trained ϵθ(xt, t), we can using the following sample method

xt−1 = µθ(xt, t) + σtz, z ∼ N (0, I).

(14)

In DDIM sampling [39], a denoising process could become deterministic when set σt = 0.

B Details of the Attention Pipeline
We present a comparative analysis of attention injection methods. As depicted in Fig. 11, MasaCtrl [3], while also adopting a self-attention injection approach, employs a more complex control mechanism in its second stage. In the first stage of MasaCtrl, the inverted latent representation XTR is directly utilized by applying a modified prompt. In the second stage, a cross-attention mask is introduced to control specific word concepts modified in the prompt, which requires an additional forward pass. In contrast, our proposed method, RIVAL, primarily focuses on generating inconsistent variations. Consequently, we aim to guide feature interaction by replacing KV features with an aligned latent distribution. Unlike MasaCtrl, our approach does not limit content transfer through editing prompts with only a few words. Hence, in the second stage, we employ a single forward pass without calculating an additional cross-attention mask, allowing fast and flexible text-to-image generation with diverse text prompts.
In recent updates, ControlNet [52] has incorporated an attention mechanism resembling the second stage of RIVAL to address image variation. However, a notable distinction lies in using vanilla noised latents as guidance, leading to a process akin to the attention-only approach employed in RePaint [26] with the Stable Diffusion model. Consequently, this methodology is limited to generating images within the fine-tuned training data domain.

C More About Comparisons
Implementation Details. We compare our work with ELITE [46], Stable Diffusion image variation [31], and DALL·E 2 [34]. We utilize the official demo of ELITE to obtain results. To extract

10

MasaCtrl

…

!#& = !!&

!#"

#'()*+ = 45

RIVAL

T2I model # ≥ #'()*+

V Q
K

!#"$%

!!"
Replacement

… !#&

T2I model
!#" # ≥ #'()*+
#'()*+ = 30

V Q
K

!#"$%

!!" Specific mask
control+ replacement

VV

T2I model

Q

!#"

# < #'()*+

KK
dec enc

(Additional forward to get mask)

!!"
injection

T2I model

VV Q

!#"

KK

# < #'()*+

(Single forward)

!#"$% !#"$%

Figure 11: Self-attention control compare with MasaCtrl [3]. The default split of two stages is shown as a bar for each method.

context tokens, we mask the entire image and employ the phrase "A photo/painting of <S>." based on the production method of each test image. Inference for ELITE employs the default setting with denoising steps set to T = 300. For Stable Diffusion’s image variation version, we utilize the default configuration, CFG guidance m = 3, and denoising steps T = 50. In the case of DALL·E 2, we utilize the official image variation API, specifically requesting using the most advanced API available to generate images of size 1024 × 1024.
Comparison with UnCLIP. UnCLIP [34], also known as DALL·E 2, is an image generation framework trained using image CLIP features as direct input. Thanks to its large-scale training and image-direct conditioning design, it generates variations solely based on image conditions when adapted to image variation. However, when faced with hybrid image-text conditions, image-only UnCLIP struggles to produce satisfactory results, particularly when CLIP does not recognize the image content correctly. We provide comparative analysis in Fig. 12. Additionally, we demonstrate in the last two columns of Figure 12 that our approach can enhance the accuracy of low-level details in open-source image variation methods such as SD image variation [31].
Additional Visual Results. We showcase additional results of our techniques in variation generation, as illustrated in Fig. 13, and text-driven image generation with image condition, as shown in Fig. 14. The results unequivocally demonstrate the efficacy of our approach in generating a wide range of image variations that accurately adhere to textual and visual guidance.
D Additional Ablation Results
Ablation on early fusion step. In addition to Fig. 8 of the main paper, we present comprehensive early-step evaluation results based on a grid search analysis in Fig. 15. By decreasing the duration of the feature replacement stage (larger talign), we observe an increase in the similarity of textures and contents in the generated images. However, excessively long or short early latent alignment durations (tearly) can lead to color misalignment. Users can adjust the size of the early fusion steps as hyperparameters to achieve the desired outcomes.
Ablation on different alignment designs. Fig. 16 illustrates ablations conducted on various alignment designs. Two latent initialization methods, as formulated in Eq. (7) and Eq. (8), exhibit comparable performance. Nevertheless, incorporating alignments in additional areas, such as hidden states within each transformer block, may harm performance. Hence, we opt for our RIVAL pipeline’s simplest noise alignment strategy.
Ablation on different text conditions. We conduct ablations on text conditions in three aspects. First, we evaluate the impact of different CFG scales m for text prompt guidance. As shown
11

“Pokémon”

fail case

“A group of people standing around a statue.”

“A building with a blue and white striped awning.”

“Silhouette of a woman looking through a glass dome filled with fish.” .” “A girl in a sailor uniform posing for a photo.”

Source exemplar

DALL·E 2

UnCLIP SDv2 ImgVar SDv1 + RIVAL RIVAL SDv1

Figure 12: Comparision and adaptation with UnCLIP [34]. We highlight texts that enhance the image understanding for each case. Our inference pipeline is adapted to the image variation model depicted in the fourth column, in contrast to the variation achieved through vanilla inference in the bottom left corner of each image.

in Fig. 17 (a), our latent rescaling technique enables control over the text guidance level while preserving the reference exemplar’s low-level features. Second, we employ an optimization-based null-text inversion method [28] to obtain an inversion chain with improved reconstruction quality. However, this method is computationally intensive, and the optimized embeddings are sensitive to the guidance scale. Furthermore, when incorporating this optimized embedding into the unconditional inference branch, there is a variation in generation quality, as depicted in Fig. 17 (b). Third, we utilize empty text as the source prompt to obtain the latents in the inversion chain while keeping the target prompt unchanged. As depicted in Fig. 17 (c), the empty text leads to weak semantic content correspondence between the two chains but sometimes benefits text-driven generation. For example, if users do not want to transfer the "gender" concept to the generated robot.
12

Exemplars

Variations

Figure 13: Text-driven free-form image generation results. The image reference is in the left column. In the last row, we also present variations for one customized concept <sks> bag.
13

Figure 14: Text-driven free-form image generation results, with the image reference placed in the top left corner. The text prompts used are identical to those presented in Fig. 5 of the main paper. Every two rows correspond to a shared text prompt.

method base model

SD [36] V1-5

ImgVar [31] V1-3

ELITE [46] V1-4

UnClIP [34] V2-1

RIVAL V1-5

KID ↓

17.1

18.5

25.7

13.5

13.2

Table 2: Quantitative comparisons for KID (×103). All methods are Stable Diffusion based.

E Quantitative Evaluations
This section comprehensively evaluates our proposed method with various carefully designed metrics, including CLIP Score, color palette matching, user study, and KID.
CLIP Score. For evaluating the CLIP Score, we employ the official ViT-Large-Patch14 CLIP model [33] and compute the cosine similarity between the projected features, yielding the output.
Color palette matching. To perform low-level matching, we utilize the Pylette tool [40] to extract a set of 10 palette colors. Subsequently, we conduct a bipartite matching between the color palette of each generated image and the reference palette colors in the RGB color space. Before matching, each color is scaled to [0, 1]. The matching result is obtained by calculating the sum of L1 distances.
User study. To evaluate the effectiveness of our approach against other methods, we conducted a user study using an online form. The user study interface, depicted in Figure 18, was designed to elicit user rankings of image variation results. We collected 41 questionnaire responses, encompassing 16 cases of ranking comparisons.
KID evaluation. To provide a comprehensive assessment of the quality, we utilize Kernel Inception Distance (KID)[1] to evaluate the perceptual generation quality of our test set. As depicted in Table2, with Stable Diffusion V1-5, our method achieves the best KID score, which is slightly superior to the UnCLIP [34], employing the advanced Stable Diffusion V2-1.

14

0

10

20

30

40

50

50

!&'()*: image similarity control

40

30

20

10

0

!!"#$%: color bias control

“A photo of a red vase.”

Figure 15: Ablation results for alignment steps, with the reference exemplar at the bottom right. We fix each generation’s initial latent XGT .

!!"~+(,, .)

!!"~#ℎ%&&'((!#")

w/o initialize !!"

AdaIN !!"

hidden state each block

hidden state before self-attn

early latent

Figure 16: Ablation studies for different feature alignment strategies.

early noise

F Additional Considerations

Data acquisition. To comprehensively evaluate our method, we collected diverse source exemplars from multiple public datasets, such as DreamBooth [37] and Interactive Video Stylization [41]. Some exemplars were obtained from Google and Behance solely for research purposes. We will not release our self-collected example data due to license restrictions.
Societal impacts. This paper introduces a novel framework for image generation that leverages a hybrid image-text condition, facilitating the generation of diverse image variations. Although this application has the potential to be misused by malicious actors for disinformation

15

(a) Ablation on different CFG scales

“A PINK building”

m=1 (failed)

m=3

m=5

m=7

m=9

(b) Ablation on null-text inversion

w/o opt. null (c) Ablation on different source prompts

w/ opt. null

w/o opt. null {adj.}: “in a sailor uniform”

w/ opt. null

{n.}: “Elon Musk” original prompt

empty prompt

{n.}: “robot” original prompt

empty prompt

{n.}: “girl” original prompt

empty prompt

Figure 17: Ablation studies on different text conditions and guidance scales. Reference exemplars are highlighted with a golden border.

(a) Specification

(b) Shuffled results

(c) Ranking questions

Figure 18: User study user interface. In this case, four methods are: (A). SD ImageVar [31], (B). ELITE [46], (C). DALL·E 2[34], (D). RIVAL (ours).

16

purposes, significant advancements have been achieved in detecting malicious generation. Consequently, we anticipate that our work will contribute to this domain. In forthcoming iterations of our method, we intend to introduce the NSFW (Not Safe for Work) test for detecting possible malicious generations. Through rigorous experimentation and analysis, our objective is to enhance comprehension of image generation techniques and alleviate their potential misuse.
References
[1] Mikołaj Bin´kowski, Danica J Sutherland, Michael Arbel, and Arthur Gretton. Demystifying mmd gans. arXiv preprint arXiv:1801.01401, 2018. 14
[2] Tim Brooks, Aleksander Holynski, and Alexei A. Efros. Instructpix2pix: Learning to follow image editing instructions. In CVPR, 2023. 2
[3] Mingdeng Cao, Xintao Wang, Zhongang Qi, Ying Shan, Xiaohu Qie, and Yinqiang Zheng. Masactrl: Tuning-free mutual self-attention control for consistent image synthesis and editing. arXiv preprint arXiv:2304.08465, 2023. 2, 3, 4, 6, 10, 11
[4] Hila Chefer, Shir Gur, and Lior Wolf. Transformer interpretability beyond attention visualization. In CVPR, pages 782–791, 2021. 5
[5] Ziyi Dong, Pengxu Wei, and Liang Lin. Dreamartist: Towards controllable one-shot text-to-image generation via contrastive prompt-tuning. arXiv preprint arXiv:2211.11337, 2022. 1, 2
[6] Alexei A Efros and Thomas K Leung. Texture synthesis by non-parametric sampling. In ICCV, volume 2, pages 1033–1038. IEEE, 1999. 1
[7] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit Haim Bermano, Gal Chechik, and Daniel Cohen-or. An image is worth one word: Personalizing text-to-image generation using textual inversion. In ICLR, 2023. 1, 2
[8] Leon Gatys, Alexander S Ecker, and Matthias Bethge. Texture synthesis using convolutional neural networks. In NeurIPS, volume 28, 2015. 1
[9] Leon A Gatys, Alexander S Ecker, and Matthias Bethge. Image style transfer using convolutional neural networks. In CVPR, pages 2414–2423, 2016. 1, 3
[10] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks. Communications of the ACM, 63(11):139– 144, 2020. 1, 2
[11] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning. In CVPR, pages 9729–9738, 2020. 1
[12] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Prompt-toprompt image editing with cross attention control. arXiv preprint arXiv:2208.01626, 2022. 2, 3
[13] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In NeurIPS, volume 33, pages 6840–6851, 2020. 1
[14] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. NeurIPS, 33:6840– 6851, 2020. 10
[15] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. 5
[16] Edward J Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. In ICLR, 2021. 2
[17] Xun Huang and Serge Belongie. Arbitrary style transfer in real-time with adaptive instance normalization. In ICCV, pages 1501–1510, 2017. 3
[18] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros. Image-to-image translation with conditional adversarial networks. In CVPR, pages 1125–1134, 2017. 1, 3
[19] Justin Johnson, Alexandre Alahi, and Li Fei-Fei. Perceptual losses for real-time style transfer and super-resolution. In ECCV, pages 694–711. Springer, 2016. 3
17

[20] Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative adversarial networks. In CVPR, pages 4401–4410, 2019. 2, 3
[21] Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip Isola, Aaron Maschinot, Ce Liu, and Dilip Krishnan. Supervised contrastive learning. In NeurIPS, volume 33, pages 18661–18673, 2020. 1
[22] Nicholas Kolkin, Michal Kucera, Sylvain Paris, Daniel Sykora, Eli Shechtman, and Greg Shakhnarovich. Neural neighbor style transfer. arXiv preprint arXiv:2203.13215, 2022. 3
[23] Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman, and Jun-Yan Zhu. Multi-concept customization of text-to-image diffusion. In CVPR, 2023. 1, 2
[24] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. BLIP-2: bootstrapping language-image pre-training with frozen image encoders and large language models. In ICML, 2023. 5
[25] Shaoteng Liu, Yuechen Zhang, Wenbo Li, Zhe Lin, and Jiaya Jia. Video-p2p: Video editing with crossattention control. arXiv preprint arXiv:2303.04761, 2023. 3
[26] Andreas Lugmayr, Martin Danelljan, Andres Romero, Fisher Yu, Radu Timofte, and Luc Van Gool. Repaint: Inpainting using denoising diffusion probabilistic models. In CVPR, pages 11461–11471, June 2022. 2, 10
[27] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. SDEdit: Guided image synthesis and editing with stochastic differential equations. In ICLR, 2022. 3
[28] Ron Mokady, Amir Hertz, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Null-text inversion for editing real images using guided diffusion models. arXiv preprint arXiv:2211.09794, 2022. 2, 10, 12
[29] Thu Nguyen-Phuoc, Feng Liu, and Lei Xiao. Snerf: stylized neural implicit representations for 3d scenes. ACM Transactions on Graphics (TOG), 41(4):1–11, 2022. 3
[30] Alexander Quinn Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob Mcgrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. In ICML, pages 16784–16804. PMLR, 2022. 3
[31] Justin Pinkney. Experiments with stable diffusion. https://github.com/justinpinkney/ stable-diffusion, 2023. 5, 6, 8, 10, 11, 14, 16
[32] Chenyang Qi, Xiaodong Cun, Yong Zhang, Chenyang Lei, Xintao Wang, Ying Shan, and Qifeng Chen. Fatezero: Fusing attentions for zero-shot text-based video editing. arXiv preprint arXiv:2303.09535, 2023. 3
[33] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In ICML, pages 8748–8763. PMLR, 2021. 7, 14
[34] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 2022. 1, 2, 3, 6, 8, 10, 11, 12, 14, 16
[35] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In ICML. PMLR, 2021. 2
[36] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models. In CVPR, pages 10684–10695, 2022. 1, 2, 3, 5, 6, 8, 14
[37] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. In CVPR, 2023. 1, 2, 5, 7, 9, 15
[38] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-toimage diffusion models with deep language understanding. In NeurIPS, volume 35, pages 36479–36494, 2022. 1, 2
[39] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In ICLR, 2021. 2, 3, 4, 10
[40] Ivar Stangeby. A python library for extracting color palettes from supplied images. https://github. com/qTipTip/Pylette, 2022. 14
18

[41] Ondˇrej Texler, David Futschik, Michal Kucˇera, Ondˇrej Jamriška, Šárka Sochorová, Menclei Chai, Sergey Tulyakov, and Daniel Sy`kora. Interactive video stylization using few-shot patch-based training. ACM Transactions on Graphics (TOG), 39(4):73–1, 2020. 15
[42] Narek Tumanyan, Michal Geyer, Shai Bagon, and Tali Dekel. Plug-and-play diffusion features for text-driven image-to-image translation. arXiv preprint arXiv:2211.12572, 2022. 3, 6
[43] Aaron Van den Oord, Nal Kalchbrenner, Lasse Espeholt, Oriol Vinyals, Alex Graves, et al. Conditional image generation with pixelcnn decoders. In NeurIPS, volume 29, 2016. 1
[44] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NeurIPS, volume 30, 2017. 4
[45] Bram Wallace, Akash Gokul, and Nikhil Naik. Edict: Exact diffusion inversion via coupled transformations. arXiv preprint arXiv:2211.12446, 2022. 2
[46] Yuxiang Wei, Yabo Zhang, Zhilong Ji, Jinfeng Bai, Lei Zhang, and Wangmeng Zuo. Elite: Encoding visual concepts into textual embeddings for customized text-to-image generation. arXiv preprint arXiv:2302.13848, 2023. 2, 3, 5, 6, 8, 10, 14, 16
[47] Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Weixian Lei, Yuchao Gu, Wynne Hsu, Ying Shan, Xiaohu Qie, and Mike Zheng Shou. Tune-a-video: One-shot tuning of image diffusion models for text-to-video generation. arXiv preprint arXiv:2212.11565, 2022. 3
[48] Weihao Xia, Yulun Zhang, Yujiu Yang, Jing-Hao Xue, Bolei Zhou, and Ming-Hsuan Yang. Gan inversion: A survey. PAMI, 45(3):3121–3138, 2023. 2
[49] Siyu Xing, Chen Gong, Hewei Guo, Xiao-Yu Zhang, Xinwen Hou, and Yu Liu. Unsupervised domain adaptation gan inversion for image editing. arXiv preprint arXiv:2211.12123, 2022. 2
[50] Binxin Yang, Shuyang Gu, Bo Zhang, Ting Zhang, Xuejin Chen, Xiaoyan Sun, Dong Chen, and Fang Wen. Paint by example: Exemplar-based image editing with diffusion models. arXiv preprint arXiv:2211.13227, 2022. 2, 7
[51] Kai Zhang, Nick Kolkin, Sai Bi, Fujun Luan, Zexiang Xu, Eli Shechtman, and Noah Snavely. Arf: Artistic radiance fields. In ECCV, pages 717–733. Springer, 2022. 3
[52] Lvmin Zhang and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. arXiv preprint arXiv:2302.05543, 2023. 10
[53] Yuechen Zhang, Zexin He, Jinbo Xing, Xufeng Yao, and Jiaya Jia. Ref-NPR: Reference-based nonphotorealistic radiance fields for controllable scene stylization. In CVPR, 2023. 3
[54] Jiapeng Zhu, Yujun Shen, Deli Zhao, and Bolei Zhou. In-domain gan inversion for real image editing. In ECCV, pages 592–608. Springer, 2020. 2
[55] Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros. Unpaired image-to-image translation using cycle-consistent adversarial networks. In ICCV, pages 2223–2232, 2017. 3
19

