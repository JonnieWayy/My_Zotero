arXiv:2305.03726v1 [cs.CV] 5 May 2023

Otter: A Multi-Modal Model with In-Context Instruction Tuning
Bo Li* Yuanhan Zhang* Liangyu Chen* Jinghao Wang* Jingkang Yang Ziwei Liu
S-Lab, Nanyang Technological University, Singapore {libo0013, yuanhan002, lchen025, c190209, jingkang001, ziwei.liu}@ntu.edu.sg
https://github.com/Luodian/Otter
Abstract
Large language models (LLMs) have demonstrated signiÔ¨Åcant universal capabilities as few/zero-shot learners in various tasks due to their pre-training on vast amounts of text data, as exempliÔ¨Åed by GPT-3, which boosted to InstrctGPT and ChatGPT, effectively following natural language instructions to accomplish real-world tasks. In this paper, we propose to introduce instruction tuning into multi-modal models, motivated by the Flamingo model‚Äôs upstream interleaved format pretraining dataset. We adopt a similar approach to construct our MultI-Modal In-Context Instruction Tuning (MIMIC-IT) dataset. We then introduce Otter, a multi-modal model based on OpenFlamingo (open-sourced version of DeepMind‚Äôs Flamingo), trained on MIMIC-IT and showcasing improved instruction-following ability and in-context learning. We also optimize OpenFlamingo‚Äôs implementation for researchers, democratizing the required training resources from 1√ó A100 GPU to 4√ó RTX-3090 GPUs, and integrate both OpenFlamingo and Otter into Huggingface Transformers for more researchers to incorporate the models into their customized training and inference pipelines.
1 Introduction & Motivation
Large language models (LLMs) have demonstrated signiÔ¨Åcant universal capabilities in performing various tasks as few/zero-shot learners. These models are pre-trained on vast amounts of text data and have been showcased in recent research, such as GPT-2 [25] and GPT-3 [6].
Recent studies have highlighted the importance of instruction tuning in empowering LLMs, as exempliÔ¨Åed by the boosting of GPT-3 [6] to InstrctGPT [22] and ChatGPT [20], which follows natural language instructions effectively to accomplish real-world tasks and allows for customizing task-speciÔ¨Åc rules into instructions during downstream Ô¨Åne-tuning, enabling pre-trained models to comprehend user intents more effectively and produce accurate and relevant responses.
Similar attempts have been introduced in multi-modal models as well. LLaMA-Adapter [38] aims to adapt LLaMA [33] into an instruction following model by adding additional adapter modules and multi-modal prompts. Mini-GPT4 [39] follows the architecture of BLIP-2 [15] but replaces the language decoder with Vicuna [9], which supports longer answers. LLaVA [17] utilizes the same CLIP [23] vision encoder and Vicuna [9] language decoder, and Ô¨Ånetunes on their high-quality instruction dataset, curated by GPT-4 [19].
Although these works have achieved excellent results and provided valuable insights, they share a minor common issue. SpeciÔ¨Åcally, they either Ô¨Ånetune the entire model or the connection part on
*Equal Contribution Corresponding Author
Technical Report.

MMC4
billion-scale corpus of images interleaved with text

ü¶©
OpenFlamingo

MIMIC-IT
multi-modal instruction tuning datasets with in-context examples

ü¶¶
Otter

Figure 1: Otter Overview. Otter is a multi-modal model Ô¨Ånetuned on our proposed MIMIC-IT dataset, based on OpenFlamingo. Otter model exhibits the improved ability to execute tasks by following given instructions and leveraging in-context examples.

task-speciÔ¨Åc data. For instance, a common practice is to use image-text data pairs from Caption [16] or VQA [11] tasks to align visual and language modules. While embedding visual information into the language model in this way can be effective, we question whether this practice is inherently task-dependent, as it relies on the task for which the data is used to train the alignment module.
Upon reÔ¨Çection, we have discovered that DeepMind Flamingo‚Äôs [1] upstream pretraining dataset, MultiModal MassiveWeb (M3W), has signiÔ¨Åcant value in aligning visual and language information in a more natural manner. The dataset comprises HTML webpages, where all images and texts are arranged in an interleaved format. SpeciÔ¨Åcally, a piece of text may describe an image (or videos) above or below it, and correlations may exist between images (or videos) and text in adjacent positions. This natural organization of context provides richer information than a caption dataset, where text only describes its corresponding image. Trained on this dataset, Flamingo achieves zeroand few-shot generalization and in-context learning ability, making it the GPT-3 moment in the multi-modal domain.
However, DeepMind has not released the Flamingo model and its M3W dataset to the public, potentially because of the model‚Äôs exceptional performance that could cause astonishment prematurely. Nevertheless, the LAION-AI‚Äôs OpenFlamingo project [4] has recently been made public, providing access to their corresponding MMC4 [40] dataset in the same interleaved format on a larger scale. Consequently, community researchers can continue to follow the Flamingo series of works for further research and development in the Ô¨Åeld of multi-modal models. Although the OpenFlamingo model exhibits impressive multi-modal in-context learning abilities and executes tasks with given in-context examples, as an upstream pre-trained model, it still requires instruction tuning to perform downstream tasks more effectively.
In our paper, we propose our MultI-Modal In-Context Instruction Tuning (MIMIC-IT) dataset and provide details on its construction in Sec. 3.1. We then introduce Otter, a multi-modal model with in-context instruction tuning based on OpenFlamingo. We illustrate the relationship between Otter and OpenFlamingo in Fig. 1. Finetuned on MIMIC-IT dataset, our Otter model demonstrates improved instruction-following ability compared to OpenFlamingo, as shown in our qualitative analysis in Sec. 4.1. Meanwhile, Otter is capable of learning to execute instructions with provided in-context learning examples, as shown in Sec. 4.2. From the engineering perspective, we optimized OpenFlamingo‚Äôs implementation to make it more accessible to researchers. Our optimizations include optimizing the training requirements from at least 1√ó A100 GPU to only 4√ó RTX3090 GPUs and integrating it into Hugging Face Transformers [34] to simplify training and inference with a few lines of code. Our contributions facilitate further research and development in the Ô¨Åeld of multi-modal models.
We summarize our key contributions as follows:
‚Ä¢ We present the MultI-Modal In-Context Instruction Tuning (MIMIC-IT) dataset. Each data sample includes an instruction-image-answer triplet and its in-context examples.
‚Ä¢ We introduce Otter, a multi-modal model with in-context instruction tuning based on OpenFlamingo, capable of the instruction following and executing new instructions with few in-context learning examples.
‚Ä¢ Our optimizations to OpenFlamingo‚Äôs implementation, including reducing the training requirements to 4√ó RTX3090 GPUs and integrating it into Huggingface Transformers to simplify the training and inference with only a few lines of code.
2

2 Related Work
2.1 Large-scale Multi-modal Models
With the recent success of ChatGPT [20], GPT-4 [19], and other large language models [33, 32, 9], recent studies start to explore incorporating information from other modalities based on pretrained language models. These studies extend the capabilities of language models to more tasks and modalities, and can be categorized into two perspectives:
System Design Perspective. This perspective involves using ChatGPT [20] as a dispatch scheduler and connecting different expert models through it to allow for different visual tasks. Language prompts serve as an interface to call expert visual-language models within their respective task domains. Works in this category include VisualChatGPT [35], HuggingGPT [29], Cola [8], XGPT [42], MM-REACT [37], and ViperGPT [31]. This approach has limitations in that each model cannot be trained individually on new tasks, and using ChatGPT [20] as a powerful instruction dispatch tool can result in high API query costs.
End-to-End Trainable Models Perspective. This perspective focuses on connecting models from different modalities into integrated end-to-end trainable models, also known as multi-modal foundation models. Early works in this Ô¨Åeld include Flamingo [1], which proposes a uniÔ¨Åed architecture for modeling language and vision and was later open-sourced as OpenFlamingo [4] by LAIONAI. Other earlier works include BLIP-2 [15], which uses a lightweight Querying Transformer and two-stage bootstrap pretraining to connect information from the image to text modality. With the popularity of GPT-4 [19], there has been an increased focus on this Ô¨Åeld since 2023. Enterpriselevel product models include OpenAI‚Äôs yet-to-be-released vision-language version of GPT-4 [19], Google‚Äôs PaLM-E [10], Baidu‚Äôs ERNIE [5], Alibaba‚Äôs Tongyi Qianwen [2], and Sensetime‚Äôs SenseNova [27]. Academic multi-modal efforts include a variety of models such as LLaMA-Adapters [38], Mini-GPT4 [39], and LLaVA [17]. LLaMA-Adapters aims to adapt LLaMA [33] into an instructionfollowing model with an additional adapters module and multi-modal prompts. Mini-GPT4 follows BLIP-2‚Äôs [15] architecture but replaces the language decoder with Vicuna [9], which better supports longer responses and multi-round conversations. LLaVA connects text and image modalities through a trainable projector matrix, which is a simple lightweight linear layer. However, since LLaVA trains both the vision encoder and language decoder on their instructing tuning dataset, its cost is relatively high compared to others. In contrast, based on the Flamingo model, Otter trains a few cross-gated attention layers to connect visual and language information and establish attention between in-context examples, leaving the vision encoder and language decoder frozen.
2.2 Multi-modal Instruction Tuning Dataset
The concept of instruction tuning in multi-modal models was Ô¨Årst introduced in Multi-Instruct [36], where 47 diverse multi-modal tasks covering 11 broad categories were organized. Each task comprises at least 5,000 instances (input-output pairs) from existing open-source datasets and 5 expert-written instructions. Multi-Instruct covers most multi-modal tasks that require visual understanding and multi-modal reasoning, such as Visual Question Answering [11, 41, 30], Image Captioning [16], Image Generation [7], and Visual Relationship Understanding [14], among others. Similarly, MiniGPT4 [39] constructs its instruction following dataset by combining Conceptual Caption [28, 7], SBU [21], and LAION [26] with handwritten instruction templates. More recently, LLaVA [17] has brought the quality of an instruction tuning dataset to a higher level, as it was obtained by expanding the original captions of COCO [16] images with handwritten seed instructions using GPT-4 [19] to provide more detailed descriptions and multi-round conversations.
To the best of our knowledge, the above-mentioned works are the only few that considered instruction tuning in multi-modal models. Our approach further differs from them in that we incorporate incontext examples into instruction tuning by grouping multiple similar instructions together to form a contextual example set. We are the Ô¨Årst to propose the in-context instruction tuning paradigm in multi-modal models and to build the corresponding multi-modal in-context instruction tuning datasets.
3

3 Method
In this section, we will introduce the details of the MIMIC-IT dataset in Sec. 3.1, our Otter‚Äôs training details in Sec. 3.2, and the integration with Hugging Fance ecosystem in Sec. 3.3.
3.1 Multi-Modal In-Context Instruction Tuning
The OpenFlamingo framework leverages the interleaved multi-modal MMC4 dataset to emerge in its few-shot, in-context learning capabilities. The MMC4 dataset is composed of image-text pairs derived from individual HTML Ô¨Åles, with signiÔ¨Åcant contextual relationships between different pairs, as depicted in Fig 2(a). An MMC4 training data sample contains (i) a queried image-text pair, where the text typically describes the image, and (ii) context, which includes the remaining image-text pairs from the same HTML Ô¨Åle. The primary training objective of OpenFlamingo is to generate text for the queried image-text pair, and the paradigm of generating query text conditioned on in-context examples ensures OpenFlamingo‚Äôs in-context learning capacity during the inference phase.
Our Multi-Modal In-Context Instruction Tuning (MIMIC-IT) dataset aims to augment OpenFlamingo‚Äôs instruction comprehension capabilities while preserving its in-context learning capacity. To unleash OpenFlamingo‚Äôs instruction-following potential, we compile data from visual-language tasks into image-instruction-answer triplets. Concurrently, to maintain OpenFlamingo‚Äôs in-context learning capacity, we retrieve in-context examples for each triplet, which often lack correlated context, such as a visual question-answer data sample in VQAv2 [3]. SpeciÔ¨Åcally, each MIMIC-IT data sample consists of (i) a queried image-instruction-answer triplet, with the instruction-answer tailored to the image, and (ii) context. The context contains a series of image-instruction-answer triplets that contextually correlate with the queried triplet, emulating the relationship between the context and the queried image-text pair found in the MMC4 dataset. The training objective for MIMIC-IT is to generate the answer within the queried image-instruction-answer triplet. The image-instruction-answer triplets are derived from (i) visual question-answer datasets, namely, VQAv2 [3] and GQA [13], (ii) visual instruction datasets, such as LLaVA [17], (iii) an in-progress, high-quality panoptic video scene graph dataset from the PVSG repository. For each video, we select 4-8 frames for instruction-following annotation, using the LLaVA dataset as a reference. We have developed three heuristics to construct the context for each image-instruction-answer triplet, as illustrated in Fig 2(b).
3.2 Training Details
Our approach adopts the OpenFlamingo training paradigm to train the Otter model. The pretrained OpenFlamingo model comprises a LLaMA-7B [33] language encoder and a CLIP ViT-L/14 [24] vision encoder. To prevent overÔ¨Åtting and leverage pretrained knowledge, we freeze both the encoders and only Ô¨Ånetune the Perceiver resampler module, cross-attention layers inserted into the language encoder and input/output embeddings of the language encoder. This results in approximately 1.3 billion trainable parameters for the Otter model.
To optimize our model, we employ the AdamW optimizer [18] with a starting learning rate of 10‚àí5 and a batch size of 4. We train Otter for 6 epochs, with the learning rate scheduled using a cosine annealing scheduler. We also use gradient clipping of a threshold of 1.0 to prevent exploding gradients.
During our training, we follow a speciÔ¨Åc format to prepare our training data. The format includes a combination of image, user instruction, "GPT"-generated answers 1, and a special token known as the [endofchunk] token. We format the training data as follows:
<context> [image] User:<instruction> GPT:[answers] <answer>.[endofchunk] where the [image], [answer], and [endofchunk] tokens are unique and serve a speciÔ¨Åc purpose. We design such a chatbot-like format to train our model to improve the instruction-following and conversation generalizability of the model. The [image] and [endofchunk] tokens are originally from the OpenFlamingo training paradigm, while the [answer] token is a new introduction by us in training Otter. The [answer] token separates the answers from the instruction, so that, we mask all tokens after the [answer] token during training and set them as the prediction objectives of the model. We train our model using a cross-entropy loss.
1To support user-assistant conversations, we adopt "GPT" as the role label because it does not have any speciÔ¨Åc semantic meaning in vocabulary.
4

Title: WALNUT AND BLUE CHEESE STUFFED MUSHROOMS
Text: ‚ÄúThis Walnut and Blue Cheese Stuffed Mushrooms recipe is sponsored by Fisher Nuts.‚Äù,

Title: HONDA: (UN)LOKING THE DOORS FROM THE INSIDE
Text: ‚ÄúWhen you lock/unlock the driver‚Äôs door and tailgate using the master lock switch, all the other doors lock/ unlock at the same time.‚Äù

Text: ‚ÄúThe ideas for stuffing mushrooms are endless, so many combinations to play with, a couple of my personal favourites are these Mediterranean Stuffed Mushrooms and these Spinach and Toasted Pine Nut Stuffed Mushrooms.‚Äù,

Text: ‚ÄúWhen you lock the door using the lock tab on the driver‚Äôs door, all of the other doors and tailgate lock at the same time.‚Äù,

ÔºàaÔºâ Multimodal C4 (MMC4)

same instruction, different images

same image, different instructions

in-context examples

Instruction: What skill set do snowboarders need to perform such tricks?

Instruction: What skill set does this player need to have\in a match, and why?

query
Instruction: What skill set does a player need to excel in this sport, based on the image?

in-context examples

Instruction: What is the baseball player holding in his hands?

Instruction: What color is the baseball player's uniform?

query
Instruction: Which team does the baseball player belong to?

‚Ä¶

‚Ä¶

Answer: To perform tricks ‚Ä¶ minimize the risks involved

Answer: A tennis player needs ‚Ä¶winning matches.

Answer: To improve his performance, ‚Ä¶his team.

Answer: The baseball player is holding a baseball bat in his hands.

Answer: The baseball player's uniform is grey.

Answer: The baseball player belongs to the Angels baseball team.

sequential images, different instructions

in-context examples

Instruction: What is the main thing happening in this picture?

Instruction: Why did the player in red who was attacking fall to the ground?

Instruction: Why is the man in the red jersey about to stand up from the ground?

Instruction: Description of the videos humorous moment?

query
Instruction: Why is the whole video humorous?

Answer: A group of ‚Ä¶attack and look for an opportunity to shoot.

Answer: Because he ..the ground to try to create a penalty.

Answer: Because ‚Ä¶the ball go in and didn't need to pretend to create a penalty.

Answer: A man on the pitch falls down after a shot, ‚Ä¶ \with his teammates.

Answer: The funny thing ‚Ä¶ it's funny how his injury seems to heal in an instant.

ÔºàbÔºâ Multi-Modal In-Context Instruction Tuning (MIMIC-IT)

Figure 2: Illustration of example data formats in MMC4 and MIMIC-IT. (a) The illustration of the data format in the MMC4 dataset that are used OpenFlamingo. (b) Three heuristics to build the multi-modal in-Context instruction tuning (MIMIC-IT) dataset.

5

3.3 Integratation with Hugging Face
We have integrated Otter into Hugging Face Transformers [34] and trained it using the Hugging Face Accelerator2, which enables automatic mapping of the model weights to different GPU devices and ofÔ¨Çoading of overÔ¨Çowed weights to CPU or disk. Additionally, we use bf16 mixed precision during training. The total optimizations enable our model to be trained on 4√óRTX-3090 GPUs, each with 24GB memory.
Meanwhile, since Otter has been integrated into Hugging Face Transformers, it can now be reused with less than Ô¨Åve lines of code, making it much easier for researchers to integrate into their respective training and inference pipelines (compared to the original OpenFlamingo implementation). We also provide the support of Fully Sharded Data Parallel (FSDP) and DeepSpeed to enable greater training efÔ¨Åciency and less memory consumption.
To enable future research and convenience, we also provide a script for converting the original OpenFlamingo-9B checkpoint into the Hugging Face Model format. The converted checkpoint and our Otter model are uploaded and available on the Hugging Face model hub luodian/ openflamingo-9b-hf and luodian/otter-9b-hf, respectively.
4 Demonstrations
In this section, we show several examples of two types of demonstrations of Otter: (1) the ability to follow instructions in Sec. 4.1, and (2) the ability of learning to execute new instructions following provided in-context examples in Sec. 4.2. Compared with OpenFlamingo, these results demonstrate the importance of in-context instruction tuning and the improvement of Otter.
4.1 Following User Instructions
In Sec. 3.1, we discussed how we Ô¨Ånetuned Otter on visual instruction pairs to transform it into a powerful instruction follower. The results of our experiments are demonstrated in Fig. 3, where we observe that Otter is able to provide more detailed descriptions of images and follow user instructions more accurately. This characteristic of Otter is attributed to the co-design of our model and data, which leverages the generalization ability of a strong language decoder and the rich variety of instructions present in the MIMIC-IT dataset. By Ô¨Åne-tuning on visual instruction pairs, Otter is able to learn the nuances of human language and accurately apply it to visual input.
In order to evaluate Otter‚Äôs reasoning capabilities, we designed a series of experiments to test its ability to handle complex scenarios that require a deeper understanding of the situation and more advanced commonsense reasoning. In Fig. 4, we present an example where OpenFlamingo fails to identify important clues in the image, while Otter is able to demonstrate a deeper understanding of the scene and apply relevant commonsense knowledge to answer the given question. In this particular example, Otter is able to correctly interpret the confusion around the trafÔ¨Åc signal, while also taking into account relevant contextual factors such as accidents, trafÔ¨Åc congestion, and disruptions to the Ô¨Çow of trafÔ¨Åc. This level of reasoning ability is not present in standard image captioning models and represents a signiÔ¨Åcant advance in the Ô¨Åeld of computer vision and natural language processing.
4.2 Multi-Modal In-context Learning
Otter is designed to support multi-modal in-context learning in a similar pattern to Flamingo [1] and OpenFlamingo [4], which involves conditioning the language model on the corresponding media, such as an image that corresponds to a caption or a question-answer pair. In Fig. 5, we demonstrate Otter‚Äôs ability to perform visual question answering tasks, where users provide examples of instruction-answering pairs and images as contextual examples to prompt the model with subtasks, such as explaining memes or the danger of sports.
Despite the fact that user inputs are often short and restricted in these scenarios, Otter is able to provide more comprehensive answers. By leveraging the information contained in the images and other contextual examples, Otter is able to better understand the underlying meaning and context of the questions, resulting in more accurate and informative answers.
2Hugging Face Accelerator
6

Instruction: Can you describe the main features of this image for me? OpenFlamingo: The man is flying a kite. Otter: The image shows a person standing in front of a kite that is flying in the sky. The person is wearing a black shirt and is positioned on the left side of the kite. The kite is quite large and can be seen above the person's head. The scene appears to be outdoors, with the person possibly preparing to launch the kites or enjoying the moment as the kit flies in the breeze. There are also two other people in the background, one on the right side and another person partially visible on the far right. The presence of multiple people in this outdoor setting suggests that it might be a gathering or a popular location for kite flying and leisure activities.
Instruction: Write a detailed description of the given image? OpenFlamingo: Pancakes with bacon and maple syrup. Otter: The image features a wooden table with various items on it. There is a sandwhich, which has been cut in half, and a bowl of cereal placed near the plate. Additionally, there is a cup, likely containing a beverage such as coffee. The table also has a ""live baccy"" container and a mug present, suggesting a cozy and relaxed setting. The scene might be a part of a breakfast or snack time arrangement, with the sandwich, cereals, and beverage as the main items on the table.
Figure 3: Comparisons of image captioning results between OpenFlamingo and Otter. Otter provides more detailed and precise descriptions of the images.
Instruction: What could happen if drivers become confused by the traffic signals at this intersection? OpenFlamingo: They could get stuck in the rain. Otter: If drivers become confused by the traffic signals at the intersection, it could lead to various potential scenarios: 1. Red light: The red light signal is on, and the car is moving. In this case, the driver might continue driving, potentially causing a redlight runner accident, which can be dangerous for pedestrians and other vehicles. 2. Green light: If the green light signal appears while the car has already started moving, the vehicle might continue moving, potentially leading to a green-light running accident, where the driver does not stop at the green signal and risks colliding with other vehicles or pedestrian crossings. 3. Confusion about the signal: If drivers are unsure about the traffic lights, they might hesitate or make an unpredictable maneuverer, which could lead not only to accidents but also to traffic congestion and general disruption of the flow of traffic.
Figure 4: Comparisons of situation understanding results between OpenFlamingo and Otter. Otter is able to demonstrate a deeper understanding of the scene and apply relevant commonsense knowledge to answer the given question.
7

User provided examples

Otter reaction

Instruction: what does this meme usually use for? Answer: to ask people to think twice and dont be silly

Instruction: what does this meme usually use for? Answer: to express users feeling of sad, or moving.

User provided examples

Instruction: what does this meme usually use for? Otter: This meme typically uses a sad-looking baby as the main image, with text encouraging people to stop being a baby and to be more like a tough, cool individual. The image and text are meant to create a playful and humorous contrast between a cute, vulnerable baby and a tuff, strong person.
Otter reaction

Instruction: What is the danger of this sport? Answer: The player may get hitted by the tennis ball.

Instruction: What is the danger of this sport? Answer: While chasing the baseball, the player may inadvertently collide with other players.

Instruction: What is the danger of this sport? Otter: The sport involves players running and trying to catch the ball while others are standing in the grass, which can lead to collisions or accidents. These collisions can result in injuries to the players involved. It is essential for players to be aware of their surroundings and maintain a safe distance from each other to minimize the risk of accidents during the game.

Figure 5: In-context learning examples of Otter. Given two examples of instruction-answering pairs and an image, Otter is able to provide more comprehensive answers.

5 Conclusion
In this work, we propose Otter, a multi-modal in-context learning foundation model with instruction tuning. Through partial Ô¨Ånetuning on MIMIC-IT dataset, we observe that Otter can convert OpenFlamingo into a zero-shot visual instruction model with strong in-context learning abilities. With the assistance of rich instructions from images and videos, Otter generalizes to achieve better instruction-following and situation-understanding performances.
5.1 Limitations
Language Hallucination. Since Otter is built upon OpenFlamingo, which depends upon LLaMA, the hallucination issue of LLaMA is inherited by Otter. Also, the current Otter model may hallucinate the language that is not related to the image. This issue might be solved by introducing negative examples in the training data.
5.2 Future Supports
In the future, we plan to explore the integration of more efÔ¨Åcient training schemas (e.g., parameterefÔ¨Åcient Ô¨Ånetuning such as LoRA [12]) and more modalities (e.g., 3D vision). Acknowledgement. We thank Chunyuan Li and Jack Hessel for their advice and support, as well as the OpenFlamingo team for their great contribution to the open-source community.

8

References
[1] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visual language model for few-shot learning. Advances in Neural Information Processing Systems, 35:23716‚Äì23736, 2022. 2, 3, 6
[2] Alibaba. Tongyi qianwen. 2023. 3
[3] Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C Lawrence Zitnick, and Devi Parikh. Vqa: Visual question answering. In Proceedings of the IEEE international conference on computer vision, pages 2425‚Äì2433, 2015. 4
[4] Anas Awadalla, Irena Gao, Joshua Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Kalyani Marathe, Yonatan Bitton, Samir Gadre, Jenia Jitsev, Simon Kornblith, Pang Wei Koh, Gabriel Ilharco, Mitchell Wortsman, and Ludwig Schmidt. OpenÔ¨Çamingo, March 2023. 2, 3, 6
[5] Baidu. Ernie bot: Enhanced representation through knowledge integration. 2023. 3
[6] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877‚Äì1901, 2020. 1
[7] Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut. Conceptual 12m: Pushing web-scale image-text pre-training to recognize long-tail visual concepts. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3558‚Äì3568, 2021. 3
[8] Liangyu Chen, Bo Li, Sheng Shen, Jingkang Yang, Chunyuan Li, Kurt Keutzer, Trevor Darrell, and Ziwei Liu. Language models are visual reasoning coordinators. In ICLR 2023 Workshop on Mathematical and Empirical Understanding of Foundation Models, 2023. 3
[9] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, March 2023. 1, 3
[10] Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, et al. Palm-e: An embodied multimodal language model. arXiv preprint arXiv:2303.03378, 2023. 3
[11] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the v in vqa matter: Elevating the role of image understanding in visual question answering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 6904‚Äì6913, 2017. 2, 3
[12] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021. 8
[13] Drew A Hudson and Christopher D Manning. Gqa: A new dataset for real-world visual reasoning and compositional question answering. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 6700‚Äì6709, 2019. 4
[14] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A Shamma, et al. Visual genome: Connecting language and vision using crowdsourced dense image annotations. International journal of computer vision, 123:32‚Äì73, 2017. 3
[15] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping languageimage pre-training with frozen image encoders and large language models. arXiv preprint arXiv:2301.12597, 2023. 1, 3
[16] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll√°r, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer Vision‚ÄìECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13, pages 740‚Äì755. Springer, 2014. 2, 3
9

[17] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. arXiv preprint arXiv:2304.08485, 2023. 1, 3, 4
[18] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. 4
[19] OpenAI. Gpt-4 technical report. 2023. 1, 3
[20] OpenAI. Introducing chatgpt. 2023. 1, 3
[21] Vicente Ordonez, Girish Kulkarni, and Tamara Berg. Im2text: Describing images using 1 million captioned photographs. Advances in neural information processing systems, 24, 2011. 3
[22] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35:27730‚Äì27744, 2022. 1
[23] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 8748‚Äì8763. PMLR, 2021. 1
[24] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 8748‚Äì8763. PMLR, 2021. 4
[25] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. 1
[26] Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m: Open dataset of clip-Ô¨Åltered 400 million image-text pairs. arXiv preprint arXiv:2111.02114, 2021. 3
[27] SenseTime. Sense nova. 2023. 3
[28] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2556‚Äì2565, 2018. 3
[29] Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang. Hugginggpt: Solving ai tasks with chatgpt and its friends in huggingface. arXiv preprint arXiv:2303.17580, 2023. 3
[30] Alane Suhr, Mike Lewis, James Yeh, and Yoav Artzi. A corpus of natural language for visual reasoning. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 217‚Äì223, 2017. 3
[31] D√≠dac Sur√≠s, Sachit Menon, and Carl Vondrick. Vipergpt: Visual inference via python execution for reasoning. arXiv preprint arXiv:2303.08128, 2023. 3
[32] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model. https://github.com/tatsu-lab/stanford_alpaca, 2023. 3
[33] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth√©e Lacroix, Baptiste Rozi√®re, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efÔ¨Åcient foundation language models. arXiv preprint arXiv:2302.13971, 2023. 1, 3, 4
10

[34] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, R√©mi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. Transformers: State-of-theart natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 38‚Äì45, Online, October 2020. Association for Computational Linguistics. 2, 6
[35] Chenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang, Zecheng Tang, and Nan Duan. Visual chatgpt: Talking, drawing and editing with visual foundation models. arXiv preprint arXiv:2303.04671, 2023. 3
[36] Zhiyang Xu, Ying Shen, and Lifu Huang. Multiinstruct: Improving multi-modal zero-shot learning via instruction tuning. arXiv preprint arXiv:2212.10773, 2022. 3
[37] Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Ehsan Azarnasab, Faisal Ahmed, Zicheng Liu, Ce Liu, Michael Zeng, and Lijuan Wang. Mm-react: Prompting chatgpt for multimodal reasoning and action. arXiv preprint arXiv:2303.11381, 2023. 3
[38] Renrui Zhang, Jiaming Han, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng Li, Peng Gao, and Yu Qiao. Llama-adapter: EfÔ¨Åcient Ô¨Åne-tuning of language models with zero-init attention. arXiv preprint arXiv:2303.16199, 2023. 1, 3
[39] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592, 2023. 1, 3
[40] Wanrong Zhu, Jack Hessel, Anas Awadalla, Samir Yitzhak Gadre, Jesse Dodge, Alex Fang, Youngjae Yu, Ludwig Schmidt, William Yang Wang, and Yejin Choi. Multimodal C4: An open, billion-scale corpus of images interleaved with text. arXiv preprint arXiv:2304.06939, 2023. 2
[41] Yuke Zhu, Oliver Groth, Michael Bernstein, and Li Fei-Fei. Visual7w: Grounded question answering in images. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 4995‚Äì5004, 2016. 3
[42] Xueyan Zou, Zi-Yi Dou, Jianwei Yang, Zhe Gan, Linjie Li, Chunyuan Li, Xiyang Dai, Harkirat Behl, Jianfeng Wang, Lu Yuan, et al. Generalized decoding for pixel, image, and language. arXiv preprint arXiv:2212.11270, 2022. 3
11

