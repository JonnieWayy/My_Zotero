1

A Survey of Large Language Models

Wayne Xin Zhao, Kun Zhou*, Junyi Li*, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, Yifan Du, Chen Yang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang, Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu, Peiyu Liu, Jian-Yun Nie and Ji-Rong Wen

arXiv:2303.18223v11 [cs.CL] 29 Jun 2023

Abstract‚ÄîEver since the Turing Test was proposed in the 1950s, humans have explored the mastering of language intelligence by machine. Language is essentially a complex, intricate system of human expressions governed by grammatical rules. It poses a significant challenge to develop capable artificial intelligence (AI) algorithms for comprehending and grasping a language. As a major approach, language modeling has been widely studied for language understanding and generation in the past two decades, evolving from statistical language models to neural language models. Recently, pre-trained language models (PLMs) have been proposed by pretraining Transformer models over large-scale corpora, showing strong capabilities in solving various natural language processing (NLP) tasks. Since the researchers have found that model scaling can lead to an improved model capacity, they further investigate the scaling effect by increasing the parameter scale to an even larger size. Interestingly, when the parameter scale exceeds a certain level, these enlarged language models not only achieve a significant performance improvement, but also exhibit some special abilities (e.g., incontext learning) that are not present in small-scale language models (e.g., BERT). To discriminate the language models in different parameter scales, the research community has coined the term large language models (LLM) for the PLMs of significant size (e.g., containing tens or hundreds of billions of parameters). Recently, the research on LLMs has been largely advanced by both academia and industry, and a remarkable progress is the launch of ChatGPT (a powerful AI chatbot developed based on LLMs), which has attracted widespread attention from society. The technical evolution of LLMs has been making an important impact on the entire AI community, which would revolutionize the way how we develop and use AI algorithms. Considering this rapid technical progress, in this survey, we review the recent advances of LLMs by introducing the background, key findings, and mainstream techniques. In particular, we focus on four major aspects of LLMs, namely pre-training, adaptation tuning, utilization, and capacity evaluation. Furthermore, we also summarize the available resources for developing LLMs and discuss the remaining issues for future directions. This survey provides an up-to-date review of the literature on LLMs, which can be a useful resource for both researchers and engineers.
Index Terms‚ÄîLarge Language Models; Emergent Abilities; Adaptation Tuning; Utilization; Alignment; Capacity Evaluation
‚ú¶

1 INTRODUCTION

‚ÄúThe limits of my language mean the limits of my world.‚Äù ‚ÄîLudwig Wittgenstein
L ANGUAGE is a prominent ability in human beings to express and communicate, which develops in early childhood and evolves over a lifetime [1, 2]. Machines, however, cannot naturally grasp the abilities of understanding and communicating in the form of human language, unless equipped with powerful artificial intelligence (AI) algorithms. It has been a longstanding research challenge to achieve this goal, to enable machines to read, write, and communicate like humans [3].
Technically, language modeling (LM) is one of the major approaches to advancing language intelligence of machines. In general, LM aims to model the generative likelihood of word sequences, so as to predict the probabilities of future (or missing) tokens. The research of LM has received extensive attention in the literature, which can be divided into four major development stages:
‚Ä¢ Statistical language models (SLM). SLMs [4‚Äì7] are de-
‚Ä¢ Version: v11 (major update on June 29, 2023). ‚Ä¢ GitHub link: https://github.com/RUCAIBox/LLMSurvey ‚Ä¢ * K. Zhou and J. Li contribute equally to this work. ‚Ä¢ The authors are mainly with Gaoling School of Artificial Intelligence and
School of Information, Renmin University of China, Beijing, China; JianYun Nie is with DIRO, Universite¬¥ de Montre¬¥al, Canada. Contact e-mail: batmanfly@gmail.com

veloped based on statistical learning methods that rose in the 1990s. The basic idea is to build the word prediction model based on the Markov assumption, e.g., predicting the next word based on the most recent context. The SLMs with a fixed context length n are also called n-gram language models, e.g., bigram and trigram language models. SLMs have been widely applied to enhance task performance in information retrieval (IR) [8, 9] and natural language processing (NLP) [10‚Äì12]. However, they often suffer from the curse of dimensionality: it is difficult to accurately estimate high-order language models since an exponential number of transition probabilities need to be estimated. Thus, specially designed smoothing strategies such as backoff estimation [13] and Good‚ÄìTuring estimation [14] have been introduced to alleviate the data sparsity problem.
‚Ä¢ Neural language models (NLM). NLMs [15‚Äì17] characterize the probability of word sequences by neural networks, e.g., recurrent neural networks (RNNs). As a remarkable contribution, the work in [15] introduced the concept of distributed representation of words and built the word prediction function conditioned on the aggregated context features (i.e., the distributed word vectors). By extending the idea of learning effective features for words or sentences, a general neural network approach was developed to build a unified solution for various NLP tasks [18]. Further, word2vec [19, 20] was proposed to build a simplified shallow neural network for learning distributed word representations, which were demonstrated to be very effective across

2

   

**3737 //D/0D0$$ &K&DKWD*W3*737
,Q,VQWUVXWUFXW*FW3*737
&R&GRHG[H[

 

*3*737 *3*737 *3*737 77
%(%5(757

     
7L7PLPH H

(a) Query=‚ÄùLanguage Model‚Äù







**3377



////DD00$$


&&KKDDWW**3377



77 **3377


,,QQVVWWUUXXFFWW**3377 &&RRGGHH[[









77LLPPHH

(b) Query=‚ÄùLarge Language Model‚Äù

Fig. 1: The trends of the cumulative numbers of arXiv papers that contain the keyphrases ‚Äúlanguage model‚Äù (since June 2018) and ‚Äúlarge language model‚Äù (since October 2019), respectively. The statistics are calculated using exact match by querying the keyphrases in title or abstract by months. We set different x-axis ranges for the two keyphrases, because ‚Äúlanguage models‚Äù have been explored at an earlier time. We label the points corresponding to important landmarks in the research progress of LLMs. A sharp increase occurs after the release of ChatGPT: the average number of published arXiv papers that contain ‚Äúlarge language model‚Äù in title or abstract goes from 0.40 per day to 8.58 per day (Figure 1(b)).

a variety of NLP tasks. These studies have initiated the use of language models for representation learning (beyond word sequence modeling), having an important impact on the field of NLP.
‚Ä¢ Pre-trained language models (PLM). As an early attempt, ELMo [21] was proposed to capture context-aware word representations by first pre-training a bidirectional LSTM (biLSTM) network (instead of learning fixed word representations) and then fine-tuning the biLSTM network according to specific downstream tasks. Further, based on the highly parallelizable Transformer architecture [22] with self-attention mechanisms, BERT [23] was proposed by pretraining bidirectional language models with specially designed pre-training tasks on large-scale unlabeled corpora. These pre-trained context-aware word representations are very effective as general-purpose semantic features, which have largely raised the performance bar of NLP tasks. This study has inspired a large number of follow-up work, which sets the ‚Äúpre-training and fine-tuning‚Äù learning paradigm. Following this paradigm, a great number of studies on PLMs have been developed, introducing either different architectures [24, 25] (e.g., GPT-2 [26] and BART [24]) or improved pre-training strategies [27‚Äì29]. In this paradigm, it often requires fine-tuning the PLM for adapting to different downstream tasks.
‚Ä¢ Large language models (LLM). Researchers find that scaling PLM (e.g., scaling model size or data size) often leads to an improved model capacity on downstream tasks (i.e., following the scaling law [30]). A number of studies have explored the performance limit by training an ever larger PLM (e.g., the 175B-parameter GPT-3 and the 540Bparameter PaLM). Although scaling is mainly conducted in model size (with similar architectures and pre-training tasks), these large-sized PLMs display different behaviors from smaller PLMs (e.g., 330M-parameter BERT and 1.5Bparameter GPT-2) and show surprising abilities (called emergent abilities [31]) in solving a series of complex tasks. For example, GPT-3 can solve few-shot tasks through in-context

learning, whereas GPT-2 cannot do well. Thus, the research community coins the term ‚Äúlarge language models (LLM)‚Äù1 for these large-sized PLMs [32‚Äì35], which attract increasing research attention (See Figure 1). A remarkable application of LLMs is ChatGPT2 that adapts the LLMs from the GPT series for dialogue, which presents an amazing conversation ability with humans. We can observe a sharp increase of the arXiv papers that are related to LLMs after the release of ChatGPT in Figure 1.
In the existing literature, PLMs have been widely discussed and surveyed [36‚Äì39], while LLMs are seldom reviewed in a systematic way. To motivate our survey, we first highlight three major differences between LLMs and PLMs. First, LLMs display some surprising emergent abilities that may not be observed in previous smaller PLMs. These abilities are key to the performance of language models on complex tasks, making AI algorithms unprecedently powerful and effective. Second, LLMs would revolutionize the way that humans develop and use AI algorithms. Unlike small PLMs, the major approach to accessing LLMs is through the prompting interface (e.g., GPT-4 API). Humans have to understand how LLMs work and format their tasks in a way that LLMs can follow. Third, the development of LLMs no longer draws a clear distinction between research and engineering. The training of LLMs requires extensive practical experiences in large-scale data processing and distributed parallel training. To develop capable LLMs, researchers have to solve complicated engineering issues, working with engineers or being engineers.
Nowadays, LLMs are posing a significant impact on the AI community, and the advent of ChatGPT and GPT-4 leads to the rethinking of the possibilities of artificial general intelligence (AGI). OpenAI has published a technical article entitled ‚ÄúPlanning for AGI and beyond‚Äù, which discusses the short-term and long-term plans to approach AGI [40],
1. Note that a LLM is not necessarily more capable than a small PLM, and emergent abilities may not occur in some LLMs.
2. https://openai.com/blog/chatgpt/

3

and a more recent paper has argued that GPT-4 might be considered as an early version of an AGI system [41]. The research areas of AI are being revolutionized by the rapid progress of LLMs. In the field of NLP, LLMs can serve as a general-purpose language task solver (to some extent), and the research paradigm has been shifting towards the use of LLMs. In the field of IR, traditional search engines are challenged by the new information seeking way through AI chatbots (i.e., ChatGPT), and New Bing3 presents an initial attempt that enhances the search results based on LLMs. In the field of CV, the researchers try to develop ChatGPT-like vision-language models that can better serve multimodal dialogues [42‚Äì45], and GPT-4 [46] has supported multimodal input by integrating the visual information. This new wave of technology would potentially lead to a prosperous ecosystem of real-world applications based on LLMs. For instance, Microsoft 365 is being empowered by LLMs (i.e., Copilot) to automate the office work, and OpenAI supports the use of plugins in ChatGPT for implementing special functions.
Despite the progress and impact, the underlying principles of LLMs are still not well explored. Firstly, it is mysterious why emergent abilities occur in LLMs, instead of smaller PLMs. As a more general issue, there lacks a deep, detailed investigation of the key factors that contribute to the superior abilities of LLMs. It is important to study when and how LLMs obtain such abilities [47]. Although there are some meaningful discussions about this problem [31, 47], more principled investigations are needed to uncover the ‚Äúsecrets‚Äú of LLMs. Secondly, it is difficult for the research community to train capable LLMs. Due to the huge demand of computation resources, it is very costly to carry out repetitive, ablating studies for investigating the effect of various strategies for training LLMs. Indeed, LLMs are mainly trained by industry, where many important training details (e.g., data collection and cleaning) are not revealed to the public. Thirdly, it is challenging to align LLMs with human values or preferences. Despite the capacities, LLMs are also likely to produce toxic, fictitious, or harmful contents. It requires effective and efficient control approaches to eliminating the potential risk of the use of LLMs [46].
Faced with both opportunities and challenges, it needs more attention on the research and development of LLMs. In order to provide a basic understanding of LLMs, this survey conducts a literature review of the recent advances in LLMs from four major aspects, including pre-training (how to pretrain a capable LLM), adaptation (how to effectively adapt pre-trained LLMs for better use), utilization (how to use LLMs for solving various downstream tasks) and capability evaluation (how to evaluate the abilities of LLMs and existing empirical findings). We thoroughly comb the literature and summarize the key findings, techniques, and methods of LLMs. For this survey, we also create a GitHub project website by collecting the supporting resources for LLMs, at the link https://github.com/RUCAIBox/LLMSurvey. We are also aware of several related review articles on PLMs or LLMs [32, 36, 38, 39, 43, 48‚Äì54]. These papers either discuss PLMs or some specific (or general) aspects of LLMs. Compared with them, we focus on the techniques and
3. https://www.bing.com/new

methods to develop and use LLMs and provide a relatively comprehensive reference to important aspects of LLMs.
The remainder of this survey is organized as follows: Section 2 introduces the background for LLMs and the evolution of GPT-series models, followed by the summarization of available resources for developing LLMs in Section 3. Sections 4, 5, 6, and 7 review and summarize the recent progress from the four aspects of pre-training, adaptation, utilization, and capacity evaluation, respectively. Then, Section 8 discusses the practical guide for prompt design, and Section 9 reviews the applications of LLMs in several representative domains. Finally, we conclude the survey in Section 10 by summarizing the major findings and discuss the remaining issues for future work.
2 OVERVIEW
In this section, we present an overview about the background of LLMs and then summarize the technical evolution of the GPT-series models.
2.1 Background for LLMs
Typically, large language models (LLMs) refer to Transformer language models that contain hundreds of billions (or more) of parameters4, which are trained on massive text data [32], such as GPT-3 [55], PaLM [56], Galactica [35], and LLaMA [57]. LLMs exhibit strong capacities to understand natural language and solve complex tasks (via text generation). To have a quick understanding of how LLMs work, this part introduces the basic background for LLMs, including scaling laws, emergent abilities and key techniques.
Scaling Laws for LLMs. Currently, LLMs are mainly built upon the Transformer architecture [22], where multi-head attention layers are stacked in a very deep neural network. Existing LLMs adopt similar Transformer architectures and pre-training objectives (e.g., language modeling) as small language models. However, LLMs significantly extend the model size, data size, and total compute (orders of magnification). Extensive research has shown that scaling can largely improve the model capacity of LLMs [26, 55, 56]. Thus, it is useful to establish a quantitative approach to characterizing the scaling effect. Next, we introduce two representative scaling laws for Transformer language models [30, 34].
‚Ä¢ KM scaling law5. In 2020, Kaplan et al. [30] (the OpenAI team) firstly proposed to model the power-law relationship of model performance with respective to three major factors, namely model size (N ), dataset size (D), and the amount of training compute (C), for neural language models. Given
4. In existing literature, there is no formal consensus on the minimum parameter scale for LLMs, since the model capacity is also related to data size and total compute. In this survey, we take a slightly loose definition of LLMs, and mainly focus on discussing language models with a model size larger than 10B.
5. Since there was not a model trained following this law in the original paper, we took the last names of the two co-first authors to name this scaling law.

4

a compute budget c, they empirically presented three basic formulas for the scaling law6:

L(N ) = L(D) = L(C) =

Nc N

Œ±N
,

Œ±N ‚àº 0.076, Nc ‚àº 8.8 √ó 1013

(1)

Dc D

Œ±D
,

Œ±D ‚àº 0.095, Dc ‚àº 5.4 √ó 1013

Cc C

Œ±C
,

Œ±C ‚àº 0.050, Cc ‚àº 3.1 √ó 108

where L(¬∑) denotes the cross entropy loss in nats. The three laws were derived by fitting the model performance with varied data sizes (22M to 23B tokens), model sizes (768M to 1.5B non-embedding parameters) and training compute, under some assumptions (e.g., the analysis of one factor should be not bottlenecked by the other two factors). They showed that the model performance has a strong dependence relation on the three factors.
‚Ä¢ Chinchilla scaling law. As another representative study, Hoffmann et al. [34] (the Google DeepMind team) proposed an alternative form for scaling laws to instruct the computeoptimal training for LLMs. They conducted rigorous experiments by varying a larger range of model sizes (70M to 16B) and data sizes (5B to 500B tokens), and fitted a similar scaling law yet with different coefficients as below [34]:

AB

L(N, D) = E + N Œ± + DŒ≤ ,

(2)

where E = 1.69, A = 406.4, B = 410.7, Œ± = 0.34 and Œ≤ = 0.28. By optimizing the loss L(N, D) under the constraint C ‚âà 6N D, they showed that the optimal allocation of compute budget to model size and data size can be derived as follows:

Nopt(C) = G

C 6

a
,

Dopt(C) = G‚àí1

C 6

b
,

(3)

where

a

=

Œ± Œ±+Œ≤

,

b

=

Œ≤ Œ±+Œ≤

and

G

is

a

scaling

coefficient

that

can be computed by A, B, Œ± and Œ≤. As analyzed in [34],

given an increase in compute budget, the KM scaling law

favors a larger budget allocation in model size than the data

size, while the Chinchilla scaling law argues that the two

sizes should be increased in equal scales, i.e., having similar

values for a and b in Equation (3).

Though with some restricted assumptions, these scaling

laws provide an intuitive understanding of the scaling ef-

fect, making it feasible to predict the performance of LLMs

during training [46]. However, some abilities (e.g., in-context

learning [55]) are unpredictable according to the scaling law,

which can be observed only when the model size exceeds a

certain level (as discussed below).

Emergent Abilities of LLMs. In the literature [31], emergent abilities of LLMs are formally defined as ‚Äúthe abilities that are not present in small models but arise in large models‚Äù, which is one of the most prominent features that distinguish LLMs from previous PLMs. It further introduces a

6. Here, Nc, Dc and Cc are measured in the number of non-
embedding parameters, the number of training tokens and the number
of FP-days, respectively. According to the original paper [30], Cc and C should be denoted by Ccmin and Cmin, corresponding to the optimal use of compute. We use the simplified notations for ease of discussions.

notable characteristic when emergent abilities occur [31]: performance rises significantly above random when the scale reaches a certain level. By analogy, such an emergent pattern has close connections with the phenomenon of phase transition in physics [31, 58]. In principle, emergent abilities can be defined in relation to some complex tasks [31, 59], while we are more concerned with general abilities that can be applied to solve a variety of tasks. Here, we briefly introduce three typical emergent abilities for LLMs and representative models that possess such an ability7.
‚Ä¢ In-context learning. The in-context learning (ICL) ability is formally introduced by GPT-3 [55]: assuming that the language model has been provided with a natural language instruction and/or several task demonstrations, it can generate the expected output for the test instances by completing the word sequence of input text, without requiring additional training or gradient update8. Among the GPTseries models, the 175B GPT-3 model exhibited a strong ICL ability in general, but not the GPT-1 and GPT-2 models. Such an ability also depends on the specific downstream task. For example, the ICL ability can emerge on the arithmetic tasks (e.g., the 3-digit addition and subtraction) for the 13B GPT-3, but 175B GPT-3 even cannot work well on the Persian QA task [31].
‚Ä¢ Instruction following. By fine-tuning with a mixture of multi-task datasets formatted via natural language descriptions (called instruction tuning), LLMs are shown to perform well on unseen tasks that are also described in the form of instructions [28, 61, 62]. With instruction tuning, LLMs are enabled to follow the task instructions for new tasks without using explicit examples, thus having an improved generalization ability. According to the experiments in [62], instruction-tuned LaMDA-PT [63] started to significantly outperform the untuned one on unseen tasks when the model size reached 68B, but not for 8B or smaller model sizes. A recent study [64] found that a model size of 62B is at least required for PaLM to perform well on various tasks in four evaluation benchmarks (i.e., MMLU, BBH, TyDiQA and MGSM), though a much smaller size might suffice for some specific tasks (e.g., MMLU).
‚Ä¢ Step-by-step reasoning. For small language models, it is usually difficult to solve complex tasks that involve multiple reasoning steps, e.g., mathematical word problems. In contrast, with the chain-of-thought (CoT) prompting strategy [33], LLMs can solve such tasks by utilizing the prompting mechanism that involves intermediate reasoning steps for deriving the final answer. This ability is speculated to be potentially obtained by training on code [33, 47]. An empirical study [33] has shown that CoT prompting can bring performance gains (on arithmetic reasoning benchmarks) when applied to PaLM and LaMDA variants with a model size larger than 60B, while its advantage over the standard prompting becomes more evident when the model size exceeds 100B. Furthermore, the performance
7. It is difficult to accurately examine the critical size for emergent abilities of LLMs (i.e., the minimum size to possess an ability), since it might vary for different models or tasks. Also, existing studies often test emergent abilities on very limited model sizes for a specific LLM. For example, PaLM is often tested with three sizes of 8B, 62B and 540B. It is unclear about the model performance of the untested sizes.
8. In a recent study [60], it also shows that in-context learning implicitly performs meta-optimization through the attention mechanism.

5

improvement with CoT prompting seems to be also varied for different tasks, e.g., GSM8K > MAWPS > SWAMP for PaLM [33].
Key Techniques for LLMs. It has been a long way that LLMs evolve into the current state: general and capable learners. In the development process, a number of important techniques are proposed, which largely improve the capacity of LLMs. Here, we briefly list several important techniques that (potentially) lead to the success of LLMs, as follows.
‚Ä¢ Scaling. As discussed in previous parts, there exists an evident scaling effect in Transformer language models: larger model/data sizes and more training compute typically lead to an improved model capacity [30, 34]. As two representative models, GPT-3 and PaLM explored the scaling limits by increasing the model size to 175B and 540B, respectively. Since compute budget is usually limited, scaling laws can be further employed to conduct a more compute-efficient allocation of the compute resources. For example, Chinchilla (with more training tokens) outperforms its counterpart model Gopher (with a larger model size) by increasing the data scale with the same compute budget [34]. In addition, data scaling should be with careful cleaning process, since the quality of pre-training data plays a key role in the model capacity.
‚Ä¢ Training. Due to the huge model size, it is very challenging to successfully train a capable LLM. Distributed training algorithms are needed to learn the network parameters of LLMs, in which various parallel strategies are often jointly utilized. To support distributed training, several optimization frameworks have been released to facilitate the implementation and deployment of parallel algorithms, such as DeepSpeed [65] and Megatron-LM [66‚Äì68]. Also, optimization tricks are also important for training stability and model performance, e.g., restart to overcome training loss spike [56] and mixed precision training [69]. More recently, GPT-4 [46] proposes to develop special infrastructure and optimization methods that reliably predict the performance of large models with much smaller models.
‚Ä¢ Ability eliciting. After being pre-trained on large-scale corpora, LLMs are endowed with potential abilities as general-purpose task solvers. These abilities might not be explicitly exhibited when LLMs perform some specific tasks. As the technical approach, it is useful to design suitable task instructions or specific in-context learning strategies to elicit such abilities. For instance, chain-of-thought prompting has been shown to be useful to solve complex reasoning tasks by including intermediate reasoning steps. Furthermore, we can perform instruction tuning on LLMs with task descriptions expressed in natural language, for improving the generalizability of LLMs on unseen tasks. These eliciting techniques mainly correspond to the emergent abilities of LLMs, which may not show the same effect on small language models.
‚Ä¢ Alignment tuning. Since LLMs are trained to capture the data characteristics of pre-training corpora (including both high-quality and low-quality data), they are likely to generate toxic, biased, or even harmful content for humans. It is necessary to align LLMs with human values, e.g., helpful, honest, and harmless. For this purpose, InstructGPT [61]

designs an effective tuning approach that enables LLMs to follow the expected instructions, which utilizes the technique of reinforcement learning with human feedback [61, 70]. It incorporates human in the training loop with elaborately designed labeling strategies. ChatGPT is indeed developed on a similar technique to InstructGPT, which shows a strong alignment capacity in producing high-quality, harmless responses, e.g., rejecting to answer insulting questions.
‚Ä¢ Tools manipulation. In essence, LLMs are trained as text generators over massive plain text corpora, thus performing less well on the tasks that are not best expressed in the form of text (e.g., numerical computation). In addition, their capacities are also limited to the pre-training data, e.g., the inability to capture up-to-date information. To tackle these issues, a recently proposed technique is to employ external tools to compensate for the deficiencies of LLMs [71, 72]. For example, LLMs can utilize the calculator for accurate computation [71] and employ search engines to retrieve unknown information [72]. More recently, ChatGPT has enabled the mechanism of using external plugins (existing or newly created apps)9, which are by analogy with the ‚Äúeyes and ears‚Äù of LLMs. Such a mechanism can broadly expand the scope of capacities for LLMs.
In addition, many other factors (e.g., the upgrade of hardware) also contribute to the success of LLMs. Currently, we limit our discussion to the major technical approaches and key findings for developing LLMs.
2.2 Technical Evolution of GPT-series Models
Due to the excellent capacity in communicating with humans, ChatGPT has ignited the excitement of the AI community since its release. ChatGPT is developed based on the powerful GPT model with specially optimized conversation capacities. Considering the ever-growing interest in ChatGPT and GPT models, we add a special discussion about the technical evolution of the GPT-series models, to briefly summarize the progress how they have been developed in the past years. The basic principle underlying GPT models is to compress the world knowledge into the decoder-only Transformer model by language modeling, such that it can recover (or memorize) the semantics of world knowledge and serve as a general-purpose task solver. Two key points to the success are (I) training decoder-onlly Transformer language models that can accurately predict the next word and (II) scaling up the size of language models. Overall, the research of OpenAI on LLMs can be roughly divided into the following stages10.
Early Explorations. According to one interview with Ilya Sutskever11 (a co-founder and chief scientist of OpenAI), the idea of approaching intelligent systems with language models was already explored in the early days of OpenAI, while it was attempted with recurrent neural networks (RNN) [104]. With the advent of Transformer, OpenAI
9. https://openai.com/blog/chatgpt-plugins 10. Note that the discussion of this part can be somewhat subjective. The overall viewpoints and summaries are made based on the understanding of the survey authors by reading the papers, blog articles, interview reports and APIs released by OpenAI. 11. https://hackernoon.com/an-interview-with-ilya-sutskever-cofounder-of-openai

6

TABLE 1: Statistics of large language models (having a size larger than 10B in this survey) in recent years, including the capacity evaluation, pre-training data scale (either in the number of tokens or storage size) and hardware resource costs. In this table, we only include LLMs with a public paper about the technical details. Here, ‚ÄúRelease Time‚Äù indicates the date when the corresponding paper was officially released. ‚ÄúPublicly Available‚Äù means that the model checkpoints can be publicly accessible while ‚ÄúClosed Source‚Äù means the opposite. ‚ÄúAdaptation‚Äù indicates whether the model has been with subsequent fine-tuning: IT denotes instruction tuning and RLHF denotes reinforcement learning with human feedback. ‚ÄúEvaluation‚Äù indicates whether the model has been evaluated with corresponding abilities in their original paper: ICL denotes in-context learning and CoT denotes chain-of-thought. ‚Äú*‚Äù denotes the largest publicly available version.

Model

Release Size Time (B)

T5 [73]

Oct-2019 11

mT5 [74]

Oct-2020 13

PanGu-Œ± [75]

Apr-2021 13*

CPM-2 [76]

Jun-2021 198

T0 [28]

Oct-2021 11

CodeGen [77]

Mar-2022 16

GPT-NeoX-20B [78] Apr-2022 20

Tk-Instruct [79]

Apr-2022 11

UL2 [80]

May-2022 20

OPT [81]

May-2022 175

NLLB [82]

Jul-2022 54.5

Publicly GLM [83]

Oct-2022 130

Available Flan-T5 [64]

Oct-2022 11

BLOOM [69]

Nov-2022 176

mT0 [84]

Nov-2022 13

Galactica [35]

Nov-2022 120

BLOOMZ [84]

Nov-2022 176

OPT-IML [85]

Dec-2022 175

LLaMA [57]

Feb-2023 65

CodeGeeX [86]

Sep-2022 13

Pythia [87]

Apr-2023 12

Base Model
T5 T5 T5 mT5 BLOOM OPT -

Adaptation Pre-train Latest Data Hardware Training Evaluation IT RLHF Data Scale Timestamp (GPUs / TPUs) Time ICL CoT

-

-

1T tokens Apr-2019 1024 TPU v3

-

‚úì-

-

-

1T tokens

-

-

-

‚úì-

--

1.1TB

-

2048 Ascend 910 -

‚úì-

--

2.6TB

-

-

-

--

‚úì-

-

-

512 TPU v3

27 h ‚úì -

- - 577B tokens

-

-

-

‚úì-

--

825GB

-

96 40G A100

-

‚úì-

‚úì-

-

-

256 TPU v3

4h ‚úì -

-

-

1T tokens Apr-2019 512 TPU v4

-

‚úì‚úì

- - 180B tokens

-

992 80G A100

-

‚úì-

--

-

-

-

-

‚úì-

- - 400B tokens

-

768 40G A100 60 d ‚úì -

‚úì-

-

-

-

-

‚úì‚úì

- - 366B tokens

-

384 80G A100 105 d ‚úì -

‚úì-

-

-

-

-

‚úì-

- - 106B tokens

-

-

-

‚úì‚úì

‚úì-

-

-

-

-

‚úì-

‚úì-

-

-

128 40G A100

-

‚úì‚úì

- - 1.4T tokens

-

2048 80G A100 21 d ‚úì -

- - 850B tokens

-

1536 Ascend 910 60 d ‚úì -

- - 300B tokens

-

256 40G A100

-

‚úì-

GPT-3 [55]

May-2020 175

-

- - 300B tokens

-

-

-

‚úì-

GShard [88]

Jun-2020 600

-

-

-

1T tokens

-

2048 TPU v3

4d

--

Codex [89]

Jul-2021 12 GPT-3 - - 100B tokens May-2020

-

-

‚úì-

ERNIE 3.0 [90]

Jul-2021 10

-

- - 375B tokens

-

384 V100

-

‚úì-

Jurassic-1 [91]

Aug-2021 178

-

- - 300B tokens

-

800 GPU

-

‚úì-

HyperCLOVA [92] Sep-2021 82

-

- - 300B tokens

-

1024 A100

13.4 d ‚úì -

FLAN [62]

Sep-2021 137 LaMDA-PT ‚úì -

-

-

128 TPU v3

60 h ‚úì -

Yuan 1.0 [93]

Oct-2021 245

-

- - 180B tokens

-

2128 GPU

-

‚úì-

Anthropic [94]

Dec-2021 52

-

- - 400B tokens

-

-

-

‚úì-

WebGPT [72]

Dec-2021 175 GPT-3 - ‚úì

-

-

-

-

‚úì-

Gopher [59]

Dec-2021 280

-

- - 300B tokens

-

4096 TPU v3 920 h ‚úì -

ERNIE 3.0 Titan [95] Dec-2021 260

-

--

-

-

-

-

‚úì-

GLaM [96]

Dec-2021 1200

-

- - 280B tokens

-

1024 TPU v4 574 h ‚úì -

Closed Source

LaMDA [63] MT-NLG [97] AlphaCode [98]

Jan-2022 137 Jan-2022 530 Feb-2022 41

-

- - 768B tokens

-

1024 TPU v3 57.7 d - -

- - 270B tokens

-

4480 80G A100

-

‚úì-

- - 967B tokens Jul-2021

-

-

--

InstructGPT [61] Mar-2022 175 GPT-3 ‚úì ‚úì

-

-

-

-

‚úì-

Chinchilla [34]

Mar-2022 70

-

- - 1.4T tokens

-

-

-

‚úì-

PaLM [56]

Apr-2022 540

-

- - 780B tokens

-

6144 TPU v4

-

‚úì‚úì

AlexaTM [99]

Aug-2022 20

-

- - 1.3T tokens

-

128 A100

120 d ‚úì ‚úì

Sparrow [100]

Sep-2022 70

-

-‚úì

-

-

64 TPU v3

-

‚úì-

WeLM [101]

Sep-2022 10

-

- - 300B tokens

-

128 A100 40G 24 d ‚úì -

U-PaLM [102]

Oct-2022 540 PaLM - -

-

-

512 TPU v4

5d ‚úì ‚úì

Flan-PaLM [64]

Oct-2022 540 PaLM ‚úì -

-

-

512 TPU v4

37 h ‚úì ‚úì

Flan-U-PaLM [64] Oct-2022 540 U-PaLM ‚úì -

-

-

-

-

‚úì‚úì

GPT-4 [46]

Mar-2023 -

-

‚úì‚úì

-

-

-

-

‚úì‚úì

PanGu-Œ£ [103]

Mar-2023 1085 PanGu-Œ± - - 329B tokens

-

512 Ascend 910 100 d ‚úì -

7

T5

GShard

Publicly Available

2019

2020

mT5

2021

1-4

GPT-3

Codex

5-8

Anthropic WebGPT

T0 HyperCLOVA

9-10 11-12

Ernie 3.0 Titan

InstructGPT

2022

Gopher GLaM

CodeGen MT-NLG

1-3 OPT

BLOOM mT0
BLOOMZ Galatica

GLM AlexaTM
WeLM

GPT-NeoX-20B Tk-Instruct Cohere

PanGu-ùõÇ
PLUG

Ernie 3.0 Jurassic-1

FLAN Yuan 1.0

LaMDA AlphaCode

CPM-2

Chinchilla

UL2

Sparrow

PaLM

Flan-T5

YaLM

Flan-PaLM

4-6

Luminous

7-10

NLLB

11-12

2023

Falcon CodeGeeX Pythia Vicuna PanGu-Œ£ Bard LLaMA
1-6

OPT-IML

ChatGPT

GPT-4

Fig. 2: A timeline of existing large language models (having a size larger than 10B) in recent years. The timeline was established mainly according to the release date (e.g., the submission date to arXiv) of the technical paper for a model. If there was not a corresponding paper, we set the date of a model as the earliest time of its public release or announcement. We mark the LLMs with publicly available model checkpoints in yellow color. Due to the space limit of the figure, we only include the LLMs with publicly reported evaluation results.

GPT-1
2018.06
decoder-only architecture generative pre-training

GPT-2
2019.02
unsupervised multitask learner scaling the model size

GPT-3
2020.05

+code

Codex
2021.07

in-context learning exploring scaling limits

code pre-training

GPT-3.5
2022.03

GPT-4
2023.03
strong reasoning ability multi-modal ability

code-davinci-002 +instruction text-davinci-002 +RLHF text-davinci-003 +chat gpt-3.5-turbo

2022.03

2022.03

2022.09

2023.03

capable code model

instruction following

human alignment

excellent comprehensive ability

ChatGPT

Fig. 3: A brief illustration for the technical evolution of GPT-series models. We plot this figure mainly based on the papers, blog articles and official APIs from OpenAI. Here, solid lines denote that there exists an explicit evidence (e.g., the official statement that a new model is developed based on a base model) on the evolution path between two models, while dashed lines denote a relatively weaker evolution relation.

developed two initial GPT models, namely GPT-1 [105] and 1 has set up the core architecture for the GPT-series models

GPT-2 [26], which can considered as the foundation to more and established the underlying principle to model natural

powerful models subsequently i.e., GPT-3 and GPT-4.

language text, i.e., predicting the next word.

‚Ä¢ GPT-1. In 2017, the Transformer model [22] was introduced by Google, and the OpenAI team quickly adapted their language modeling work to this new neural network architecture. They released the first GPT model in 2018, i.e., GPT-1 [105], and coined the abbreviation term GPT as the model name, standing for Generative Pre-Training. GPT-1 was developed based on a generative, decoder-only Transformer architecture, and adopted a hybrid approach of unsupervised pretraining and supervised fine-tuning. GPT-

‚Ä¢ GPT-2. Following a similar architecture of GPT-1, GPT-2 [26] increased the parameter scale to 1.5B, which was trained with a large webpage dataset WebText. As claimed in the paper of GPT-2, it sought to perform tasks via unsupervised language modeling, without explicit fine-tuning using labeled data. To motivate the approach, they introduced a probabilistic form for multi-task solving, i.e., p(output|input, task) (similar approaches have been adopted in [106]), which predicts the output conditioned on

8

the input and task information. To model this conditional probability, language text can be naturally employed as a unified way to format input, output and task information. In this way, the process of solving a task can be cast as a word prediction problem for generating the solution text. Further, they introduced a more formal claim for this idea: ‚ÄúSince the (task-specific) supervised objective is the same as the unsupervised (language modeling) objective but only evaluated on a subset of the sequence, the global minimum of the unsupervised objective is also the global minimum of the supervised objective (for various tasks)‚Äù [26]12. A basic understanding of this claim is that each (NLP) task can be considered as the word prediction problem based on a subset of the world text. Thus, unsupervised language modeling could be capable in solving various tasks, if it was trained to have sufficient capacity in recovering the world text. These early discussion in GPT-2‚Äôs paper echoed in the interview of Ilya Sutskever by Jensen Huang: ‚ÄúWhat the neural network learns is some representation of the process that produced the text. This text is actually a projection of the world...the more accurate you are in predicting the next word, the higher the fidelity, the more resolution you get in this process...‚Äù13.
Capacity Leap. Although GPT-2 is intended to be an ‚Äúunsupervised multitask learner‚Äù, it overall has an inferior performance compared with supervised fine-tuning stateof-the-art methods. Because it has a relatively small model size, it has widely fine-tuned in downstream tasks, especially the dialog tasks [107, 108]. Based on GPT-2, GPT-3 demonstrates a key capacity leap by scaling of the (nearly same) generative pre-training architecture.
‚Ä¢ GPT-3. GPT-3 [55] was released in 2020, which scaled the model parameters to an ever larger size of 175B. In the GPT-3‚Äôs paper, it formally introduced the concept of in-context learning (ICL)14, which utilizes LLMs in a fewshot or zero-shot way. ICL can teach (or instruct) LLMs to understand the tasks in the form of natural language text. With ICL, the pre-training and utilization of LLMs converge to the same language modeling paradigm: pre-training predicts the following text sequence conditioned on the context, while ICL predicts the correct task solution, which can be also formatted as a text sequence, given the task description and demonstrations. GPT-3 not only demonstrates very excellent performance in a variety of NLP tasks, but also on a number of specially designed tasks that require the abilities of reasoning or domain adaptation. Although the GPT-3‚Äôs paper does not explicitly discuss the emergent abilities of LLMs, we can observe large performance leap that might transcend the basic scaling law [30], e.g., larger models have significantly stronger ICL ability (illustrated in the original Figure 1.2 of the GPT-3‚Äôs paper [55]). Overall, GPT-3 can be viewed as a remarkable landmark in the journey evolving from PLMs to LLMs. It has empirically proved that scaling the neural networks to a significant size can lead to a huge increase in model capacity.
12. To better understand this sentence, we put some explanation words in parentheses.
13. https://lifearchitect.ai/ilya/ 14. GPT-2 essentially used ICL for unsupervised task learning, though it wasn‚Äôt called ICL at that time.

Capacity Enhancement. Due to the strong capacities, GPT3 has been the base model to develop even more capable LLMs for OpenAI. Overall, OpenAI has explored two major approaches to further improving the GPT-3 model, i.e., training on code data and alignment with human preference, which are detailed as follows.
‚Ä¢ Training on code data. A major limitation of the original GPT-3 model (pre-trained on plain text) lies in the lack of the reasoning ability on complex tasks, e.g., completing the code and solving math problems. To enhance this ability, Codex [89] was introduced by OpenAI in July 2021, which was a GPT model fine-tuned on a large corpus of GitHub code. It demonstrated that Codex can solve very difficult programming problems, and also lead to a significant performance improvement in solving math problems [109]. Further, a contrastive approach [110] to training text and code embedding was reported in January 2022, which was shown to improve a series of related tasks (i.e., linearprobe classification, text search and code search). Actually, the GPT-3.5 models are developed based on a code-based GPT model (i.e., code-davinci-002), which indicates that training on code data is a very useful practice to improve the model capacity of GPT models, especially the reasoning ability. Furthermore, there is also a speculation that training on code data can greatly increase the chain-of-thought prompting abilities of LLMs [47], while it is still worth further investigation with more thorough verification.
‚Ä¢ Human alignment. The related research of human alignment can be dated back to the year 2017 (or earlier) for OpenAI: a blog article entitled ‚Äúlearning from human preferences‚Äù15 was posted on the OpenAI blog describing a work that applied reinforcement learning (RL) to learn from the preference comparisons annotated by humans [70] (similar to the reward training step in the aligning algorithm of InstructGPT in Figure 9). Shortly after the release of this RL paper [70], the paper of the Proximal Policy Optimization (PPO) [111] was published in July 2017, which now has been the foundational RL algorithm for learning from human preferences [61]. Later in January 2020, GPT-2 was finetuned using the aforementioned RL algorithms [70, 111], which leveraged human preferences to improve the capacities of GPT-2 on NLP tasks. In the same year, another work [112] trained a summarization model for optimizing human preferences in a similar way. Based on these prior work, InstructGPT [61] was proposed in January 2022 to improve the GPT-3 model for human alignment, which formally established a three-stage reinforcement learning from human feedback (RLHF) algorithm. Note that it seems that the wording of ‚Äúinstruction tuning‚Äù has seldom been used in OpenAI‚Äôs paper and documentation, which is substituted by supervised fine-tuning on human demonstrations (i.e., the first step of the RLHF algorithm [61]). In addition to improving the instruction following capacity, the RLHF algorithm is particularly useful to mitigate the issues of generating harm or toxic content for LLMs, which is key to the safe deployment of LLMs in practice. OpenAI describes their approach to alignment research in a technical article [113], which has summarized three promising directions: ‚Äútraining AI systems to use human feedback, to assist human evaluation
15. https://openai.com/research/learning-from-human-preferences

9

and to do alignment research‚Äù.
These enhancement techniques lead to the improved GPT-3 models with stronger capacities, which are called GPT-3.5 models by OpenAI (see the discussion about the OpenAI API in Section 3.1).
The Milestones of Language Models. Based on all the exploration efforts, two major milestones have been achieved by OpenAI, namely ChatGPT [114] and GPT-4 [46], which have largely raised the capacity bar of existing AI systems.
‚Ä¢ ChatGPT. In November 2022, OpenAI released the conversation model ChatGPT, based on the GPT models (GPT-3.5 and GPT-4). As the official blog article introduced [114], ChatGPT was trained in a similar way as InstructGPT (called ‚Äúa sibling model to InstructGPT‚Äù in the original post), while specially optimized for dialogue. They reported a difference between the training of ChatGPT and InstructGPT in the data collection setup: human-generated conversations (playing both the roles of user and AI) are combined with the InstructGPT dataset in a dialogue format for training ChatGPT. ChatGPT exhibited superior capacities in communicating with humans: possessing a vast store of knowledge, skill at reasoning on mathematical problems, tracing the context accurately in multi-turn dialogues, and aligning well with human values for safe use. Later on, the plugin mechanism has been supported in ChatGPT, which further extends the capacities of ChatGPT with existing tools or apps. So far, it seems to be the ever most powerful chatbot in the AI history. The launch of ChatGPT has a significant impact on the AI research in the future, which sheds light on the exploration of human-like AI systems.
‚Ä¢ GPT-4. As another remarkable progress, GPT-4 [46] was released in March 2023, which extended the text input to multimodal signals. Overall, GPT-4 has stronger capacities in solving complex tasks than GPT-3.5, showing a large performance improvement on many evaluation tasks. A recent study [41] investigated the capacities of GPT-4 by conducting qualitative tests with human-generated problems, spanning a diverse range of difficult tasks, and showed that GPT-4 can achieve more superior performance than prior GPT models such as ChatGPT. Furthermore, GPT-4 responds more safely to malicious or provocative queries, due to a six-month iterative alignment (with an additional safety reward signal in the RLHF training). In the technical report, OpenAI has emphasized how to safely develop GPT-4 and applied a number of intervention strategies to mitigate the possible issues of LLMs, such as hallucinations, privacy and overreliance. For example, they introduced the mechanism called red teaming [115] to reduce the harm or toxic content generation. As another important aspect, GPT4 has been developed on a well-established deep learning infrastructure with improved optimization methods. They introduced a new mechanism called predictable scaling that can accurately predict the final performance with a small proportion of compute during model training.
Despite the huge progress, there are still limitations with these superior LLMs, e.g., generating hallucinations with factual errors or potentially risky response within some specific context [46]. More limitations or issues of LLMs will be discussed in Section 7. It poses long-standing research challenges to develop more capable, safer LLMs. From

the perspective of engineering, OpenAI has adopted an iterative deployment strategy [116] to develop the models and products by following a five-stage development and deployment life-cycle, which aims to effectively reduce the potential risks of using the models. In the following, we will dive into the technical details in order to have a specific understanding of how they have been developed.
3 RESOURCES OF LLMS
It is by no means an easy job to develop or reproduce LLMs, considering the challenging technical issues and huge demands of computation resources. A feasible way is to learn experiences from existing LLMs and reuse publicly available resources for incremental development or experimental study. In this section, we briefly summarize the publicly available resources for developing LLMs, including model checkpoints (or APIs), corpora and libraries.
3.1 Publicly Available Model Checkpoints or APIs
Given the huge cost of model pre-training, well-trained model checkpoints are critical to the study and development of LLMs for the research community. Since the parameter scale is a key factor to consider for using LLMs, we categorize these public models into two scale levels (i.e., tens of billions of parameters and hundreds of billions of parameters), which is useful for users to identify the suitable resources according to their resource budget. In addition, for inference, we can directly employ public APIs to perform our tasks, without running the model locally. Next, we introduce the publicly available model checkpoints and APIs.
Models with Tens of Billions of Parameters. Most of the models in this category have a parameter scale ranging from 10B to 20B, except LLaMA [57] (containing 65B parameters in the largest version), NLLB [82] (containing 54.5B parameters in the largest version), and Falcon [117] (containing 40B parameters in the largest version). Other models within this range include mT5 [74], PanGu-Œ± [75], T0 [28], GPTNeoX-20B [78], CodeGen [77], UL2 [80], Flan-T5 [64], and mT0 [84]. Among them, Flan-T5 (11B version) can serve as a premier model for research on instruction tuning, since it explores the instruction tuning from three aspects [64]: increasing the number of tasks, scaling the model size, and fine-tuning with chain-of-thought prompting data. Besides, CodeGen (11B version), as an autoregressive language model designed for generating code, can be considered as a good candidate for exploring the code generation ability. It also introduces a new benchmark MTPB [77] specially for multi-turn program synthesis, which is composed by 115 expert-generated problems. To solve these problems, it requires LLMs to acquire sufficient programming knowledge (e.g., math, array operations, and algorithms). As for multilingual tasks, mT0 (13B version) might be a good candidate model, which has been fine-tuned on multilingual tasks with multilingual prompts. Furthermore, PanGu-Œ± [75] shows good performance in Chinese downstream tasks in zero-shot or few-shot settings, which is developed based on the deep learning framework MindSpore [118]. Note that PanGu-Œ± [75] holds multiple versions of models (up to 200B parameters), while the largest public version has 13B

Continue pre-training

Model inheritance Data inheritance

Instruction tuning

+ chinese data

LLaMA
+ chat data

10
Parameter-efficient fine-tuning Full parameter fine-tuning

Open-Chinese-LLaMA

Linly-Chinese-LLaMA
+ chat data Cornucopia

Panda
Chinese LLaMA

Lawyer LLaMA

+ Alpaca data

+ chat data QiZhenGPT
+ task data TaoLi

Chinese Alpaca

Chinese Vicuna
BiLLa

+ synthetic data

Alpaca
Alpaca Lora

+ task data
RLHF Goat
PKU-Beaver

+ chat data

Vicuna

Yulan-Chat

+ synthetic data

Baize

BELLE Ziya
+ task data Koala

+ task data Guanaco

OpenFlamingo

LLaVA

MiniGPT-4

+ task data

+ task data

VisionLLM

InstructBLIP

Chatbridge

BenTsao

LAWGPT

ChatMed

LLaMA Adapter

Multimodal models

PandaGPT

Math

Finance

Medicine

Law

Bilingualism

Education

Fig. 4: An evolutionary graph of the research work conducted on LLaMA. Due to the huge number, we cannot include all the LLaMA variants in this figure, even much excellent work. To support incremental update, we share the source file of this figure, and welcome the readers to include the desired models by submitting the pull requests on our GitHub page.

parameters. As a popular LLM, LLaMA (65B version) [57], which contains approximately five times as many parameters as other models, has exhibited superior performance in tasks related to instruction following. Due to the openness and effectiveness, LLaMA has attracted significant attention from the research community, and many efforts [119‚Äì122] have been devoted to fine-tuning or continually pre-training its different model versions for implementing new models or tools. More recently, Falcon [117], as another opensource LLM, has also achieved very excellent performance on open benchmarks. It is featured by a more careful data cleaning process to prepare the pre-training data (with a publicly shared dataset RefinedWeb [123]). Typically, pretraining models at this scale require hundreds or even thousands of GPUs or TPUs. For instance, GPT-NeoX-20B uses 12 supermicro servers, each equipped with 8 NVIDIA A100-SXM4-40GB GPUs, while LLaMA utilizes 2,048 A10080G GPUs as reported in their original publications. To accurately estimate the computation resources needed, it is suggested to use the metrics measuring the number of involved computations such as FLOPS (i.e., FLoating point number Operations Per Second) [30].
Models with Hundreds of Billions of Parameters. For models in this category, only a handful of models have been publicly released. For example, OPT [81], OPT-IML [85], BLOOM [69], and BLOOMZ [84] have nearly the same number of parameters as GPT-3 (175B version), while GLM [83]

and Galactica [35] have 130B and 120B parameters, respectively. Among them, OPT (175B version), with the instruction-tuned version OPT-IML, has been specially motivated for open sharing, which aims to enable researchers to carry out reproducible research at scale. For research in cross-lingual generalization, BLOOM (176B version) and BLOOMZ (176B version) can be used as base models, due to the competence in multilingual language modeling tasks. As a bilingual LLM, GLM has also provided a popular small-sized Chinese chat model ChatGLM2-6B (a updated version for ChatGLM-6B), which is featured with many improvements in efficiency and capacity (e.g., quantization, 32K-length context, fast inference rate). Models of this scale typically require thousands of GPUs or TPUs to train. For instance, OPT (175B version) used 992 A100-80GB GPUs, while GLM (130B version) used a cluster of 96 NVIDIA DGX-A100 (8x40G) GPU nodes.
LLaMA Model Family. The collection of LLaMA models [57] were introduced by Meta AI in February, 2023, consisting of four sizes (7B, 13B, 30B and 65B). Since released, LLaMA has attracted extensive attention from both research and industry communities. LLaMA models have achieved very excellent performance on various open benchmarks, which have become the most popular open language models thus far. A large number of researchers have extended LLaMA models by either instruction tuning or continual pretraining. In particular, in-

struction tuning LLaMA has become a major approach to developing customized or specialized models, due to the relatively low computational costs. To effectively adapt LLaMA models in non-English languages, it often needs to extend the original vocabulary (trained mainly on English corpus) or fine-tune it with instructions or data in the target language. Among these extended models, Stanford Alpaca [124] is the first open instruct-following model fine-tuned based on LLaMA (7B). It is trained by 52K instruction-following demonstrations generated via selfinstruct [125] using text-davinci-003. The instruction data, named Alpaca-52K, and training code have been extensively adopted in subsequent work, such as AlpacaLoRA [126] (a reproduction of Stanford Alpaca using LoRA [127]), Koala [128], and BELLE [129]. In addition, Vicuna [120] is another popular LLaMA variant, trained upon user-shared conversations collected from ShareGPT 16. Due to the excellent performance and availability of the LLaMA model family, many multimodal models incorporate them as the base language models, to achieve strong language understanding and generation abilities. Compared with other variants, Vicuna is more preferred in multimodal language models, which have led to the emergence of a variety of popular models, including LLaVA [130], MiniGPT4 [131], InstructBLIP [132], and PandaGPT [133]. The release of LLaMA has greatly advanced the research progress of LLMs. To summarize the research work conducted on LLaMA, we present a brief evolutionary graph in Figure 4.
Public API of LLMs. Instead of directly using the model copies, APIs provide a more convenient way for common users to use LLMs, without the need of running the model locally. As a representative interface for using LLMs, the APIs for the GPT-series models [46, 55, 61, 89] have been widely used for both academia and industry17. OpenAI has provided seven major interfaces to the models in GPT-3 series: ada, babbage, curie, davinci (the most powerful version in GPT-3 series), text-ada-001, text-babbage-001, and text-curie-001. Among them, the first four interfaces can be further fine-tuned on the host server of OpenAI. In particular, babbage, curie, and davinci correspond to the GPT-3 (1B), GPT-3 (6.7B), and GPT-3 (175B) models, respectively [55]. In addition, there are also two APIs related to Codex [89], called code-cushman-001 (a powerful and multilingual version of the Codex (12B) [89]) and code-davinci-002. Further, GPT-3.5 series include one base model code-davinci-002 and three enhanced versions, namely text-davinci-002, text-davinci-003, and gpt-3.5-turbo-0301. It is worth noting that gpt-3.5-turbo-0301 is the interface to invoke ChatGPT. More recently, OpenAI has also released the corresponding APIs for GPT-4, including gpt-4, gpt-4-0314, gpt-4-32k, and gpt-4-32k-0314. Overall, the choice of API interfaces depends on the specific application scenarios and response requirements. The detailed usage can be found on their project websites18.
16. https://sharegpt.com/ 17. https://platform.openai.com/docs/api-reference/introduction 18. https://platform.openai.com/docs/models/overview

11
TABLE 2: Statistics of commonly-used data sources.

Corpora

Size

Source Latest Update Time

BookCorpus [134] 5GB

Books

Gutenberg [135] -

Books

C4 [73]

800GB CommonCrawl

CC-Stories-R [136] 31GB CommonCrawl

CC-NEWS [27]

78GB CommonCrawl

REALNEWs [137] 120GB CommonCrawl

OpenWebText [138] 38GB Reddit links

Pushift.io [139]

2TB

Reddit links

Wikipedia [140] 21GB Wikipedia

BigQuery [141]

-

Codes

the Pile [142]

800GB

Other

ROOTS [143]

1.6TB

Other

Dec-2015 Dec-2021 Apr-2019 Sep-2019 Feb-2019 Apr-2019 Mar-2023 Mar-2023 Mar-2023 Mar-2023 Dec-2020 Jun-2022

3.2 Commonly Used Corpora
In contrast to earlier PLMs, LLMs which consist of a significantly larger number of parameters require a higher volume of training data that covers a broad range of content. For this need, there are increasingly more accessible training datasets that have been released for research. In this section, we will briefly summarize several widely used corpora for training LLMs. Based on their content types, we categorize these corpora into six groups: Books, CommonCrawl, Reddit links, Wikipedia, Code, and others.
Books. BookCorpus [134] is a commonly used dataset in previous small-scale models (e.g., GPT [105] and GPT-2 [26]), consisting of over 11,000 books covering a wide range of topics and genres (e.g., novels and biographies). Another large-scale book corpus is Project Gutenberg [135], consisting of over 70,000 literary books including novels, essays, poetry, drama, history, science, philosophy, and other types of works in the public domain. It is currently one of the largest open-source book collections, which is used in training of MT-NLG [97] and LLaMA [57]. As for Books1 [55] and Books2 [55] used in GPT-3 [55], they are much larger than BookCorpus but have not been publicly released so far.
CommonCrawl. CommonCrawl [144] is one of the largest open-source web crawling databases, containing a petabytescale data volume, which has been widely used as training data for existing LLMs. As the whole dataset is very large, existing studies mainly extract subsets of web pages from it within a specific period. However, due to the widespread existence of noisy and low-quality information in web data, it is necessary to perform data preprocessing before usage. Based on CommonCrawl, there are four filtered datasets that are commonly used in existing work: C4 [73], CCStories [136], CC-News [27], and RealNews [137]. The Colossal Clean Crawled Corpus (C4) includes five variants19, namely en (806G), en.noclean (6T), realnewslike (36G), webtextlike (17G), and multilingual (38T). The en version has been utilized for pre-training T5 [73], LaMDA [63], Gopher [59], and UL2 [80]. The multilingual C4, also called mC4, has been used in mT5 [74]. CC-Stories (31G) is composed of a subset of CommonCrawl data, in which the contents are made in a story-like way. Because the original source of CC-Stories is not available now, we include a reproduction version, CC-Stories-R [145], in Table 2. Moreover,
19. https://www.tensorflow.org/datasets/catalog/c4

12

two news corpora extracted from CommonCrawl, i.e., REALNEWS (120G) and CC-News (76G), are also commonly used as the pre-training data.
Reddit Links. Reddit is a social media platform that enables users to submit links and text posts, which can be voted on by others through ‚Äúupvotes‚Äù or ‚Äúdownvotes‚Äù. Highly upvoted posts are often considered useful, and can be utilized to create high-quality datasets. WebText [26] is a well-known corpus composed of highly upvoted links from Reddit, but it is not publicly available. As a surrogate, there is a readily accessible open-source alternative called OpenWebText [138]. Another corpus extracted from Reddit is PushShift.io [139], a real-time updated dataset that consists of historical data from Reddit since its creation day. Pushshift provides not only monthly data dumps but also useful utility tools to support users in searching, summarizing, and conducting preliminary investigations on the entire dataset. This makes it easy for users to collect and process Reddit data.
Wikipedia. Wikipedia [140] is an online encyclopedia containing a large volume of high-quality articles on diverse topics. Most of these articles are composed in an expository style of writing (with supporting references), covering a wide range of languages and fields. Typically, the Englishonly filtered versions of Wikipedia are widely used in most LLMs (e.g., GPT-3 [55], LaMDA [63], and LLaMA [57]). Wikipedia is available in multiple languages, so it can be used in multilingual settings.
Code. To collect code data, existing work mainly crawls open-source licensed codes from the Internet. Two major sources are public code repositories under open-source licenses (e.g., GitHub) and code-related question-answering platforms (e.g., StackOverflow). Google has publicly released the BigQuery dataset [141], which includes a substantial number of open-source licensed code snippets in various programming languages, serving as a representative code dataset. CodeGen has utilized BIGQUERY [77], a subset of the BigQuery dataset, for training the multilingual version of CodeGen (CodeGen-Multi).
Others. The Pile [142] is a large-scale, diverse, and opensource text dataset consisting of over 800GB of data from multiple sources, including books, websites, codes, scientific papers, and social media platforms. It is constructed from 22 diverse high-quality subsets. The Pile dataset is widely used in models with different parameter scales, such as GPT-J (6B) [146], CodeGen (16B) [77], and Megatron-Turing NLG (530B) [97]. ROOTS [143] is composed of various smaller datasets (totally 1.61 TB of text) and covers 59 different languages (containing natural languages and programming languages), which have been used for training BLOOM [69].
In practice, it commonly requires a mixture of different data sources for pre-training LLMs (see Figure 5), instead of a single corpus. Therefore, existing studies commonly mix several ready-made datasets (e.g., C4, OpenWebText, and the Pile), and then perform further processing to obtain the pre-training corpus. Furthermore, to train the LLMs that are adaptive to specific applications, it is also important to extract data from relevant sources (e.g., Wikipedia and

BigQuery) for enriching the corresponding information in pre-training data. To have a quick reference of the data sources used in existing LLMs, we present the pre-training corpora of three representative LLMs:
‚Ä¢ GPT-3 (175B) [55] was trained on a mixed dataset of 300B tokens, including CommonCrawl [144], WebText2 [55], Books1 [55], Books2 [55], and Wikipedia [140].
‚Ä¢ PaLM (540B) [56] uses a pre-training dataset of 780B tokens, which is sourced from social media conversations, filtered webpages, books, Github, multilingual Wikipedia, and news.
‚Ä¢ LLaMA [57] extracts training data from various sources, including CommonCrawl, C4 [73], Github, Wikipedia, books, ArXiv, and StackExchange. The training data size for LLaMA (6B) and LLaMA (13B) is 1.0T tokens, while 1.4T tokens are used for LLaMA (32B) and LLaMA (65B).
3.3 Library Resource
In this part, we briefly introduce a series of available libraries for developing LLMs.
‚Ä¢ Transformers [147] is an open-source Python library for building models using the Transformer architecture, which is developed and maintained by Hugging Face. It has a simple and user-friendly API, making it easy to use and customize various pre-trained models. It is a powerful library with a large and active community of users and developers who regularly update and improve the models and algorithms.
‚Ä¢ DeepSpeed [65] is a deep learning optimization library (compatible with PyTorch) developed by Microsoft, which has been used to train a number of LLMs, such as MTNLG [97] and BLOOM [69]. It provides the support of various optimization techniques for distributed training, such as memory optimization (ZeRO technique, gradient checkpointing), and pipeline parallelism.
‚Ä¢ Megatron-LM [66‚Äì68] is a deep learning library developed by NVIDIA for training large-scale language models. It also provides rich optimization techniques for distributed training, including model and data parallelism, mixedprecision training, and FlashAttention. These optimization techniques can largely improve the training efficiency and speed, enabling efficient distributed training across GPUs.
‚Ä¢ JAX [148] is a Python library for high-performance machine learning algorithms developed by Google, allowing users to easily perform computations on arrays with hardware acceleration (e.g., GPU or TPU). It enables efficient computation on various devices and also supports several featured functions, such as automatic differentiation and just-in-time compilation.
‚Ä¢ Colossal-AI [149] is a deep learning library developed by HPC-AI Tech for training large-scale AI models. It is implemented based on PyTorch and supports a rich collection of parallel training strategies. Furthermore, it can also optimize heterogeneous memory management with methods proposed by PatrickStar [150]. Recently, a ChatGPT-like model called ColossalChat [122] has been publicly released with two versions (7B and 13B), which are developed using Colossal-AI based on LLaMA [57].
‚Ä¢ BMTrain [151] is an efficient library developed by OpenBMB for training models with large-scale parameters

13

in a distributed manner, which emphasizes code simplicity, low resource, and high availability. BMTrain has already incorporated several common LLMs (e.g., Flan-T5 [64] and GLM [83]) into its ModelCenter, where developers can use these models directly.
‚Ä¢ FastMoE [152] is a specialized training library for MoE (i.e., mixture-of-experts) models. It is developed based on PyTorch, prioritizing both efficiency and user-friendliness in its design. FastMoE simplifies the process of transferring Transformer models to MoE models and supports both data parallelism and model parallelism during training.
In addition to the above library resources, existing deep learning frameworks (e.g., PyTorch [153], TensorFlow [154], MXNet [155], PaddlePaddle [156], MindSpore [118] and OneFlow [157]) have also provided the support for parallel algorithms, which are commonly used for training largescale models.
4 PRE-TRAINING
Pre-training establishes the basis of the abilities of LLMs. By pre-training on large-scale corpora, LLMs can acquire essential language understanding and generation skills [55, 56]. In this process, the scale and quality of the pre-training corpus are critical for LLMs to attain powerful capabilities. Furthermore, to effectively pre-train LLMs, model architectures, acceleration methods, and optimization techniques need to be well designed. In what follows, we first discuss the data collection and processing in Section 4.1, then introduce the commonly used model architectures in Section 4.2, and finally present the training techniques to stably and efficiently optimize LLMs in Section 4.3.
4.1 Data Collection
Compared with small-scale language models, LLMs have a stronger demand for high-quality data for model pretraining, and their model capacities largely rely on the pretraining corpus and how it has been preprocessed. In this part, we discuss the collection and processing of pre-training data, including data sources, preprocessing methods, and important analysis of how pre-training data affects the performance of LLMs.
4.1.1 Data Source
To develop a capable LLM, it is key to collect a large amount of natural language corpus from various data sources. Existing LLMs mainly leverage a mixture of diverse public textual datasets as the pre-training corpus. Figure 5 shows the distribution of the sources of pre-training data for a number of representative LLMs.
The source of pre-training corpus can be broadly categorized into two types: general data and specialized data. General data, such as webpages, books, and conversational text, is utilized by most LLMs [55, 56, 81] due to its large, diverse, and accessible nature, which can enhance the language modeling and generalization abilities of LLMs. In light of the impressive generalization capabilities exhibited by LLMs, there are also studies that extend their pre-training corpus to more specialized datasets, such as multilingual data, scientific data, and code, endowing LLMs with specific

task-solving capabilities [35, 56, 77]. In what follows, we describe these two types of pre-training data sources and their effects on LLMs. For a detailed introduction to the commonly used corpus, one can refer to Section 3.2.
General Text Data. As we can see in Figure 5, the vast majority of LLMs adopt general-purpose pre-training data, such as webpages, books, and conversational text, which provides rich text sources on a variety of topics. Next, we briefly summarize three important kinds of general data.
‚Ä¢ Webpages. Owing to the proliferation of the Internet, various types of data have been created, which enables LLMs to gain diverse linguistic knowledge and enhance their generalization capabilities [26, 73]. For convenient use of these data resources, a large amount of data is crawled from the web in previous work, such as CommonCrawl [144]. However, the crawled web data tends to contain both high-quality text, such as Wikipedia and lowquality text, like spam mail, thus it is important to filter and process webpages for improving the data quality.
‚Ä¢ Conversation text. Conversation data can enhance the conversational competence of LLMs [81] and potentially improve their performance on a range of question-answering tasks [56]. Researchers can utilize subsets of public conversation corpus (e.g., PushShift.io Reddit corpus) [139, 158] or collect conversation data from online social media. Since online conversational data often involves discussions among multiple participants, an effective processing way is to transform a conversation into a tree structure, where the utterance is linked to the one it responds to. In this way, the multi-party conversation tree can be divided into multiple sub-conversations, which can be collected in the pre-training corpus. Furthermore, a potential risk is that the excessive integration of dialogue data into LLMs may result in a side effect [81]: declarative instructions and direct interrogatives are erroneously perceived as the beginning of conversations, thus leading to a decline in the efficacy of the instructions.
‚Ä¢ Books. Compared to other corpus, books provide an important source of formal long texts, which are potentially beneficial for LLMs to learn linguistic knowledge, model long-term dependency, and generate narrative and coherent texts. To obtain open-source book data, existing studies usually adopt the Books3 and Bookcorpus2 datasets, which are available in the Pile dataset [142].
Specialized Text Data. Specialized datasets are useful to improve the specific capabilities of LLMs on downstream tasks. Next, we introduce three kinds of specialized data.
‚Ä¢ Multilingual text. In addition to the text in the target language, integrating a multilingual corpus can enhance the multilingual abilities of language understanding and generation. For example, BLOOM [69] and PaLM [56] have curated multilingual data covering 46 and 122 languages, respectively, within their pre-training corpora. These models demonstrate impressive performance in multilingual tasks, such as translation, multilingual summarization, and multilingual question answering, and achieve comparable or superior performance to the state-of-the-art models that are fine-tuned on the corpus in the target language(s).
‚Ä¢ Scientific text. The exploration of science by humans has been witnessed by the increasing growth of scientific publications. In order to enhance the understanding of scientific

T5 (11B) 100%

Falcon (40B) 100%

LLaMA (65B) 3%
2% 5% 5%
87%

GPT-3 (175B) 16%
84%

MT-NLG (530B)

2%

26% 4%

6%

62%

Gopher (280B)
3% 37%
60%

14 Chinchilla (70B)
4% 40%
56%

GLaM (1200B)
22% 48%
30%

PaLM (540B) 5%
14% 31%
50%

LaMDA (137B) 13% 38%
50%

Galactica (120B) 8%
7%
86%

GPT-NeoX (20B)
8% 30%
38% 10%
15%

CodeGen (16B) AlphaCode (41B)

20%

39%

6%

10%

25%

100%

Webpages

Conversation Data

Books & News

Scientific Data

Code

Fig. 5: Ratios of various data sources in the pre-training data for existing LLMs.

knowledge for LLMs [35, 159], it is useful to incorporate a scientific corpus for model pre-training [35, 159]. By pretraining on a vast amount of scientific text, LLMs can achieve impressive performance in scientific and reasoning tasks [160]. To construct the scientific corpus, existing efforts mainly collect arXiv papers, scientific textbooks, math webpages, and other related scientific resources. Due to the complex nature of data in scientific fields, such as mathematical symbols and protein sequences, specific tokenization and preprocessing techniques are usually required to transform these different formats of data into a unified form that can be processed by language models.
‚Ä¢ Code. Program synthesis has been widely studied in the research community [89, 161‚Äì164], especially the use of PLMs trained on code [146, 165]. However, it remains challenging for these PLMs (e.g., GPT-J [146]) to generate highquality and accurate programs. Recent studies [89, 164] have found that training LLMs on a vast code corpus can lead to a substantial improvement in the quality of the synthesized programs. The generated programs can successfully pass expert-designed unit-test cases [89] or solve competitive programming questions [98]. In general, two types of code corpora are commonly used for pre-training LLMs. The first source is from programming question answering communities like Stack Exchange [166]. The second source is from public software repositories such as GitHub [77, 89, 164], where code data (including comments and docstrings) are collected for utilization. Compared to natural language text, code is in the format of a programming language, corresponding to long-range dependencies and accurate execution logic [167]. A recent study [47] also speculates that training on code might be a source of complex reasoning abilities (e.g., chain-of-thought ability [33]). Furthermore, it has been shown that formatting reasoning tasks into code can help LLMs generate more accurate results [167].
4.1.2 Data Preprocessing
After collecting a large amount of text data, it is essential to preprocess the data for constructing the pre-training corpus, especially removing noisy, redundant, irrelevant, and potentially toxic data [56, 59, 168], which may largely affect the

capacity and performance of LLMs. In this part, we review the detailed data preprocessing strategies to improve the quality of the collected data [59, 69, 96]. A typical pipeline of preprocessing the pre-training data for LLMs has been illustrated in Figure 6.
Quality Filtering. To remove low-quality data from the collected corpus, existing work generally adopts two approaches: (1) classifier-based, and (2) heuristic-based. The former approach trains a selection classifier based on highquality texts and leverages it to identify and filter out lowquality data. Typically, these methods [55, 56, 96] train a binary classifier with well-curated data (e.g., Wikipedia pages) as positive instances and sample candidate data as negative instances, and predict the score that measures the quality of each data example. However, several studies [59, 96] find that a classifier-based approach may result in the unintentional removal of high-quality texts in dialectal, colloquial, and sociolectal languages, which potentially leads to bias in the pre-training corpus and diminishes the corpus diversity. As the second approach, several studies, such as BLOOM [69] and Gopher [59], employ heuristicbased approaches to eliminate low-quality texts through a set of well-designed rules, which can be summarized as follows:
‚Ä¢ Language based filtering. If a LLM would be mainly used in the tasks of certain languages, the text in other languages can be filtered.
‚Ä¢ Metric based filtering. Evaluation metrics about the generated texts, e.g., perplexity, can be employed to detect and remove unnatural sentences.
‚Ä¢ Statistic based filtering. Statistical features of a corpus, e.g., the punctuation distribution, symbol-to-word ratio, and sentence length, can be utilized to measure the text quality and filter the low-quality data.
‚Ä¢ Keyword based filtering. Based on specific keyword set, the noisy or unuseful elements in the text, such as HTML tags, hyperlinks, boilerplates, and offensive words, can be identified and removed.

15

Raw Corpus

Quality Filtering
Language Filtering Metric Filtering Statistic Filtering Keyword Filtering
Alice is writing a paper about LLMs. #$^& Alice is writing a paper about LLMs.

De-duplication
Sentence-level Document-level Set-level
Alice is writing a paper about LLMs. Alice is writing a paper about LLMs.

Privacy Reduction
Detect Personality Identifiable Information (PII) Remove PII
Replace('Alice') is writing a paper about LLMs.

Tokenization
Reuse Existing Tokenizer SentencePiece Byte-level BPE
Encode('[Somebody] is writing a paper about LLMs.')

Ready to pre-train!
32, 145, 66, 79, 12, 56, ...

Fig. 6: An illustration of a typical data preprocessing pipeline for pre-training large language models.

De-duplication. Existing work [169] has found that duplicate data in a corpus would reduce the diversity of language models, which may cause the training process to become unstable and thus affect the model performance. Therefore, it is necessary to de-duplicate the pre-training corpus. Specially, de-duplication can be performed at different granularities, including sentence-level, document-level, and dataset-level de-duplication. First, low-quality sentences that contain repeated words and phrases should be removed, as they may introduce repetitive patterns in language modeling [170]. At the document level, existing studies mostly rely on the overlap ratio of surface features (e.g., words and n-grams overlap) between documents to detect and remove duplicate documents containing similar contents [57, 59, 69, 171]. Furthermore, to avoid the dataset contamination problem, it is also crucial to prevent the overlap between the training and evaluation sets [56], by removing the possible duplicate texts from the training set. It has been shown that the three levels of de-duplication are useful to improve the training of LLMs [56, 172], which should be jointly used in practice.
Privacy Redaction. The majority of pre-training text data is obtained from web sources, including user-generated content involving sensitive or personal information, which may increase the risk of privacy breaches [173]. Thus, it is necessary to remove the personally identifiable information (PII) from the pre-training corpus. One direct and effective approach is to employ rule-based methods, such as keyword spotting, to detect and remove PII such as names, addresses, and phone numbers [143]. Furthermore, researchers also find that the vulnerability of LLMs under privacy attacks can be attributed to the presence of duplicate PII data in the pre-training corpus [174]. Therefore, de-duplication can also reduce privacy risks to some extent.
Tokenization. Tokenization is also a crucial step for data preprocessing. It aims to segment raw text into sequences of individual tokens, which are subsequently used as the inputs of LLMs. In traditional NLP research (e.g., sequence labeling with conditional random fields [175]), word-based tokenization is the predominant approach, which is more aligned with human‚Äôs language cognition. However, wordbased tokenization can yield different segmentation results for the same input in some languages (e.g., Chinese word segmentation), generate a huge word vocabulary containing many low-frequency words, and also suffer from the ‚Äúoutof-vocabulary‚Äù issue. Thus, several neural network models employ character as the minimum unit to derive the word representation (e.g., a CNN word encoder in ELMo [21]).

Recently, subword tokenizers have been widely used in Transformer based language models, typically including BytePair Encoding tokenization, WordPiece tokenization and Unigram tokenization. HuggingFace has maintained an excellent online NLP course on tokenizer20 with running examples, and we refer to the beginners to this course. Next, we briefly describe the three representative tokenization methods.
‚Ä¢ Byte-Pair Encoding (BPE) tokenization. BPE was originally proposed as a general data compression algorithm in 1994 [176], and then adapted to NLP for tokenization [177]. It starts with a set of basic symbols (e.g., the alphabets and boundary characters), and iteratively combine frequent pairs of two consecutive tokens in the corpus as new tokens (called merge). For each merge, the selection criterion is based on the co-occurrence frequency of two contiguous tokens: the top frequent pair would be selected. The merge process continues until it reaches the predefined size. Further, Byte-level BPE has been used to improve the tokenization quality for multilingual corpus (e.g., the text containing non-ASCII characters) by considering bytes as the basic symbols for merge. Representative language models with this tokenization approach include GPT-2, BART, and LLaMA.
‚Ä¢ WordPiece tokenization. WordPiece was a Google internal subword tokenization algorithm. It was originally proposed by Google in developing voice search systems [178]. Then, it was used in the neural machine translation system in 2016 [179], and was adopted as the word tokenizer for BERT in 2018 [23]. WordPiece has a very similar idea with BPE by iteratively merging consecutive tokens, whereas taking a slightly different selection criterion for the merge. To conduct the merge, it first trains a language model and employs it to score all possible pairs. Then, at each merge, it selects the pair that leads to the most increase in the likelihood of training data. Since Google has‚Äôt released the official implementation of the WordPiece algorithm, HuggingFace gives a more intuitive selection measure in its online NLP course: a pair is scored by dividing the co-occurrence count by the product of the occurrence counts of two tokens in the pair based on training corpus.
‚Ä¢ Unigram tokenization. Unlike BPE and WordPiece, Unigram tokenization [180] starts with a sufficiently large set of possible substrings or subtokens for a corpus, and iteratively removes the tokens in the current vocabulary until the expected vocabulary size is reached. As the se-
20. https://huggingface.co/learn/nlp-course/chapter6

16

lection criterion, it calculates the yielded increase in the likelihood of training corpus by assuming that some token was removed from current vocabulary. This step is conducted based on a trained unigram language model. To estimate the unigram language model, it adopts an expectation‚Äìmaximization (EM) algorithm: at each iteration, we first find the currently optimal tokenization of words based on the old language model, and then re-estimate the probabilities of unigrams to update the language model. During this procedure, dynamic programming algorithms (i.e., the Viterbi algorithm) are used to efficiently find the optimal decomposition way of a word given the language model. Representative models that adopt this tokenization approach include T5 and mBART.
Although it is expedient to leverage an existing tokenizer (e.g., OPT [81] and GPT-3 [55] utilize the tokenizer of GPT2 [26]), using a tokenizer specially designed for the pretraining corpus can be highly beneficial [69], especially for the corpus that consists of diverse domains, languages, and formats. Therefore, recent LLMs often train the customized tokenizers specially for the pre-training corpus with the SentencePiece library [181], which includes Byte-level BPE and Unigram tokenization. A note is that normalization techniques in BPE, such as NFKC [182], may degrade the tokenization performance [34, 59, 69]. When extending existing LLMs (i.e., continual pre-training or instruction tuning), we should be also aware of the potential side effect with customized tokenizers. For example, LLaMA trains the BPE tokenizer based on a pre-training corpus mainly consisting of English texts, and the derived vocabulary might be less capable in processing non-English data, e.g., taking longer inference latency to generate Chinese texts.
4.1.3 Effect of Pre-training Data on LLMs
Unlike small-scale PLMs, it is usually infeasible to iterate the pre-training of LLMs multiple times, due to the huge demand for computational resources. Thus, it is particularly important to construct a well-prepared pre-training corpus before training a LLM. In this part, we discuss how the quality and distribution of the pre-training corpus potentially influence the performance of LLMs.
Mixture of Sources. As discussed before, pre-training data from different domains or scenarios has distinct linguistic characteristics or semantic knowledge. By pre-training on a mixture of text data from diverse sources, LLMs can acquire a broad scope of knowledge and may exhibit a strong generalization capacity. Thus, when mixing different sources, it is suggested to include as many high-quality data sources as possible, and carefully set the distribution of pre-training data, since it is also likely to affect the performance of LLMs on downstream tasks [59]. Gopher [59] conducts the ablation experiment on data distribution to examine the impact of mixed sources on downstream tasks. Experimental results on the LAMBADA dataset [183] show that increasing the proportion of books data can improve the capacity of the model in capturing long-term dependencies from text, and increasing the proportion of the C4 dataset [73] leads to performance improvement on the C4 validation dataset [59]. However, as a side effect, training on excessive data about a certain domain would affect the generalization capability of

LLMs on other domains [35, 59]. Therefore, it is suggested that researchers should carefully determine the proportion of data from different domains in the pre-training corpus, in order to develop LLMs that better meet their specific needs. The readers can refer to Figure 5 for a comparison of the data sources for different LLMs.
Amount of Pre-training Data. For pre-training an effective LLM, it is important to collect sufficient high-quality data that satisfies the data quantity demand of the LLM. Existing studies have found that with the increasing parameter scale in the LLM, more data is also required to train the model [34, 57]: a similar scaling law as model size is also observed in data size, with respect to model performance. A recent study has shown that a number of existing LLMs suffer from sub-optimal training due to inadequate pretraining data [34]. By conducting extensive experiments, it further demonstrates increasing the model size and data size in equal scales can lead to a more compute-efficient model (i.e., the Chinchilla model), for a given compute budget. More recently, LLaMA [57] shows that with more data and longer training, smaller models can also achieve good performance. Overall, it is suggested that researchers should pay more attention to the amount of high-quality data for adequately training the model, especially when scaling the model parameters.
Quality of Pre-training Data. Existing work has shown that pre-training on the low-quality corpus, such as noisy, toxic, and duplicate data, may hurt the performance of models [59, 169, 171, 174]. For developing a well-performing LLM, it is crucial to consider both the quantity and the quality of the collected training data. Recent studies, such as T5 [73], GLaM [96], and Gopher [59], have investigated the influence of data quality on the performance of downstream tasks. By comparing the performance of models trained on the filtered and unfiltered corpus, they reach the same conclusion that pre-training LLMs on cleaned data can improve the performance. More specifically, the duplication of data may result in ‚Äúdouble descent‚Äù (referring to the phenomenon of performance initially deteriorating and subsequently improving) [169, 184], or even overwhelm the training process [169]. In addition, it has been shown that duplicate data degrades the ability of LLMs to copy from the context, which might further affect the generalization capacity of LLMs using in-context learning [169]. Therefore, as suggested in [56, 59, 69], it is essential to incorporate preprocessing methods on the pre-training corpus carefully (as illustrated in Section 4.1.2), to improve stability of the training process and avoid affecting the model performance.
4.2 Architecture
In this section, we review the architecture design of LLMs, i.e., mainstream architecture, pre-training objective, and detailed configuration. Table 3 presents the model cards of several representative LLMs with public details.
4.2.1 Mainstream Architectures
Due to the excellent parallelizability and capacity, the Transformer architecture [22] has become the de facto backbone to develop various LLMs, making it possible to scale language

17
TABLE 3: Model cards of several selected LLMs with public configuration details. Here, PE denotes position embedding, #L denotes the number of layers, #H denotes the number of attention heads, dmodel denotes the size of hidden states, and MCL denotes the maximum context length during training.

Model

Category

Size Normalization

PE

Activation Bias #L #H dmodel MCL

GPT3 [55] PanGU- Œ± [75] OPT [81] PaLM [56] BLOOM [69] MT-NLG [97] Gopher [59] Chinchilla [34] Galactica [35] LaMDA [63] Jurassic-1 [91] LLaMA [57] GLM-130B [83] T5 [73]

Causal decoder Causal decoder Causal decoder Causal decoder Causal decoder Causal decoder Causal decoder Causal decoder Causal decoder Causal decoder Causal decoder Causal decoder Prefix decoder Encoder-decoder

175B 207B 175B 540B 176B 530B 280B
70B 120B 137B 178B 65B 130B 11B

Pre LayerNorm Pre LayerNorm Pre LayerNorm Pre LayerNorm Pre LayerNorm
Pre RMSNorm Pre RMSNorm Pre LayerNorm
Pre LayerNorm Pre RMSNorm Post DeepNorm Pre RMSNorm

Learned Learned Learned
RoPE ALiBi
Relative Relative Learned Relative Learned
RoPE RoPE Relative

GeLU GeLU ReLU SwiGLU GeLU
GeLU GeGLU GeLU SwiGLU GeGLU ReLU

‚úì 96 96 12288 2048

‚úì 64 128 16384 1024

‚úì 96 96 12288 2048

√ó 118 48 18432 2048

‚úì 70 112 14336 2048

- 105 128 20480 2048

-

80 128 16384 2048

-

80 64

8192

-

√ó 96 80 10240 2048

-

64 128

8192

-

‚úì 76 96 13824 2048

‚úì

80 64

8192 2048

‚úì 70 96 12288 2048

√ó

24 128

1024 512

Causal Decoder

Prefix Decoder

Encoder-Decoder

of Survey A

Decoder Encoder

of Survey A

of Survey A

Decoder

Decoder

Models Language Large

Models Language Large

Models Language Large

A Survey of Large Language Models

A Survey of Large Language Models

A Survey of Large Language Models

Decoder

Decoder

Encoder

Decoder

Fig. 7: A comparison of the attention patterns in three mainstream architectures. Here, the blue, green, yellow and grey rounded rectangles indicate the attention between prefix tokens, attention between prefix and target tokens, attention between target tokens, and masked attention respectively.

models to hundreds or thousands of billions of parameters. In general, the mainstream architectures of existing LLMs can be roughly categorized into three major types, namely encoder-decoder, causal decoder, and prefix decoder, as shown in Figure 7.
Encoder-decoder Architecture. The vanilla Transformer model is built on the encoder-decoder architecture [22], which consists of two stacks of Transformer blocks as the encoder and decoder, respectively. The encoder adopts stacked multi-head self-attention layers to encode the input sequence for generating its latent representations, while the decoder performs cross-attention on these representations and autoregressively generates the target sequence. Encoder-decoder PLMs (e.g., T5 [73] and BART [24]) have shown effectiveness on a variety of NLP tasks. So far, there are only a small number of LLMs that are built based on the encoder-decoder architecture, e.g., Flan-T5 [64]. We leave a detailed discussion about the architecture selection in Section 4.2.4.

tecture incorporates the unidirectional attention mask, to guarantee that each input token can only attend to the past tokens and itself. The input and output tokens are processed in the same fashion through the decoder. As representative language models of this architecture, the GPT-series models [26, 55, 105] are developed based on the causal-decoder architecture. In particular, GPT-3 [55] has successfully demonstrated the effectiveness of this architecture, also showing an amazing in-context learning capability of LLMs. Interestingly, GPT-1 [105] and GPT2 [26] do not exhibit such superior abilities as those in GPT-3, and it seems that scaling plays an important role in increasing the model capacity of this model architecture. So far, the causal decoders have been widely adopted as the architecture of LLMs by various existing LLMs, such as OPT [81], BLOOM [69], and Gopher [59]. Note that both the causal decoder and prefix decoder discussed next belong to decoder-only architectures. When mentioning ‚Äúdecoderonly architecture‚Äù, it mainly refers to the causal decoder architecture in existing literature, unless specified.

Causal Decoder Architecture. The causal decoder archi- Prefix Decoder Architecture. The prefix decoder architec-

18

ture (a.k.a., non-causal decoder [185]) revises the masking mechanism of causal decoders, to enable performing bidirectional attention over the prefix tokens [186] and unidirectional attention only on generated tokens. In this way, like the encoder-decoder architecture, the prefix decoders can bidirectionally encode the prefix sequence and autoregressively predict the output tokens one by one, where the same parameters are shared during encoding and decoding. Instead of pre-training from scratch, a practical suggestion is to continually train causal decoders and then convert them into prefix decoders for accelerating convergence [29], e.g., U-PaLM [102] is derived from PaLM [56]. Existing representative LLMs based on prefix decoders include GLM130B [83] and U-PaLM [102].
For the three types of architectures, we can also consider extending them via the mixture-of-experts (MoE) scaling, in which a subset of neural network weights for each input are sparsely activated, e.g., Switch Transformer [25] and GLaM [96]. It has been shown that substantial performance improvement can be observed by increasing either the number of experts or the total parameter size [187].
4.2.2 Detailed Configuration
Since the launch of Transformer [22], various improvements have been proposed to enhance its training stability, performance, and computational efficiency. In this part, we will discuss the corresponding configurations for four major parts of the Transformer, including normalization, position embeddings, activation functions, and attention and bias. To make this survey more self-contained, we present the detailed formulations for these configurations in Table 4.
Normalization Methods. Training instability is a challenging issue for pre-training LLMs. To alleviate this issue, normalization is a widely adopted strategy to stabilize the training of neural networks. In the vanilla Transformer [22], LayerNorm [189] is employed. Recently, several advanced normalization techniques have been proposed as alternatives to LayerNorm, e.g., RMSNorm, and DeepNorm.
‚Ä¢ LayerNorm. In the early research, BatchNorm [198] is a commonly used normalization method. However, it is difficult to deal with sequence data of variable lengths and small-batch data. Thus, LayerNorm [189] is introduced to conduct layerwise normalization. Specifically, the mean and variance over all activations per layer are calculated to recenter and re-scale the activations.
‚Ä¢ RMSNorm. To improve the training speed of LayerNorm (LN), RMSNorm [190] is proposed by re-scaling the activations with only the root mean square (RMS) of the summed activations, instead of the mean and variance. Related research has demonstrated its superiority in training speed and performance on Transformer [199]. Representative models that adopt RMSNorm include Gopher [59] and Chinchilla [34].
‚Ä¢ DeepNorm. DeepNorm is proposed by Microsoft [191] to stabilize the training of deep Transformers. With DeepNorm as residual connections, Transformers can be scaled up to 1,000 layers [191], which has shown the advantages of stability and good performance. It has been adopted by GLM-130B [83].

Normalization Position. In addition to the normalization method, normalization position also plays a crucial role in the LLMs. There are generally three choices for the normalization position, i.e., post-LN, pre-LN, and sandwich-LN.
‚Ä¢ Post-LN. Post-LN is used in the vanilla Transformer [22], which is placed between residual blocks. However, existing work has found that the training of Transformers with post-LN tends to be instable due to the large gradients near the output layer [200]. Thus, post-LN is rarely employed in existing LLMs except combined with other strategies (e.g., combining post-LN with pre-LN in GLM130B [83]).
‚Ä¢ Pre-LN. Different from post-LN, pre-LN [201] is applied before each sub-layer, and an additional LN is placed before the final prediction. Compared with post-LN, the Transformers with pre-LN are more stable in training. However, it performs worse than the variants with post-LN [202]. Despite the decreasing performance, most LLMs still adopt pre-LN due to the training stability. However, one exception is that pre-LN has been found unstable in GLM when training models more than 100B parameters [83].
‚Ä¢ Sandwich-LN. Based on pre-LN, Sandwich-LN [188] adds extra LN before the residual connections to avoid the value explosion issues in Transformer layer outputs. However, it has been found that Sandwich-LN sometimes fails to stabilize the training of LLMs and may lead to the collapse of training [83].
Activation Functions. To obtain good performance, activation functions also need to be properly set in feed-forward networks. In existing LLMs, GeLU activations [203] are widely used. Specially, in the latest LLMs (e.g., PaLM and LaMDA), variants of GLU activation [195, 204] have also been utilized, especially the SwiGLU and GeGLU variants, which often achieve better performance in practice [199]. However, compared with GeLU, they require extra parameters (about 50%) in the feed-forward networks [205].
Position Embeddings. Since the self-attention modules in Transformer are permutation equivariant, position embeddings (PE) are employed to inject absolute or relative position information for modeling sequences.
‚Ä¢ Absolute position embedding. In the vanilla Transformer [22], absolute position embeddings are employed. At the bottoms of the encoder and the decoder, the absolute positional embeddings are added to the input embeddings. There are two variants of absolute position embeddings proposed in the vanilla Transformer [22], i.e., sinusoidal and learned position embeddings, where the latter is commonly used in existing pre-trained language models.
‚Ä¢ Relative position embedding. Unlike absolute position embeddings, relative positional embeddings are generated according to the offsets between keys and queries [206]. A popular variant of relative PE was introduced in Transformer-XL [207, 208]. The calculation of attention scores between keys and queries has been modified to introduce learnable embeddings corresponding to relative positions. T5 [73] further simplified relative positional embeddings, which was subsequently adopted by Gopher [59]. Specifically, it adds learnable scalars to the attention scores, where the scalars are calculated based on the distances between the positions of the query and the key. Compared

19
TABLE 4: Detailed formulations for the network configurations. Here, Sublayer denotes a FFN or a self-attention module in a Transformer layer, d denotes the size of hidden states, pi denotes position embedding at position i, Aij denotes the attention score between a query and a key, ri‚àíj denotes a learnable scalar based on the offset between the query and the key, and RŒ∏,t denotes a rotary matrix with rotation degree t ¬∑ Œ∏.

Configuration Normalization position Normalization method
Activation function
Position embedding

Method
Post Norm [22] Pre Norm [26] Sandwich Norm [188]
LayerNorm [189]
RMSNorm [190] DeepNorm [191]
ReLU [192] GeLU [193] Swish [194] SwiGLU [195] GeGLU [195]
Absolute [22] Relative [73] RoPE [196] Alibi [197]

Equation
Norm(x+Sublayer(x)) x + Sublayer(Norm(x)) x + Norm(Sublayer(Norm(x)))

x‚àö‚àí¬µ œÉ

¬∑

Œ≥

+

Œ≤,

¬µ

=

1 d

d i=1

xi,

œÉ=

1 d

x RMS(x)

¬∑

Œ≥,

RMS(x) =

1 d

d i=1

x2i

LayerNorm(Œ± ¬∑ x + Sublayer(x))

d i=1

(xi

‚àí

¬µ))2

ReLU(x) = max(x, 0)

‚àö

GeLU(x) = 0.5x ‚äó [1 + erf(x/ 2)],

Swish(x) = x ‚äó sigmoid(x) SwiGLU(x1, x2) = Swish(x1) ‚äó x2 GeGLU(x1, x2) = GeLU(x1) ‚äó x2

erf (x)

=

‚àö2 œÄ

x 0

e‚àít2

dt

xi = xi + pi Aij = Wq xixTj WkT + ri‚àíj Aij = Wq xiRŒ∏,i‚àíj xTj WkT Aij = Wq xiRŒ∏,i‚àíj xTj WkT Aij = Wq xixTj WkT ‚àí m(i ‚àí j)

with the absolute PE, Transformers with relative position embedding can generalize to sequences longer than those sequences for training, i.e., extrapolation [197].
‚Ä¢ Rotary Position Embedding. Rotary position embedding (RoPE) [196] sets specific rotatory matrices based on the absolute position of each token. The scores between keys and queries can be computed with relative position information. Due to the excellent performance and the longterm decay property, RoPE is widely adopted in the latest LLMs, e.g., PaLM [56] and LLaMA [57]. Based on RoPE, xPos [209] further improves the translation invariance and length extrapolation of Transformer. At each dimension of the rotation degree vector, xPos adds a special exponential decay that is smaller when the rotation degree is larger. It can alleviate the unstable phenomenon during training as the distance increases.
‚Ä¢ ALiBi. ALiBi [197] is proposed to improve the extrapolation of Transformer. Similar to relative position embedding, it biases attention scores with a penalty based on the distances between keys and queries. Different from the relative positional embedding methods like T5 [73], the penalty scores in ALiBi are pre-defined without any trainable parameters. Empirical results in [197] have shown that ALiBi has a better extrapolation performance on sequences that are longer than those for training than several popular position embedding methods such as sinusoidal PE [22], RoPE [196], and T5 bias [73]. In addition, it has been shown that ALiBi can also improve training stability in BLOOM [69].
Attention and Bias. Attention mechanism is a critical component of Transformer. It allows the tokens across the sequence to interact with each other and compute the representations of the input and output sequence.
‚Ä¢ Full attention. In the vanilla Transformer [22], the attention mechanism is conducted in a pairwise way, considering the relations between all token pairs in a sequence. It adopts scaled dot-product attention, in which the hidden states are mapped into queries, keys, and values. Additionally,

Transformer uses multi-head attention instead of single attention, projecting the queries, keys, and values with different projections in different heads. The concatenation of the output of each head is taken as the final output.
‚Ä¢ Sparse attention. A crucial challenge of full attention is the quadratic computational complexity, which becomes a burden when dealing with long sequences. Therefore, various efficient Transformer variants are proposed to reduce the computational complexity of the attention mechanism [210, 211]. For instance, locally banded sparse attention (i.e., Factorized Attention [212] has been adopted in GPT3 [55]. Instead of the whole sequence, each query can only attend to a subset of tokens based on the positions.
‚Ä¢ Multi-query attention. Multi-query attention refers to the attention variant where different heads share the same linear transformation matrices on the keys and values [213]. It can significantly save computation costs with only a minor sacrifice in model quality. Representative models with multiquery attention include PaLM [56] and StarCoder [214].
‚Ä¢ FlashAttention. Different from most existing approximate attention methods that trade-off model quality to improve the computing efficiency, FlashAttention [215] proposes to optimize the speed and memory consumption of attention modules on GPUs from an IO-aware perspective. There exist different levels of memory on modern GPUs, e.g., SRAM with a fast IO and HBM with a relatively slow IO. FlashAttention organizes the input into blocks and introduces necessary recomputation, both to make better use of the fast memory SRAM. Implemented as a fused kernel in CUDA, FlashAttention has been integrated into PyTorch [153], DeepSpeed [65], and Megatron-LM [66].
To put all these discussions together, we summarize the suggestions from existing literature for detailed configuration. For stronger generalization and training stability, it is suggested to choose the pre RMSNorm for layer normalization, and SwiGLU or GeGLU as the activation function. In addition, LN may not be used immediately after embedding layers, which is likely to incur performance degradation. As

20

for position embeddings, RoPE or ALiBi is a better choice since it performs better on long sequences.

4.2.3 Pre-training Tasks
Pre-training plays a key role that encodes general knowledge from large-scale corpus into the massive model parameters. For training LLMs, there are two commonly used pretraining tasks, namely language modeling and denoising autoencoding.

Language Modeling. The language modeling task (LM) is the most commonly used objective to pre-train decoder-only LLMs, e.g., GPT3 [55] and PaLM [56]. Given a sequence of tokens x = {x1, . . . , xn}, the LM task aims to autoregressively predict the target tokens xi based on the preceding tokens x<i in a sequence. A general training objective is to maximize the following likelihood:

n

LLM (x) = log P (xi|x<i).

(4)

i=1

Since most language tasks can be cast as the prediction problem based on the input, these decoder-only LLMs might be potentially advantageous to implicitly learn how to accomplish these tasks in a unified LM way. Some studies have also revealed that decoder-only LLMs can be naturally transferred to certain tasks by autoregressively predicting the next tokens [26, 55], without fine-tuning. An important variant of LM is the prefix language modeling task, which is designed for pre-training models with the prefix decoder architecture. The tokens within a randomly selected prefix would not be used in computing the loss of prefix language modeling. With the same amount of tokens seen during pretraining, prefix language modeling performs slightly worse than language modeling, since fewer tokens in the sequence are involved for model pre-training [29].

Denoising Autoencoding. In addition to conventional LM, the denoising autoencoding task (DAE) has also been widely used to pre-train language models [24, 73]. The inputs x\xÀú for DAE task are corrupted text with randomly replaced spans. Then, the language models are trained to recover the replaced tokens xÀú. Formally, the training objective of DAE is denoted as follows:

LDAE(x) = log P (xÀú|x\xÀú).

(5)

However, the DAE task seems to be more complicated in implementation than LM task. As a result, it has not been widely used to pre-train large language models. Existing LLMs that take DAE as pre-training objectives include T5 [73] and GLM-130B [83]. These models are mainly trained to recover the replaced spans in an autoregressive way.

Mixture-of-Denoisers. Mixture-of-Denoisers (MoD) [80], also known as UL2 loss, was introduced as a unified objective for pre-training language models. MoD regards both LM and DAE objectives as different types of denoising tasks, namely S-denoiser (LM), R-denoiser (DAE, short span and low corruption), and X-denoiser (DAE, long span or high corruption). Among the three denoising tasks, S-denoiser is similar to the conventional LM objective (Equation (4)), while R-denoiser and X-denoiser are similar to DAE objectives (Equation (5)) but differ from each other in the

lengths of spans and ratio of corrupted text. For input sentences started with different special tokens (i.e., {[R], [S], [X]}), the model will be optimized using the corresponding denoisers. MoD has been applied in the latest PaLM 2 model [216].
4.2.4 Summary and Discussion
The choice of architecture and pre-training tasks may incur different inductive biases for LLMs, which would lead to different model capacities. In this part, we discuss on several two open issues about LLM architecture.
Architecture Choice. In earlier literature of pre-trained language models, there are lots of discussions on the effects of different architectures [29, 80]. However, most LLMs are developed based on the causal decoder architecture, and there still lacks a theoretical analysis on its advantage over the other alternatives. Next, we briefly summarize existing discussions on this issue.
‚Ä¢ By pre-training with the LM objective, it seems that causal decoder architecture can achieve a superior zeroshot and few-shot generalization capacity. Existing research has shown that without multi-task fine-tuning, the causal decoder has better zero-shot performance than other architectures [29]. The success of GPT-3 [55] has demonstrates that the large causal decoder model can be a good fewshot learner. In addition, instruction tuning and alignment tuning discussed in Section 5 have been proven to further enhance the capability of large causal decoder models [61, 62, 64].
‚Ä¢ Scaling law has been widely observed in causal decoders. By scaling the model size, the dataset size, and the total computation, the performance of causal decoders can be substantially improved [30, 55]. Thus, it has become an important strategy to increase the model capacity of the causal decoder via scaling. However, more detailed investigation on encoder-decoder models is still lacking, and more efforts are needed to investigate the performance of encoder-decoder models at a large scale.
More research efforts about the discussions on architectures and pre-training objectives are in need to analyze how the choices of the architecture and pre-training tasks affect the capacity of LLMs, especially for encoder-decoder architectures. Besides the major architecture, the detailed configuration of LLM is also worth attention, which has been discussed in Section 4.2.2.
Long Context. One of the main drawbacks of Transformerbased language models is the context length is limited due to the involved quadratic computational costs in both time and memory. Meanwhile, there is an increasing demand for LLM applications with long context windows, such as in PDF processing and story writing [217]. ChatGPT has recently released an updated variant with a context window size of up to 16K tokens, which is much longer than the initial one, i.e., 4K tokens. Additionally, GPT-4 was launched with variants with context window of 32K tokens [46]. Next, we discuss two important factors that support long context modeling for LLMs.
‚Ä¢ Extrapolation. In real-world applications, it is possible that LLMs need to process long input text that exceeds the maximum length of the training corpus. The ability of LLMs

21

to encode longer texts is often referred to as extrapolation capability [197]. Several position embedding methods, such as RoPE [196] and T5 bias [73], have been empirically validated to possess certain extrapolation capabilities [197]. Specifically, language models equipped with ALiBi [197] have been shown to maintain relatively stable perplexity on sequences even ten times longer than those for training. There are also efforts like xPos [209] to enhance the extrapolation ability of RoPE by improving the design of rotation matrix.
‚Ä¢ Efficiency. In order to reduce the quadratic computational cost in attention modules, several studies design highly efficient attention computation methods that can make the memory consumption scales approximately linearly, exemplified by sparse or linear attentions [212, 218‚Äì 221]. In addition to the algorithmic improvements, another important work, FlashAttention [215], improves the efficiency from a system-level perspective (i.e., GPU memory IO efficiency). With the same computing budget, one can train LLMs with longer context windows. Furthermore, some studies also aim to devise new architectures rather than Transformers for language modeling, including parameterized state space models (e.g., S4 [222], GSS [223], and H3 [224]) and stacked linear attention modules that incorporate recurrency mechanisms like RWKV [225].
Why does Predicting the Next Word Works?
The essence of decoder-only architecture is to accurately predict the next word for reconstructing the pre-training data. Till now, there has been no formal study that theoretically demonstrates its advantage over other architectures. An interesting explanation was from Ilya Sutskever during the interview held by Jensen Huanga. The original transcript from the interview was copied belowb:
Say you read a detective novel. It‚Äôs like complicated plot, a storyline, different characters, lots of events, mysteries like clues, it‚Äôs unclear. Then, let‚Äôs say that at the last page of the book, the detective has gathered all the clues, gathered all the people and saying, "okay, I‚Äôm going to reveal the identity of whoever committed the crime and that person‚Äôs name is". Predict that word. ... Now, there are many different words. But predicting those words better and better, the understanding of the text keeps on increasing. GPT-4 predicts the next word better.
a. https://www.nvidia.com/en-us/ondemand/session/gtcspring23-S52092/
b. https://lifearchitect.ai/ilya/

4.3 Model Training
In this part, we review the important settings, techniques, or tricks for training LLMs.
4.3.1 Optimization Setting
For parameter optimization of LLMs, we present the commonly used settings for batch training, learning rate, optimizer, and training stability.
Batch Training. For language model pre-training, existing work generally sets the batch size to a large number (e.g., 2,048 examples or 4M tokens) to improve the training stability and throughput. For LLMs such as GPT-3 and PaLM, they have introduced a new strategy that dynamically increases the batch size during training, ultimately reaching a million scale. Specifically, the batch size of GPT-3 is gradually increasing from 32K to 3.2M tokens. Empirical results have demonstrated that the dynamic schedule of batch size can effectively stabilize the training process of LLMs [56].
Learning Rate. Existing LLMs usually adopt a similar learning rate schedule with the warm-up and decay strategies during pre-training. Specifically, in the initial 0.1% to 0.5% of the training steps, a linear warm-up schedule is employed for gradually increasing the learning rate to the maximum value that ranges from approximately 5 √ó 10‚àí5 to 1 √ó 10‚àí4 (e.g., 6 √ó 10‚àí5 for GPT-3). Then, a cosine decay strategy is adopted in the subsequent steps, gradually reducing the learning rate to approximately 10% of its maximum value, until the convergence of the training loss.
Optimizer. The Adam optimizer [226] and AdamW optimizer [227] are widely utilized for training LLMs (e.g., GPT3), which are based on adaptive estimates of lower-order moments for first-order gradient-based optimization. Commonly, its hyper-parameters are set as follows: Œ≤1 = 0.9, Œ≤2 = 0.95 and œµ = 10‚àí8. Meanwhile, the Adafactor optimizer [228] has also been utilized in training LLMs (e.g., PaLM and T5), which is a variant of the Adam optimizer specially designed for conserving GPU memory during training. The hyper-parameters of the Adafactor optimizer are set as: Œ≤1 = 0.9 and Œ≤2 = 1.0 ‚àí k‚àí0.8, where k denotes the number of training steps.
Stabilizing the Training. During the pre-training of LLMs, it often suffers from the training instability issue, which may cause the model collapse. To address this issue, weight decay and gradient clipping have been widely utilized, where existing studies [55, 69, 81, 83, 97] commonly set the threshold of gradient clipping to 1.0 and weight decay rate to 0.1. However, with the scaling of LLMs, the training loss spike is also more likely to occur, leading to unstable training. To mitigate this problem, PaLM [56] and OPT [81] use a simple strategy that restarts the training process from an earlier checkpoint before the occurrence of the spike and skips over the data that may have caused the problem. Further, GLM [83] finds that the abnormal gradients of the embedding layer usually lead to spikes, and proposes to shrink the embedding layer gradients to alleviate it.

22
TABLE 5: Detailed optimization settings of several existing LLMs.

Model
GPT3 (175B) PanGu-Œ± (200B) OPT (175B) PaLM (540B) BLOOM (176B) MT-NLG (530B) Gopher (280B) Chinchilla (70B) Galactica (120B) LaMDA (137B) Jurassic-1 (178B) LLaMA (65B) GLM (130B) T5 (11B) ERNIE 3.0 Titan (260B) PanGu-Œ£ (1.085T)

Batch Size (#tokens)
32K‚Üí3.2M -
2M 1M‚Üí4M
4M 64 K‚Üí3.75M
3M‚Üí6M 1.5M‚Üí3M
2M 256K 32 K‚Üí3.2M
4M 0.4M‚Üí8.25M
64K -
0.5M

Learning Rate
6 √ó 10‚àí5 2 √ó 10‚àí5 1.2 √ó 10‚àí4 1 √ó 10‚àí2 6 √ó 10‚àí5 5 √ó 10‚àí5 4 √ó 10‚àí5 1 √ó 10‚àí4 7 √ó 10‚àí6
6 √ó 10‚àí5 1.5 √ó 10‚àí4 8 √ó 10‚àí5 1 √ó 10‚àí2 1 √ó 10‚àí4 2 √ó 10‚àí5

Warmup
yes -
yes no yes yes yes yes yes yes yes yes no yes

Decay Method
cosine decay to 10% -
manual decay inverse square root cosine decay to 10% cosine decay to 10% cosine decay to 10% cosine decay to 10% linear decay to 10%
cosine decay to 10% cosine decay to 10% inverse square root -

Optimizer
Adam Adam AdamW Adafactor Adam Adam Adam AdamW AdamW
AdamW AdamW AdaFactor Adam Adam

Precision Type
FP16 -
FP16 BF16 BF16 BF16 BF16 BF16
BF16
FP16 FP16 FP16

Weight Decay
0.1 0.1 0.1 lr2 0.1 0.1 0.1 0.1 0.1 0.1 -

Grad Clip
1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 -

Dropout
0.1 0.1 0.0 0.1 0.1 0.1 -

4.3.2 Scalable Training Techniques
As the model and data sizes increase, it has become challenging to efficiently train LLMs under a limited computational resource. Especially, two primary technical issues are required to be resolved, i.e., increasing training throughput and loading larger models into GPU memory. In this part, we review several widely used approaches in existing work to address the above two challenges, namely 3D parallelism [66, 229, 230], ZeRO [231], and mixed precision training [232], and also give general suggestions about how to utilize them for training.
3D Parallelism. 3D parallelism is actually a combination of three commonly used parallel training techniques, namely data parallelism, pipeline parallelism [229, 230], and tensor parallelism [66]21. We next introduce the three parallel training techniques.
‚Ä¢ Data parallelism. Data parallelism is one of the most fundamental approaches to improving the training throughput. It replicates the model parameters and optimizer states across multiple GPUs and then distributes the whole training corpus into these GPUs. In this way, each GPU only needs to process the assigned data for it, and performs the forward and backward propagation to obtain the gradients. The computed gradients on different GPUs will be further aggregated to obtain the gradients of the entire batch for updating the models in all GPUs. In this way, as the calculations of gradients are independently performed on different GPUs, the data parallelism mechanism is highly scalable, enabling the way that increases the number of GPUs to improve training throughput. Furthermore, this technique is simple in implementation, and most of existing popular deep learning libraries have already implemented data parallelism, such as TensorFlow and PyTorch.
‚Ä¢ Pipeline parallelism. Pipeline parallelism aims to distribute the different layers of a LLM into multiple GPUs. Especially, in the case of a Transformer model, pipeline parallelism loads consecutive layers onto the same GPU, to reduce the cost of transmitting the computed hidden states or gradients between GPUs. However, a naive implemen-
21. Model parallelism is a more broader term that includes tensor parallelism and pipeline parallelism in some work [66].

tation of pipeline parallelism may result in a lower GPU utilization rate as each GPU has to wait for the previous one to complete the computation, leading to the unnecessary cost of bubbles overhead [229]. To reduce these bubbles in pipeline parallelism, GPipe [229] and PipeDream [230] propose the techniques of padding multiple batches of data and asynchronous gradient update to improve the pipeline efficiency.
‚Ä¢ Tensor parallelism. Tensor parallelism is also a commonly used technique that aims to decompose the LLM for multi-GPU loading. Unlike pipeline parallelism, tensor parallelism focuses on decomposing the tensors (the parameter matrices) of LLMs. For a matrix multiplication operation Y = XA in the LLM, the parameter matrix A can be split into two submatrices, A1 and A2, by column, which can be expressed as Y = [XA1, XA2]. By placing matrices A1 and A2 on different GPUs, the matrix multiplication operation would be invoked at two GPUs in parallel, and the final result can be obtained by combining the outputs from the two GPUs through across-GPU communication. Currently, tensor parallelism has been supported in several open-source libraries, e.g., Megatron-LM [66], and can be extended to higher-dimensional tensors. Also, Colossal-AI has implemented tensor parallelism for higher-dimensional tensors [233‚Äì235] and proposed sequence parallelism [236] especially for sequence data, which can further decompose the attention operation of the Transformer model.
ZeRO. ZeRO [231] technique, proposed by the DeepSpeed [65] library, focuses on the issue of memory redundancy in data parallelism. As mentioned before, data parallelism requires each GPU to store the same copy of a LLM, including model parameters, model gradients, and optimizer parameters. Whereas, not all of the above data is necessary to be retained on each GPU, which would cause a memory redundancy problem. To resolve it, the ZeRO technique aims to retain only a fraction of data on each GPU, while the rest data can be retrieved from other GPUs when required. Specifically, ZeRO provides three solutions, depending on how the three parts of the data are stored, namely optimizer state partitioning, gradient partitioning, and parameter partitioning. Empirical results indicate that the first two solutions do not increase the communication

23

overhead, and the third solution increases about 50% communication overhead but saves memory proportional to the number of GPUs. PyTorch has implemented a similar technique as ZeRO, called FSDP [237].

behaviors of LLMs with human values or preferences. Further, we will also discuss efficient tuning and quantization for model adaptation in resource-limited settings. In what follows, we will introduce the four parts in detail.

Mixed Precision Training. In previous PLMs (e.g., BERT [23]), 32-bit floating-point numbers, also known as FP32, have been predominantly used for pre-training. In recent years, to pre-train extremely large language models, some studies [232] have started to utilize 16-bit floatingpoint numbers (FP16), which reduces memory usage and communication overhead. Additionally, as popular NVIDIA GPUs (e.g., A100) have twice the amount of FP16 computation units as FP32, the computational efficiency of FP16 can be further improved. However, existing work has found that FP16 may lead to the loss of computational accuracy [59, 69], which affects the final model performance. To alleviate it, an alternative called Brain Floating Point (BF16) has been used for training, which allocates more exponent bits and fewer significant bits than FP16. For pre-training, BF16 generally performs better than FP16 on representation accuracy [69].
Overall Training Suggestion. In practice, the above training techniques, especially 3D parallelism, are often jointly used to improve the training throughput and large model loading. For instance, researchers have incorporated 8-way data parallelism, 4-way tensor parallelism, and 12-way pipeline parallelism, enabling the training of BLOOM [69] on 384 A100 GPUs. Currently, open-source libraries like DeepSpeed [65], Colossal-AI [149], and Alpa [238] can well support the three parallel training methods. To reduce the memory redundancy, ZeRO, FSDP, and activation recomputation techniques [68, 239] can be also employed for training LLMs, which have already been integrated into DeepSpeed, PyTorch, and Megatron-LM. In addition, the mixed precision training technique such as BF16 can be also leveraged to improve the training efficiency and reduce GPU memory usage, while it requires necessary support on hardware (e.g., A100 GPU). Because training large models is a time-intensive process, it would be useful to forecast the model performance and detect abnormal issues at an early stage. For this purpose, GPT-4 [46] has recently introduced a new mechanism called predictable scaling built on a deep learning stack, enabling the performance prediction of large models with a much smaller model, which might be quite useful for developing LLMs. In practice, one can further leverage the supporting training techniques of mainstream deep learning frameworks. For instance, PyTorch supports the data parallel training algorithm FSDP [237] (i.e., fully sharded data parallel), which allows for partial offloading of training computations to CPUs if desired.
5 ADAPTATION OF LLMS
After pre-training, LLMs can acquire the general abilities for solving various tasks. However, an increasing number of studies have shown that LLM‚Äôs abilities can be further adapted according to specific goals. In this section, we introduce two major approaches to adapting pre-trained LLMs, namely instruction tuning and alignment tuning. The former approach mainly aims to enhance (or unlock) the abilities of LLMs, while the latter approach aims to align the

TABLE 6: A detailed list of available collections for instruction tuning.

Categories Collections

Time #Examples

Task

Nat. Inst. [240]

Apr-2021

FLAN [62]

Sep-2021

P3 [241]

Oct-2021

Super Nat. Inst. [79] Apr-2022

MVPCorpus [242] Jun-2022

xP3 [84]

Nov-2022

OIG22

Mar-2023

193K 4.4M 12.1M 5M 41M 81M 43M

Chat

HH-RLHF [243] Apr-2022

HC3 [244]

Jan-2023

ShareGPT23

Mar-2023

Dolly 24

Apr-2023

OpenAssistant [245] Apr-2023

160K 87K 90K 15K 161K

Synthetic

Self-Instruct [125] Alpaca [119] Guanaco 25 Baize [246] BELLE [247]

Dec-2022 Mar-2023 Mar-2023 Apr-2023 Apr-2023

82K 52K 535K 158K 1.5M

5.1 Instruction Tuning
In essence, instruction tuning is the approach to fine-tuning pre-trained LLMs on a collection of formatted instances in the form of natural language [62], which is highly related to supervised fine-tuning [61] and multi-task prompted training [28]. In order to perform instruction tuning, we first need to collect or construct instruction-formatted instances. Then, we employ these formatted instances to fine-tune LLMs in a supervised learning way (e.g., training with the sequence-to-sequence loss). After instruction tuning, LLMs can demonstrate superior abilities to generalize to unseen tasks [28, 62, 64], even in a multilingual setting [84].
A recent survey [248] presents a systematic overview of the research on instruction tuning. In comparison to that, we mainly focus on the effect of instruction tuning on LLMs and provide detailed guidelines or strategies for instance collection and tuning. In addition, we also discuss the use of instruction tuning for satisfying the real needs of users, which has been widely applied in existing LLMs, e.g., InstructGPT [61] and GPT-4 [46].
5.1.1 Formatted Instance Construction
Generally, an instruction-formatted instance consists of a task description (called an instruction), an optional input, the corresponding output, and a small number of demonstrations (optional). As important public resources, existing studies have released a large number of labeled data formatted in natural language (see the list of available resources in Table 6). Next, we introduce three major methods for constructing formatted instances (see an illustration in
22. https://laion.ai/blog/oig-dataset/ 23. https://sharegpt.com/ 24. https://github.com/databrickslabs/dolly 25. https://huggingface.co/datasets/JosephusCheung/GuanacoDataset

24

Human-written NLP Datasets

Task description
Please answer this question:

API collection Human-written &

Demonstrations
Q: What is the capital of France? A: Paris.
Q: What is the capital of Brazil? A: Brasilia

Input

Output

Q: What is the capital of China? A: Beijing.

Task description
Can you recommend some ways to lose weight?
Desired output written by human
Output
Here are some ways to lose weight: 1. Eat a healthy diet: Focus on ‚Ä¶ 2. Increase physical activity: Engage ‚Ä¶

Seed Instances

Instance Pool

Instruction Generation LLM
Task description
Give me a quote from a famous person on this topic.

Filter

Input-Output Generation LLM

Input

Output

Input: The importance of being honest. Output: Honesty is the first chapter in the book of wisdom.

(a) Formatting Task Datasets

(b) Formatting Daily Chat Data

(c) Formatting Synthetic Data

Fig. 8: An illustration of instance formatting and three different methods for constructing the instruction-formatted instances.

Figure 8) and then discuss several key factors for instance construction.
Formatting Task Datasets. Before instruction tuning was proposed, several early studies [242, 249, 250] collected the instances from a diverse range of tasks (e.g., text summarization, text classification, and translation) to create supervised multi-task training datasets. As a major source of instruction tuning instances, it is convenient to format these multi-task training datasets with natural language task descriptions. Specifically, recent work [28, 61, 62, 79] augments the labeled datasets with human-written task descriptions, which instructs LLMs to understand the tasks by explaining the task goal. For example, in Figure 8(a), a task description ‚ÄúPlease answer this question‚Äù is added for each example in the question-answering task. After instruction tuning, LLMs can generalize well to other unseen tasks by following their task descriptions [28, 62, 64]. In particular, it has been shown that instructions are the crucial factor in task generalization ability for LLMs [62]: by fine-tuning the model on labeled datasets with the task descriptions removed, it results in a dramatic drop in model performance. To better generate labeled instances for instruction tuning, a crowd-sourcing platform, PromptSource [241] has been proposed to effectively create, share, and verify the task descriptions for different datasets. To enrich the training instances, several studies [28, 242, 251] also try to invert the input-output pairs of existing instances with specially designed task descriptions for instruction tuning. For instance, given a questionanswer pair, we can create a new instance by predicting the question-conditioned answer (e.g., ‚ÄúPlease generate a question based on the answer:‚Äù).
Formatting Daily Chat Data. Despite that a large number of training instances have been formatted with instructions, they mainly come from public NLP datasets, either lacking instruction diversity or mismatching with real human needs [61]. To overcome this issue, InstructGPT [61] proposes to take the queries that real users have submitted

to the OpenAI API as the task descriptions. User queries are expressed in natural languages, which are particularly suitable for eliciting the ability of instruction following for LLMs. Additionally, to enrich the task diversity, human labelers are also asked to compose the instructions for reallife tasks, including open-ended generation, open question answering, brainstorming, and chatting. Then, they let another group of labelers directly answer these instructions as the output. Finally, they pair one instruction (i.e., the collected user query) and the expected output (i.e., the human-written answer) as a training instance. Note that InstructGPT also employs these real-world tasks formatted in natural language for alignment tuning (discussed in Section 5.2). Further, GPT-4 [46] has designed potentially high-risk instructions and guided the model to reject these instructions through supervised fine-tuning for safety concerns. Recently, researchers also collect the users‚Äô chat requests as the input data and employ ChatGPT or GPT-4 to respond to these requests as the output data. A representative collection of such dataset is the conversational data from ShareGPT.
Formatting Synthetic Data. To reduce the burden of human annotation or manual collection, several semi-automated approaches [125] have been proposed for constructing instances by feeding existing instances into LLMs to synthesize diverse task descriptions and instances. As illustrated in Figure 8(c), the Self-Instruct method only needs around 100 instances as the initial task pool. Then, they randomly select a few instances from the pool as demonstrations and prompt a LLM to generate new instructions and corresponding input-output pairs. After the quality and diversity filtering, newly generated instances would be added into the task pool. Hence, the synthetic method is an effective and economical way to generate large-scale instruction data for LLMs.
Key Factors for Instance Construction. The quality of instruction instances has an important impact on the perfor-

25

mance of the model. Here, we discuss some essential factors for instance construction.
‚Ä¢ Scaling the instructions. It has been widely shown that scaling the number of tasks can largely enhance the generalization ability of LLMs [28, 62, 79]. With the increasing of the task number, the model performance initially shows a continuous growth pattern, while the gain becomes negligible when it reaches a certain level [64, 79]. A plausible speculation is that a certain number of representative tasks can provide relatively sufficient knowledge and adding more tasks may not bring additional gains [64]. Also, it is beneficial to enhance the diversity of the task descriptions in several aspects, such as length, structure, and creativity [28]. As for the number of instances per task, it has been found that a small number of instances can usually saturate the generalization performance of the model [62, 64]. Whereas, increasing the number of instances for some tasks to a large number (e.g., a few hundreds) could potentially result in the overfitting issue and impair the model performance [79, 252].
‚Ä¢ Formatting design. As an important factor, the design of natural language format also highly impacts the generalization performance of LLMs [79]. Typically, we can add task descriptions and optional demonstrations to the inputoutput pairs of existing datasets, where the task description is the most key part for LLMs to understand the task [79]. Further, it can lead to substantial improvements by using an appropriate number of exemplars as demonstrations [64], which also alleviates the model sensitivity to instruction engineering [62, 64]. However, incorporating other components (e.g., things to avoid, reasons, and suggestions) into instructions may have a negligible or even adverse effect on the performance of LLMs [79, 240]. Recently, to elicit the step-by-step reasoning ability of LLMs, some work [64] proposes to include chain-of-thought (CoT) examples for some reasoning datasets, such as arithmetic reasoning. It has been shown that fine-tuning LLMs with both CoT and non-CoT examples can lead to a good performance across various reasoning tasks, including those that require multihop reasoning ability (e.g., commonsense question answering and arithmetic reasoning) as well as those without the need for such a reasoning way (e.g., sentiment analysis and extractive question answering) [64, 85].
To summarize, it seems that the diversity and quality of instructions is more important than the number of instances [253] since the well-performing InstructGPT [61] and Alpaca [124] utilize fewer but more diverse instructions (or instances) than the Flan-series LLMs [62, 64]. Further, it is more useful to invite labelers to compose human-need tasks than using dataset-specific tasks. However, it still lacks general guidelines to annotate human-need instances, making the task composition somehow heuristic. To reduce human efforts, we can either reuse existing formatted datasets (Table 6) or automatically construct the instructions using existing LLMs [125]. We conduct a preliminary experiment to show the effectiveness of different construction methods in Section 5.1.4.
5.1.2 Instruction Tuning Strategies
Unlike pre-training, instruction tuning is often more efficient since only a moderate number of instances are used for training. Since instruction tuning can be considered as

a supervised training process, its optimization is different from pre-training in several aspects [64], such as the training objective (i.e., sequence-to-sequence loss) and optimization configuration (e.g., smaller batch size and learning rate), which require special attention in practice. In addition to these optimization configurations, there are also two important aspects to consider for instruction tuning:
Balancing the Data Distribution. Since instruction tuning involves a mixture of different tasks, it is important to balance the proportion of different tasks during finetuning. A widely used method is the examples-proportional mixing strategy [73], i.e., combining all the datasets and sampling each instance equally from the mixed datasets. Furthermore, increasing the sampling ratio of high-quality collections (e.g., FLAN [62] and P3 [241]) can generally lead to performance improvement according to recent findings [64, 85]. Further, it is common to set a maximum cap to control the maximum number of examples that a dataset can contain during instruction tuning [73], which is set to prevent larger datasets from overwhelming the entire distribution [73, 85]. In practice, the maximum cap is typically set to several thousands or tens of thousands according to different datasets [62, 64].
Combining Instruction Tuning and Pre-Training. To make the tuning process more effective and stable, OPT-IML [85] incorporates pre-training data during instruction tuning, which can be regarded as regularization for model tuning. Further, instead of using a separate two-stage process (pretraining then instruction tuning), some studies attempt to train a model from scratch with a mixture of pre-training data (i.e., plain texts) and instruction tuning data (i.e., formatted datasets) using multi-task learning [73]. Specifically, GLM-130B [83] and Galactica [35] integrate instructionformatted datasets as a small proportion of the pre-training corpora to pre-train LLMs, which potentially achieves the advantages of pre-training and instruction tuning at the same time.
5.1.3 The Effect of Instruction Tuning
In this part, we discuss the effect of instruction tuning on LLMs in three major aspects.
Performance Improvement. Despite being tuned on a moderate number of instances, instruction tuning has become an important way to improve or unlock the abilities of LLMs [64]. Recent studies have experimented with language models in multiple scales (ranging from 77M to 540B), showing that the models of different scales can all benefit from instruction tuning [64, 251], yielding improved performance as the parameter scale increases [84]. Further, smaller models with instruction tuning can even perform better than larger models without fine-tuning [28, 64]. Besides the model scale, instruction tuning demonstrates consistent improvements in various model architectures, pre-training objectives, and model adaptation methods [64]. In practice, instruction tuning offers a general approach to enhancing the abilities of existing language models [64] (including small-sized PLMs). Also, it is much less costly than pretraining, since the amount of instruction data required by LLMs is significantly smaller than pre-training data.

26
TABLE 7: Basic statistics of the required number of GPUs, tuning time, batch size (denoted as BS) per device (full tuning and LoRA tuning), and inference rate (the number of generated tokes per second). Our experiments are conducted based on two Linux servers having 8 A800-80G GPUs and 8 3090-24G GPUs, respectively. The major difference between A800 and A100 lies in the NVLink interconnect speed. Thus, our estimations about training and inference efficiency would be slightly improved for A100, while the rest memory consumption would remain the same. The full tuning experiments are conducted using data parallel training, ZeRO Stage 3, BF16, and gradient checkpointing. Additionally, the LoRA tuning can be executed on one 80G GPU utilizing INT8 quantization with the rank setting set to 16. The max sequence length for both training settings is set to 512. The inference experiments are performed with the batch size set to 1.

Models

A800 Full Training A800 LoRA Training A800 Inference (16-bit) 3090 Inference (16-bit) 3090 Inference (8-bit) #GPU BS Time #GPU BS Time #GPU #Token/s #GPU #Token/s #GPU #Token/s

LLaMA-7B 2 8 3.0h

1 80 3.5h

1

36.6

1

24.3

1

7.5

LLaMA-13B 4 8 3.1h

1 48 5.1h

1

26.8

2

9.9

1

4.5

LLaMA-30B 8 4 6.1h

1 24 14.3h

1

17.7

4

3.8

2

2.6

LLaMA-65B 16 2 11.2h 1 4 60.6h

2

8.8

8

2.0

4

1.5

Task Generalization. Instruction tuning encourages the model to understand natural language instructions for task completion. It endows LLMs with the ability (often considered as an emergent ability) to follow human instructions [31] to perform specific tasks without demonstrations, even on unseen tasks [64]. A large number of studies have confirmed the effectiveness of instruction tuning to achieve superior performance on both seen and unseen tasks [85, 251]. Also, instruction tuning has been shown to be useful in alleviating several weaknesses of LLMs (e.g., repetitive generation or complementing the input without accomplishing a certain task) [61, 64], leading to a superior capacity to solve real-world tasks for LLMs. Furthermore, LLMs trained with instruction tuning can generalize to related tasks across languages. For example, BLOOMZ-P3 [84] is fine-tuned based on BLOOM [69] using English-only task collection P3 [241]. Interestingly, BLOOMZ-P3 can achieve a more than 50% improvement in multilingual sentence completion tasks compared to BLOOM, which shows that instruction tuning can help LLMs acquire general task skills from English-only datasets and transfer such skills into other languages [84]. In addition, it has been found that using English-only instructions can produce satisfactory results on multilingual tasks [84], which helps reduce the effort of instruction engineering for a specific language.
Domain Specialization. Existing LLMs have showcased superior capabilities in traditional NLP tasks (e.g., generation and reasoning) and daily questions. However, they may still lack domain knowledge to accomplish specific tasks, such as medicine, law, and finance (See Section 9 for a detailed discussion of LLMs in different applications). Instruction tuning is an effective approach to adapting existing general LLMs to be domain-specific experts. For instance, researchers propose to fine-tune Flan-PaLM [64] using medical datasets to create Med-PaLM [254], a medical knowledge assistant that achieves performance levels comparable to those of expert clinicians. Furthermore, a recent study [255] fine-tunes FLAN-T5 to support e-commerce recommender systems with natural language instructions, showing strong performance in a variety of recommendation tasks. There are also several open-sourced medical models instructiontuned based on LLaMA [57], such as BenTsao [256]. Also, researchers explore instruction tuning on law [257], finance [258], and arithmetic computation [259].

5.1.4 Empirical Analysis for Instruction Tuning
Fine-tuning LLMs with different instruction sets tend to lead to model variants with varied performance on downstream tasks. In this section, we will explore the effect of different types of instructions in fine-tuning LLMs (i.e., 7B LLaMA26), as well as examine the usefulness of several instruction improvement strategies.
Instruction Datasets. According to the discussion in Section 5.1.1, we mainly consider three common kinds of instructions as follows:
‚Ä¢ Task-specific instructions. For the first type of instructions, we adopt the most commonly-used multi-task instruction dataset, FLAN-T5 [64], which contains 1,836 tasks and over 15M instructions by combining four data mixtures from prior work.
‚Ä¢ Daily chat instructions. This type of instructions are conversations posed by users about daily life, which are more closely related to real-life scenarios. We adopt the ShareGPT instruciton set27, consisting of 63K real-user instructions. It has been used as the core instructions for Vicuna.
‚Ä¢ Synthetic instructions. In addition to reusing existing instructions, we can also automatically synthesize massive instructions using LLMs. We adopt the popular synthetic instruction dataset Self-Instruct-52K [125], consisting of 52K instructions paired with about 82K instance inputs and outputs. These generated instructions have a similar data distribution as the human-written seed tasks (e.g., grammar checking, brainstorming).
As the original FLAN-T5 dataset is very large (i.e., over 15M), we randomly sample 80,000 instructions from it for conducting a fair comparison with other instruction datasets (i.e., ShareGPT and Self-Instruct-52K) at a similar scale. In our experiments, we test on each individual instruction set to explore their own effects and also examine their combinatorial effects on model performance.
Improvement Strategies. Although real-world instructions from human users are more suitable for fine-tuning LLMs, it is difficult to collect them at a large scale. As alternatives to human-generated instructions, most existing research
26. Due to the limit of computational resources, we cannot conduct large-scale experiments on 10B+ LLaMA variants right now, which would be scheduled in a future version.
27. https://github.com/domeccleston/sharegpt

27
TABLE 8: Results of instruction-tuning experiments (all in a single-turn conversation) based on the LLaMA (7B) model under the chat and QA setting. We employ four instruction improvement strategies on the Self-Instruct-52K dataset, i.e., enhancing the complexity (w/ complexity), increasing the diversity (w/ diversity), balancing the difficulty (w/ difficulty), and scaling the instruction number (w/ scaling). ‚àóSince we select the LLaMA-7B model fine-tuned on Self-Instruct-52K as the baseline, we omit the win rate of the fine-tuned model with Self-Instruct-52K against itself.

Models LLaMA (7B)

Dataset Mixtures
‚ë† FLAN-T5 ‚ë° ShareGPT ‚ë¢ Self-Instruct-52K ‚ë°+‚ë¢ ‚ë†+‚ë°+‚ë¢
‚ë¢ Self-Instruct-52K w/ complexity w/ diversity w/ difficulty w/ scaling

Instruction Numbers
80,000 63,184 82,439 145,623 225,623
82,439 70,000 70,000 70,000 220,000

Lexical Diversity
48.48 77.31 25.92 48.22 48.28
25.92 70.43 75.59 73.48 57.78

Chat
AlpacaFarm
23.77 81.30
/‚àó 71.36 70.00
/‚àó 76.96 81.55 79.15 51.13

QA

MMLU BBH3k

38.58 38.11 37.52 41.26 43.69

32.79 27.71 29.81 28.36 29.69

37.52 39.73 38.01 32.55 33.81

29.81 33.25 30.03 31.25 26.63

mainly adopts synthetic instructions generated by LLMs. However, there are some potential problems with synthetic instructions, such as poor topic diversity and uneven instruction difficulty (either too simple or too difficult). Thus, it is necessary to improve the quality of the synthetic instructions. Next, we summarize four major improvement strategies widely used in existing work as follows:
‚Ä¢ Enhancing the instruction complexity. As discussed in existing work [260], enhancing the complexity of instructions can improve the model capacity of LLMs in following complex instructions, e.g., including more task demands or requiring more reasoning steps. To validate this strategy, we follow WizardLM [260] by gradually increasing the complexity levels, e.g., adding constraints, increasing reasoning steps, and complicating the input. We leverage the publicly released WizardLM-70K instructions28 as the complexityenhanced instruction dataset, which has been generated via the above enhancement approach based on the Self-Instruct52K dataset [260].
‚Ä¢ Increasing the topic diversity. In addition to the complexity, improving the topic diversity of the instruction dataset can help elicit different abilities of LLMs on diverse tasks in real world [261]. However, it is difficult to directly control the self-instruct process for generating diverse instructions. Following YuLan-Chat [262], we employ ChatGPT to rewrite the instructions from Self-Instruct-52K dataset for adapting them into 293 topics via specific prompts. Finally, we obtain 70K instructions as the diversity-increased dataset.
‚Ä¢ Scaling the instruction number. In addition to the above aspects, the number of instructions is also an important factor that may affect the model performance. Specially, using more instructions can extend the task knowledge and improve the ability of instruction following for LLMs [64]. To examine this strategy, we sample new instructions from the synthesized instruction set released from the MOSS project29, as they are also synthesized using the same selfinstruct method [125]. We mix them with the Self-Instruct52K dataset to compose a larger one containing 220K instructions.
28. https://huggingface.co/datasets/victor123/evol instruct 70k 29. https://github.com/OpenLMLab/MOSS

‚Ä¢ Balancing the instruction difficulty. As the synthetic instructions tend to contain too easy or too hard ones, it is likely to result in training instability or even overfitting for LLMs. To explore the potential effects, we leverage the perplexity score of LLMs to estimate the difficulty of instructions and remove too easy or too hard instructions. To generate the same scale of instructions for fair comparison, we adopt a LLaMA-7B model to compute the perplexity for the 220K instructions from the large instruction dataset, and then keep 70K instructions of moderate perplexity scores as the difficulty-balanced dataset.
Experimental Setup. To conduct the experiments on the effect of instruction data, we leverage these new instruction datasets for tuning LLaMA-7B, a popular LLM backbone that has been widely used for instruction-tuning. We use the code from YuLan-Chat [262] for our experiments, and train the model on a server of 8 A800-80G GPUs. All the hyperparameters settings remain the same as Stanford Alpaca. To better evaluate the instruction following ability of fine-tuned models, we consider two settings, namely Chat setting and QA setting. The chat setting mainly utilizes user instructions and queries from daily chat, whereas the QA setting mainly employs question answering examples from existing NLP datasets. The evaluation on the chat setting is conducted based on the AlpacaFarm evaluation set [263]. Instead of using a full pairwise comparison, we select the LLaMA7B model fine-tuned on Self-Instruct-52K as the reference baseline, and then compare all other fine-tuned models with it. Since our focus is to examine the usefulness of different strategies to generate the instructions, the model fine-tuned on Self-Instruct-52K can serve as a good reference. Following AlpacaFarm [263], for each comparison, we employ ChatGPT to automatically annotate which response from two compared models each time is the best for the user query, and report the win rate (%) as the evaluation metric. For the QA setting, we select two benchmarks, MMLU [264] and BBH3k (a subset of BBH benchmark [265] released by YuLan-Chat), and evaluate the accuracy based on their default settings by using heuristic rules to parse the answers from these LLMs.
For both instruction tuning and evaluation, we adopt

28

the following prompt: ‚ÄúThe following is a conversation between a human and an AI assistant. The AI assistant gives helpful, detailed, and polite answers to the user‚Äôs questions.\n [|Human|]:{input}\n[|AI|]:‚Äù. To reproduce our results, we release the code and data at the link: https://github.com/ RUCAIBox/LLMSurvey/tree/main/Experiments.
Results and Analysis. The experimental results using different instruction datasets based on 7B LLaMA are shown in Table 8. Next, we summarize and analyze each of our findings in detail.
‚Ä¢ Task-formatted instructions are more proper for the QA setting, but may not be useful for the chat setting. By comparing the performance of instruction tuning using FLAN-T5 with that of ShareGPT and Self-Instruct-52K, we can observe that FLAN-T5 consistently achieves a better performance on QA benchmarks while underperforms ShareGPT on the chat setting. The reason is that FLAN-T5 is composed of a mixture of instructions and examples from existing NLP tasks, e.g., translation and reading comprehension. As a result, LLaMA fine-tuned with FLAN-T5 performs better on QA tasks, but poorly on user queries. In contrast, ShareGPT consists of real-world human-ChatGPT conversations, which is able to better elicit LLaMA to follow user instructions in daily life, while may not be suitable for accomplishing the QA tasks.
‚Ä¢ A mixture of different kinds of instructions are very helpful in improving the comprehensive abilities of LLMs. After mixing the three kinds of instructions for fine-tuning, we can see that the derived LLaMA variant (with FLAN-T5, ShareGPT and Self-Instruct-52K) performs well in both task settings. In MMLU, its performance can surpass the ones using individual instruction set by a large margin, i.e., 43.69 vs. 38.58 (FLAN-T5). It shows that mixing multiple sources of instruction datasets is helpful to improve the performance of instruction-tuned LLMs, which scales the instruction number as well as increases the diversity.
‚Ä¢ Enhancing the complexity and diversity of instructions leads to an improved model performance. By increasing the complexity and diversity of the Self-Instruct-52K dataset respectively, the chat and QA performance of LLaMA can be consistently improved, e.g., from 37.52 to 39.73 in MMLU. It demonstrates that both strategies are useful to improve the instruction following ability of LLMs. Further, we can see that improving the complexity yields a larger performance improvement on QA tasks. The reason is that the QA tasks mostly consist of difficult questions for evaluating LLMs, which can be better solved by LLMs that have learned complex instructions at the fine-tuning stage.
‚Ä¢ Simply increasing the number of instructions may not be that useful, and balancing the difficulty is not always helpful. As the results shown in Table 8, balancing the difficulty and increasing the number of fine-tuning instructions are not very helpful in our experiments. Especially for scaling the instruction number, it even hurts the performance, e.g., a decrease from 29.81 to 26.63 in BBH3k. It shows that simply scaling the number of synthesized instructions without quality control may not be effective to improve the performance. Furthermore, fine-tuning with the instructions of moderate difficulty also performs well in the chat setting, while slightly decreasing the performance in the QA setting. A possible reason is that we filter complex and hard

instructions with large perplexity scores, which would hurt the model performance in answering complex questions.
Instruction Tuning Suggestions
To conduct instruction tuning on LLMs, one can prepare the computational resources according to the basic statistics about the required number of GPUs and tuning time in Table 7. After setting up the development environment, we recommend beginners to follow the code of Alpaca repositorya for instruction tuning. Subsequently, one should select the base model and construct the instruction datasets as we discuss in this section. When computational resources for training are constrained, users can utilize LoRA for parameter-efficient tuning (see Section 5.3). As for inference, users can further use quantization methods to deploy LLMs on fewer or smaller GPUs (see Section 5.4).
a. https://github.com/tatsu-lab/stanford alpaca/#finetuning
5.2 Alignment Tuning
This part first presents the background of alignment with its definition and criteria, then focuses on the collection of human feedback data for aligning LLMs, and finally discusses the key technique of reinforcement learning from human feedback (RLHF) for alignment tuning.
5.2.1 Background and Criteria for Alignment
Background. LLMs have shown remarkable capabilities in a wide range of NLP tasks [55, 56, 62, 81]. However, these models may sometimes exhibit unintended behaviors, e.g., fabricating false information, pursuing inaccurate objectives, and producing harmful, misleading, and biased expressions [61, 266]. For LLMs, the language modeling objective pre-trains the model parameters by word prediction while lacking the consideration of human values or preferences. To avert these unexpected behaviors, human alignment has been proposed to make LLMs act in line with human expectations [61, 267]. However, unlike the original pre-training and adaptation tuning (e.g., instruction tuning), such an alignment requires considering very different criteria (e.g., helpfulness, honesty, and harmlessness). It has been shown that alignment might harm the general abilities of LLMs to some extent, which is called alignment tax in related literature [268].
Alignment Criteria. Recently, there is increasing attention on developing multifarious criteria to regulate the behaviors of LLMs. Here, we take three representative alignment criteria (i.e., helpful, honest, and harmless) as examples for discussion, which have been widely adopted in existing literature [61, 268]. In addition, there are other alignment criteria for LLMs from different perspectives including behavior, intent, incentive, and inner aspects [266], which are essentially similar (or at least with similar alignment techniques) to the above three criteria. It is also feasible to modify the three criteria according to specific needs, e.g.,

29

substituting honesty with correctness [100]. Next, we give brief explanations about the three representative alignment criteria:
‚Ä¢ Helpfulness. To be helpful, the LLM should demonstrate a clear attempt to assist users in solving their tasks or answering questions in a concise and efficient manner as possible. At a higher level, when further clarification is needed, the LLM should demonstrate the capability of eliciting additional relevant information through pertinent inquiries and exhibit suitable levels of sensitivity, perceptiveness, and prudence [268]. Realizing the alignment of helpful behavior is challenging for LLMs since it is difficult to precisely define and measure the intention of users [266].
‚Ä¢ Honesty. At a basic level, a LLM aligned to be honest should present accurate content to users instead of fabricating information. Additionally, it is crucial for the LLM to convey appropriate degrees of uncertainty in its output, in order to avoid any form of deception or misrepresentation of information. This requires the model to know about its capabilities and levels of knowledge (e.g., ‚Äúknow unknowns‚Äù). According to the discussion in [268], honesty is a more objective criterion compared to helpfulness and harmlessness, hence honesty alignment could potentially be developed with less reliance on human efforts.
‚Ä¢ Harmlessness. To be harmless, it requires that the language produced by the model should not be offensive or discriminatory. To the best of its abilities, the model should be capable of detecting covert endeavors aimed at soliciting requests for malicious purposes. Ideally, when the model was induced to conduct a dangerous action (e.g., committing a crime), the LLM should politely refuse. Nonetheless, what behaviors are deemed harmful and to what extent vary amongst individuals or societies [268] highly depend on who is using the LLM, the type of the posed question, and the context (e.g., time) at which the LLM is being used.
As we can see, these criteria are quite subjective, and are developed based on human cognition. Thus, it is difficult to directly formulate them as optimization objectives for LLMs. In existing work, there are many ways to fulfill these criteria when aligning LLMs. A promising technique is red teaming [269], which involves using manual or automated means to probe LLMs in an adversarial way to generate harmful outputs and then updates LLMs to prevent such outputs.
5.2.2 Collecting Human Feedback
During the pre-training stage, LLMs are trained using the language modeling objective on a large-scale corpus. However, it cannot take into account the subjective and qualitative evaluations of LLM outputs by humans (called human feedback in this survey). High-quality human feedback is extremely important for aligning LLMs with human preferences and values. In this part, we discuss how to select a team of human labelers for feedback data collection.
Human Labeler Selection. In existing work, the dominant method for generating human feedback data is human annotation [61, 100, 267]. This highlights the critical role of selecting appropriate human labelers. To provide highquality feedback, human labelers are supposed to have a qualified level of education and excellent proficiency in En-

glish. For example, Sparrow [100] requires human labelers to be UK-based native English speakers who have obtained at least an undergraduate-level educational qualification. Even then, several studies [267] have found that there still exists a mismatch between the intentions of researchers and human labelers, which may lead to low-quality human feedback and cause LLMs to produce unexpected output. To address this issue, InstructGPT [61] further conducts a screening process to filter labelers by assessing the agreement between human labelers and researchers. Specifically, researchers first label a small amount of data and then measure the agreement between themselves and human labelers. The labelers with the highest agreement will be selected to proceed with the subsequent annotation work. In some other work [270], ‚Äúsuper raters‚Äù are used to ensure the high quality of human feedback. Researchers evaluate the performance of human labelers and select a group of well-performing human labelers (e.g., high agreement) as super raters. The super raters will be given priority to collaborate with the researchers in the subsequent study. When human labelers annotate the output of LLMs, it is helpful to specify detailed instructions and provide instant guidance for human labelers, which can further regulate the annotation of labelers.
Human Feedback Collection. In existing work, there are mainly three kinds of approaches to collecting feedback and preference data from human labelers.
‚Ä¢ Ranking-based approach. In early work [267], human labelers often evaluate model-generated outputs in a coarsegrained manner (i.e., only selecting the best) without taking into account more fine-grained alignment criteria. Nonetheless, different labelers may hold diverse opinions on the selection of the best candidate output, and this method disregards the unselected samples, which may lead to inaccurate or incomplete human feedback. To address this issue, subsequent studies [100] introduce the Elo rating system to derive the preference ranking by comparing candidate outputs. The ranking of outputs serves as the training signal that guides the model to prefer certain outputs over others, thus inducing outputs that are more reliable and safer.
‚Ä¢ Question-based approach. Further, human labelers can provide more detailed feedback by answering certain questions designed by researchers [72], covering the alignment criteria as well as additional constraints for LLMs. Specially, in WebGPT [72], to assist the model in filtering and utilizing relevant information from retrieved documents, human labelers are required to answer questions with multiple options about whether the retrieved documents are useful for answering the given input.
‚Ä¢ Rule-based approach. Many studies also develop rulebased methods to provide more detailed human feedback. As a typical case, Sparrow [100] not only selects the response that labelers consider the best but also uses a series of rules to test whether model-generated responses meet the alignment criteria of being helpful, correct, and harmless. In this way, two kinds of human feedback data can be obtained: (1) the response preference feedback is obtained by comparing the quality of model-generated output in pairs, and (2) the rule violation feedback is obtained by collecting the assessment from human labelers (i.e., a score indicating

30

Human Annotator

Prompts Demonstrations

Supervised Fine-tuning
Training with demonstration data
üî•
Pre-trained LM

Prompts

Demonstration Data LM Outputs

üî•
Reward Model

Reward Model Training
üßä
Pre-trained LM

Ranking Human Feedback Training with feedback data

Prompts LM Outputs

üßä
Reward Model

RL Fine-tuning
üî•

Aligned LM

üòä/üòû
Reward

Training with RL algorithm (PPO)

Fig. 9: The workflow of the RLHF algorithm.

to what extent the generated output has violated the rules). Furthermore, GPT-4 [46] utilizes a set of zero-shot classifiers (based on GPT-4 itself) as rule-based reward models, which can automatically determine whether the model-generated outputs violate a set of human-written rules.
In the following, we focus on a well-known technique, reinforcement learning from human feedback (RLHF), which has been widely used in the recent powerful LLMs such as ChatGPT. As discussed below, the alignment criteria introduced in Section 5.2.1 can be fulfilled by learning from human feedback on the responses of LLMs to users‚Äô queries.
5.2.3 Reinforcement Learning from Human Feedback
To align LLMs with human values, reinforcement learning from human feedback (RLHF) [70, 267] has been proposed to fine-tune LLMs with the collected human feedback data, which is useful to improve the alignment criteria (e.g., helpfulness, honesty, and harmlessness). RLHF employs reinforcement learning (RL) algorithms (e.g., Proximal Policy Optimization (PPO) [111]) to adapt LLMs to human feedback by learning a reward model. Such an approach incorporates humans in the training loop for developing well-aligned LLMs, as exemplified by InstructGPT [61].
RLHF System. The RLHF system mainly comprises three key components: a pre-trained LM to be aligned, a reward model learning from human feedback, and a RL algorithm training the LM. Specifically, the pre-trained LM is typically a generative model that is initialized with existing pretrained LM parameters. For example, OpenAI uses 175B GPT-3 for its first popular RLHF model, InstructGPT [61], and DeepMind uses the 280 billion parameter model Gopher [59] for its GopherCite model [270]. Further, the reward model (RM) provides (learned) guidance signals that reflect human preferences for the text generated by the LM, usually in the form of a scalar value. The reward model can take on two forms: a fine-tuned LM or a LM trained de novo using

human preference data. Existing work typically employs reward models having a parameter scale different from that of the aligned LM [61, 270]. For example, OpenAI uses 6B GPT-3 and DeepMind uses 7B Gopher as the reward model, respectively. Finally, to optimize the pre-trained LM using the signal from the reward model, a specific RL algorithm is designed for large-scale model tuning. Specifically, Proximal Policy Optimization (PPO) [111] is a widely used RL algorithm for alignment in existing work [61, 100, 270].
Key Steps for RLHF. Figure 9 illustrates the overall threestep process of RLHF [61] as introduced below.
‚Ä¢ Supervised fine-tuning. To make the LM initially perform desired behaviors, it usually needs to collect a supervised dataset containing input prompts (instruction) and desired outputs for fine-tuning the LM. These prompts and outputs can be written by human labelers for some specific tasks while ensuring the diversity of tasks. For example, InstructGPT [61] asks human labelers to compose prompts (e.g., ‚ÄúList five ideas for how to regain enthusiasm for my career‚Äù) and desired outputs for several generative tasks such as open QA, brainstorming, chatting, and rewriting. Note that the first step is optional in specific settings or scenarios.
‚Ä¢ Reward model training. The second step is to train the RM using human feedback data. Specifically, we employ the LM to generate a certain number of output texts using sampled prompts (from either the supervised dataset or the human-generated prompt) as input. We then invite human labelers to annotate the preference for these pairs. The annotation process can be conducted in multiple forms, and a common approach is to annotate by ranking the generated candidate texts, which can reduce the inconsistency among annotators. Then, the RM is trained to predict the human-preferred output. In InstructGPT, labelers rank model-generated outputs from best to worst, and the RM (i.e., 6B GPT-3) is trained to predict the ranking.
‚Ä¢ RL fine-tuning. At this step, aligning (i.e., fine-tuning) the LM is formalized as an RL problem. In this setting, the pre-trained LM acts as the policy that takes as input a prompt and returns an output text, the action space of it is the vocabulary, the state is the currently generated token sequence, and the reward is provided by the RM. To avoid eviating significantly from the initial (before tuning) LM, a penalty term is commonly incorporated into the reward function. For example, InstructGPT optimizes the LM against the RM using the PPO algorithm. For each input prompt, InstructGPT calculates the KL divergence between the generated results from the current LM and the initial LM as the penalty. It is noted that the second and final steps can be iterated in multiple turns for better aligning LLMs. Due to the instability of the RL algorithm, recent work [271] replaces the RL tuning with another supervised fine-tuning by reusing the best ranked samples with higher rewards.
5.3 Parameter-Efficient Model Adaptation
In the above, we have discussed the approaches of instruction tuning and alignment tuning to adapt LLMs according to specific goals. Since LLMs consist of a huge amount of model parameters, it would be costly to perform the fullparameter tuning. In this section, we will discuss how to conduct efficient tuning on LLMs. We first review several

31

MHA Adapter FFN Adapter
‚Ä¶
MHA Adapter FFN Adapter

Prefix Prefix

Layer #N
‚Ä¶
Layer #1

Layer #N
‚Ä¶
Layer #1

Wdown
LoRA
Wdown

Layer #N
‚Ä¶
Layer #1

Input (a) Adapter Tuning

Input (b) Prefix Tuning

Prompt

Input

(c) Prompt Tuning

Input (d) Low-Rank Adapation

Fig. 10: An illustration of four different parameter-efficient fine-tuning methods. MHA and FFN denote the multi-head attention and feed-forward networks in the Transformer layer, respectively.

representative parameter-efficient fine-tuning methods for Transformer language models, and then summarize existing work on parameter-efficient fine-tuned LLMs.
5.3.1 Parameter-Efficient Fine-Tuning Methods
In existing literature, parameter-efficient fine-tuning [127, 272, 273] has been an important topic that aims to reduce the number of trainable parameters while retaining a good performance as possible. In what follows, we briefly review four parameter-efficient fine-tuning methods for Transformer language models, including adapter tuning, prefix tuning, prompt tuning and LoRA. The illustration of these four methods are shown in Figure 12.
Adapter Tuning. Adapter tuning incorporates small neural network modules (called adapter) into the Transformer models [274]. To implement the adapter module, a bottleneck architecture has been proposed in [274, 275], which first compresses the original feature vector into a smaller dimension (followed by a nonlinear transformation) and then recovers it to the original dimension. The adapter modules would be integrated into each Transformer layer, typically using a serial insertion after each of the two core parts (i.e., attention layer and feed-forward layer) of a Transformer layer. Alternatively, parallel adapters [276] can be also used in Transformer layers, where it places two adapter modules in parallel with the attention layer and feed-forward layer accordingly. During fine-tuning, the adapter modules would be optimized according to the specific task goals, while the parameters of the original language model are frozen in this process. In this way, we can effectively reduce the number of trainable parameters during fine-tuning.
Prefix Tuning. Prefix tuning [272] prepends a sequence of prefixes, which are a set of trainable continuous vectors, to each Transformer layer in language models. These prefix vectors are task-specific, which can be considered as virtual token embeddings. To optimize the prefix vectors, a reparameterization trick [272] has been proposed by learning a MLP function that maps a smaller matrix to the parameter matrix of prefixes, instead of directly optimizing the prefixes. It has been shown that this trick is useful for stable training. After optimization, the mapping function would be discarded, and only the derived prefix vectors are kept to enhance task-specific performance. Since only the prefix parameters would be trained, it can lead to a parameterefficient model optimization. Similar to prefix tuning, ptuning v2 [277] incorporates layer-wise prompt vectors into

the Transformer architecture specially for natural language understanding, which also utilizes multi-task learning for jointly optimizing shared prompts. It has been shown to be useful in improving the model performance of different parameter scales on natural language understanding tasks.
Prompt Tuning. Different from prefix tuning, prompt tuning [273, 278] mainly focuses on incorporating trainable prompt vectors at the input layer30. Based on the discrete prompting methods [280, 281], it augments the input text by including a group of soft prompt tokens (either in a free form [278] or a prefix form [273]), and then takes the prompt-augmented input to solve specific downstream tasks. In implementation, task-specific prompt embeddings are combined with the input text embeddings, which are subsequently fed into language models. P-tuning [278] has proposed a free form to combine the context, prompt and target tokens, which can be applied to the architectures for both natural language understanding and generation. They further learn the representations of soft prompt tokens by a bidirectional LSTM. Another representative approach [273] named prompt tuning directly prepends prefix prompts to the input. During training, only the prompt embeddings would be learned according to task-specific supervisions. Since this method only includes a small number of trainable parameters at the input layer, it has been found that the performance highly relies on the model capacity of the underlying language models [273].
Low-Rank Adaptation (LoRA). LoRA [127] imposes the low-rank constraint for approximating the update matrix at each dense layer, so as to reduce the trainable parameters for adapting to downstream tasks. Consider the case of optimizing a parameter matrix W. The update process can be written in a general form as: W ‚Üê W + ‚àÜW. The basic idea of LoRA is to freeze the original matrix W ‚àà Rm√ón while approximating the parameter update ‚àÜW by lowrank decomposition matrices, i.e., ‚àÜW = A ¬∑ B‚ä§, where A ‚àà Rm√ók and B ‚àà Rn√ók are the trainable parameters for task adaptation and k ‚â™ min(m, n) is the reduced rank. The
30. Here, prompt tuning denotes a category of related efficient tuning methods exemplified by the work [273, 278, 279], instead of a specific method as used in [273]. Indeed, the prefix based tuning methods [272, 277] can be also considered as prompting methods, which are called deep prompting tuning in [277]. In this survey, prompt tuning specially refer to the methods that only include the prompt tokens at the input layer, in the context of LLMs. We assign p-tuning v2 [277] to the category of prefix tuning, because it incorporates layerwise prompts in langauge models.

32

major merit of LoRA is that it can largely save the memory and storage usage (e.g., VRAM). Further, one can only keep a single large model copy, while maintaining a number of task-specific low-rank decomposition matrices for adapting to different downstream tasks. Further, several studies have also discussed how to set the rank in a more principled approach, e.g., importance score based allocation [282] and search-free optimal rank selection [283].
Besides the above methods, there is extensive research on efficient tuning of Transformer language models. However, a more comprehensive discussion of efficient tuning is beyond the scope of this article, which can be found in the related papers on this topic [276, 284].
5.3.2 Parameter-Efficient Fine-Tuning on LLMs
With the rising of LLMs, efficient tuning has attracted increasing research attention for developing a more lightweight adaptation approach in downstream tasks.
In particular, LoRA [127] has been widely applied to open-source LLMs (e.g., LLaMA and BLOOM) for parameter-efficient fine-tuning. Among these research attempts, LLaMA and its variants have gained much attention for parameter-efficient tuning. For example, AlpacaLoRA [126] has been trained using LoRA as a lightweight tuned version of Alpaca [124] (a fine-tuned 7B LLaMA model with 52K human demonstrations of instruction following). There are extensive explorations of Alpaca-LoRA ranging in different languages or model sizes, which can be found in the collection page31. A recent study LLaMAAdapter [285] inserts learnable prompt vectors into each Transformer layer, in which zero-initialized attention has been proposed to improve the training by mitigating the influence of under-fitted prompt vectors. They also extend this approach to a multi-modal setting, e.g., visual question answering.
Further, an empirical study [275] has been conducted to examine the effect of different tuning methods on language models. They compare four efficient tuning methods including serial adapter tuning [274], parallel adapter tuning [276, 286], and LoRA [127], on three open-source LLMs, namely GPT-J (6B), BLOOM (7.1B) and LLaMA (7B), for evaluation. Based on the experimental results on six math reasoning datasets, they show that these efficient-tuning methods under-perform the reference baseline GPT-3.5 on difficult tasks, while achieving a comparable performance on simple tasks. Overall, LoRA performs relatively well among these comparison methods, using significantly fewer trainable parameters.
As an important resource, the library PEFT [287] (standing for parameter-efficient fine-tuning) has been released on GitHub32. It has included several widely used efficient tuning methods, including LoRA [127]/AdaLoRA [282], prefixtuning [272, 277], P-Tuning [278], and prompt-tuning [273]. Further, it supports a number of language models such as GPT-2 and LLaMA, and also covers several representative vision Transformer models (e.g., ViT and Swin Transformer).
As discussed in Section 5.3.1, there have been a large number of efficient tuning methods proposed in the existing
31. https://github.com/tloen/alpaca-lora 32. https://github.com/huggingface/peft

literature. However, most of these approaches are tested on small-sized pre-trained language models, instead of the LLMs. So far, there still lacks a thorough investigation on the effect of different efficient tuning methods on large-sized language models at different settings or tasks.
5.4 Memory-Efficient Model Adaptation
Due to the huge number of model parameters, LLMs take a significant memory footprint for inference, making it very costly to be deployed in real-world applications. In this section, we discuss how to reduce the memory footprint of LLMs via a popular model compression approach (i.e., model quantization), so that large-sized LLMs can be used in resource-limited settings, which also likely reduces the inference latency.
5.4.1 Background for Quantization
In this part, we present a general introduction of quantization techniques for neural networks.
In neural network compression, quantization often refers to the mapping process from floating-point numbers to integers [288], especially the 8-bit integer quantization (i.e., INT8 quantization). For neural network models, there are typically two kinds of data to be quantized, namely weights (model parameters) and activations (hidden activations), which are originally represented in floating-point numbers. To illustrate the essential idea of model quantization, we introduce a simple yet popular quantization function: xq = R(x/S)‚àíZ, which transforms a floating number x into a quantized value xq. In this function, S and Z denote the scaling factor (involving two parameters Œ± and Œ≤ that determine the clipping range) and zero-point factor (determining symmetric or asymmetric quantization), respectively, and R(¬∑) denotes the rounding operation that maps a scaled floating value to an approximate integer.
As the reverse process, dequantization recovers the original value from the quantized value accordingly: xÀú = S ¬∑ (xq + Z). The quantization error is calculated as the numerical difference between the original value x and the recovered value xÀú. The range parameters Œ± and Œ≤ have a large impact on the quantization performance, which often need to be calibrated according to real data distributions, in either a static (offline) or dynamic way (runtime).
For more details, we refer to the readers to the excellent survey [288] about quantization methods on neural networks.
5.4.2 Quantization Methods for LLMs
There are generally two major model quantization approaches, namely quantization-aware training (QAT) (requiring additional full model retraining) and post-training quantization (PTQ) (requires no model retraining). Compared with small-sized language models, two major differences need to be considered when designing or selecting quantization methods for LLMs. Firstly, LLMs consist of a huge number of parameters, and thus PTQ methods are more preferred due to a much lower computational cost than QAT methods. Secondly, LLMs exhibit very different activation patterns (i.e., large outlier features), and it becomes more difficult to quantize LLMs, especially hidden activations. Next, we

will briefly review several representative PTQ methods33 for LLMs.
Post-Training Quantization (PTQ). We first introduce the PTQ methods for LLMs.
‚Ä¢ Mixed-precision decomposition. As observed in [289], extreme large values occur in hidden activations (called the emergence of outliers) when the model size reaches 6.7B parameters or above. Interestingly, these outliers are mainly distributed in some specific feature dimensions at Transformer layers. Based on this finding, a vector-wise quantization approach, called LLM.int8(), has been proposed in [289], which separates the feature dimensions with outliers and the rest dimensions in matrix multiplication. Then, the calculations for the two parts are performed with 16bit floating numbers and 8-bit integers, respectively, so as to recover these outliers in a high precision.
‚Ä¢ Fine-grained quantization. For Transformer models, weights and activations are usually represented in the form of tensors. A straightforward approach is to use coarse-grained quantization parameters for the whole tensor (i.e., per-tensor quantization) [290]. However, it usually leads to inaccurate reconstruction results. Thus, finegrained methods are proposed to reduce the quantization error. ZeroQuant [291] adopts a token-wise quantization approach with dynamic calibration for compressing activations. Whereas for weights (easier to be quantized), it uses a group-wise quantization. In practice, a group size of 128 [291, 292] is commonly used for model quantization.
‚Ä¢ Balancing the quantization difficulty. Considering that weights are easier to be quantized than activations, SmoothQuant [290] proposes to migrate the difficulty from activations to weights. Specially, they incorporate a scaling transformation to balance the difficulty between weights and activations in a linear layer: Y = (Xdiag(s)‚àí1) ¬∑ (diag(s)W). By introducing an mathematically equivalent transformation, this formula controls the quantization difficulty through the scaling factor s. To set s, it incorporates a migration strength parameter Œ± to balance the difficulties, where each entry sj = max(xj)Œ±/ max(wj)(1‚àíŒ±) is determined by the migration strength.
‚Ä¢ Layerwise quantization. This approach finds optimal quantized weights that minimize a layerwise reconstruction loss: arg minW ‚à• WX‚àíWX ‚à•22. To efficiently optimize this objective, GPTQ [293] improves the original optimal brain quantization (OBQ) [294] method by fixing the quantization order of weights for all rows. Further, with specially designed methods (i.e., lazy batch-updates and Cholesky reformulation), GPTQ is feasible to quantize very large models (e.g., 175B OPT) in 3 or 4 bit precision. More recently, AWQ [292] further simplifies the optimization form by incorporating activation-aware scaling for weights, which resembles the idea of SmoothQuant [290]: weights corresponding to outlier activations are more important to be precisely quantized. It does not directly optimize the reconstruction loss, but instead performs simple hyper-parameter search to achieve the minimal loss on calibration data.
33. Since we mainly focus on discussing quantization methods in the context of LLMs, the line of quantization work on small-sized language models (e.g., BERT) has not been included in this survey.

33
These strategies in the above methods can be jointly used to improve the quantization performance. In order to achieve high-efficiency implementation, quantization methods also rely on hardware- or system-level support (e.g., efficient GPU kernels or hardware-friendly group partition).
Other Quantization Methods. In the above, we mainly focus on PTQ methods, and next introduce two recent studies that explore efficient fine-tuning methods or QAT methods for quanitizing LLMs.
‚Ä¢ Efficient fine-tuning enhanced quantization. For posttraining quantization, direct low-bit quantization (e.g., INT4 quantization) often results in large performance degradation. To overcome this challenge, QLoRA [295] incorporates additional small tunable adapters (16-bit precision) into the quantized models, to achieve an efficient, high-precision model fine-tuning. It combines the merits of LoRA (See Section 5.3.1) and quantization methods. The experiment results show that 4-bit quantized models can achieve the full 16-bit fine-tuning performance by QLoRA.
‚Ä¢ Quantization-aware training (QAT) for LLMs. A recent study [296] explores the effect of QAT methods by applying a data-free distillation method to compress the weights, activations as well as key-value cache. By conducting extensive experiments based on LLaMA, they show promising results with 4-bit quantization on both weights and keyvalue cache, but not on 4-bit activation quantization, which still needs more exploration.
5.4.3 Empirical Analysis and Findings
Quantization has currently become a common technique to reduce the memory footprint and latency of LLMs in deployment. In particular, it is important to understand what level of precision (e.g., INT8 or INT4) can be applied to quantize different parts of LLMs (e.g., weights or activations), while retaining a high accuracy.
Recently, a very comprehensive evaluation [297] has been conducted about the impact of multiple factors (e.g., model size and sensitivity) on the post-training quantization methods. Another study [298] examines the scaling law of k-bit quantization in inference performance. Also, prior work (e.g., LLM.int8() [299], GPTQ [293], QLoRA [295], and GLM [83]) has also extensively examined the performance of quantization methods in various settings. Next, we summarize several important findings from these studies, which will be useful for those who may not want to delve into the technical details of quantization methods.
‚Ä¢ INT8 weight quantization can often yield very good results on LLMs, while the performance of lower precision weight quantization depends on specific methods [290, 292, 293, 297]. In most cases, INT8 weight quantization can be effectively applied to reduce the memory footprint without performance degradation. While for INT4 (or INT3) weight quantization, existing methods rely on specific strategies to reduce the performance degradation, e.g., layerwise method [291, 293], activation-aware scaling [292] and low-rank adapter tuning [295]. Interestingly, LLMs seem to be less sensitive to low-bit weight quantization than small-sized language models [297]. In practice, with the same memory cost, it is suggested to use a larger language model with a lower quantization precision rather than a smaller language model

34

with a higher quantization precision. For example, a 4-bit 60GB LLM is demonstrated to have better performance than a 8-bit 30GB LLM [298].
‚Ä¢ Activations are more difficult to be quantized than weights [289, 290, 297]. It has been found that large outliers would occur for Transformer language models having a size of 6.7B or above [289]. This issue has been one of the most fundamental difficulties to quantize LLMs. To overcome this issue, various methods, e.g., mixed-precision decomposition [289], fine-grained quantization [289, 300] and difficulty migration [290], can be applied to alleviate the influence of outlier values. Since large outliers mainly exist in the activations of LLMs, small language models are more resistant to activation quantization [297]. In practice, high-quality INT8 activation quantization is still a difficult task, though several methods can attain satisfying results. Further, lower precision activation quantization has still not been successfully explored, even for QAT methods [296].
‚Ä¢ Efficient fine-tuning enhanced quantization is a good option to enhance the performance of quantized LLMs [127, 295]. The benefits of efficient fune-tuning methods in quantization can be twofold. Firstly, it can directly compensate the performance degradation suffered from low-bit quantization [297], by increasing the fitting capacity by updating high precision adapters. Secondly, it is flexible to support task-specific or goal-specific fine-tuning of LLMs in a lightweight way [295], e.g., instruction tuning or chat-oriented tuning, by only tuning the small adapters. Overall, it makes a good trade-off between the effectiveness and training cost, which provides a promising approach to enhancing the performance of quantized LLMs.
5.4.4 Open-source Libraries and Quantized LLMs
In this part, we briefly introduce the available open-source quantization libraries and quantized LLMs.

implementation. It also supports a number of LLaMA based models, such as Alpaca and Vicuna.
Quantized LLMs. Compared with original models, quantized language models take a smaller memory footprint, and likely have a faster inference speed [83, 289, 302]. Recently, a nubmer of quantized model copies of several publicly available language models have been released on HuggingFace, including BLOOM, GPT-J, and ChatGLM. In particular, GPTQ [293] has been widely used to quantize generative language models, leading to various quantized variants for LLaMA and OPT. Further, it has been also applied to quantize instruction-tuned models, such as Vicuna and WizardLM. Due to the large number of quantized LLMs, we do not directly incorporate the corresponding links of these models. The readers can easily find them by searching on HuggingFace.
6 UTILIZATION
After pre-training or adaptation tuning, a major approach to using LLMs is to design suitable prompting strategies for solving various tasks. A typical prompting method is incontext learning [50, 55], which formulates the task description and/or demonstrations in the form of natural language text. In addition, chain-of-thought prompting [33] can be employed to enhance in-context learning by involving a series of intermediate reasoning steps in prompts. Furthermore, planning [303] is proposed for solving complex tasks, which first breaks them down into smaller sub-tasks and then generates a plan of action to solve these sub-tasks one by one. Next, we will elaborate on the details of the three techniques.

Quantization Libraries. Next, we introduce three major quantization libraries for LLMs, including:
‚Ä¢ Bitsandbytes34 is developed based on the methods introduced in the papers of LLM.int8() [289] and 8-bit optimizers [301]. It focuses on INT8 quantization for LLMs, which mainly provides the support on 8-bit matrix multiplication and 8-bit optimizer.
‚Ä¢ GPTQ-for-LLaMA35 is developed specially for quantizing LLaMA models. It enables 4-bit quantization of LLaMA models of varied sizes based on the GPTQ algorithm [293]. Also, it provides a comparison with bitsandbytes in both memory and performance (PPL) on the project website.
‚Ä¢ AutoGPTQ36 is a quantization package developed based on the GPTQ algorithm [293], which supports INT4 quantization for LLMs. It includes a number of quantized models in the library, and supports LoRA by integrating with HuggingFace PEFT library.
‚Ä¢ llama.cpp37 makes it feasible to run quantized LLaMA models on a MacBook device. It supports INT4, INT5 and INT8 quantization, which is developed in efficient C/C++
34. https://github.com/TimDettmers/bitsandbytes 35. https://github.com/qwopqwop200/GPTQ-for-LLaMa 36. https://github.com/PanQiWei/AutoGPTQ 37. https://github.com/ggerganov/llama.cpp

6.1 In-Context Learning
As a special prompting form, in-context learning (ICL) is first proposed along with GPT-3 [55], which has become a typical approach to utilizing LLMs.
6.1.1 Prompting Formulation
As stated in [55], ICL uses a formatted natural language prompt, consisting of the task description and/or a few task examples as demonstrations. Figure 11 presents the illustration of ICL. First, starting with a task description, a few examples are selected from the task dataset as demonstrations. Then, they are combined in a specific order to form natural language prompts with specially designed templates. Finally, the test instance is appended to the demonstration as the input for LLMs to generate the output. Based on task demonstrations, LLMs can recognize and perform a new task without explicit gradient update.
Formally, let Dk = {f (x1, y1), . . . , f (xk, yk)} represent a set of demonstrations with k examples, where f (xk, yk) is the prompt function that transforms the k-th task example into natural language prompts. Given the task description I, demonstration Dk, and a new input query xk+1, the

In-Context Learning Answer the following mathematical reasoning questions:

Q: If you have 12 candies and you give 4 candies to your friend,
how many candies do you have left?

N x A: The answer is 8.

Q:

If a rectangle has a length of 6 cm and a width of 3 cm, what is the perimeter of the rectangle?

A: The answer is 18 cm.

Q: Sam has 12 marbles. He gives 1/4 of them to his sister. How many marbles does Sam have left?

35
Chain-of-Thought Prompting Answer the following mathematical reasoning questions:
Q: If a rectangle has a length of 6 cm and a width of 3 cm,
what is the perimeter of the rectangle?
N x A: For a rectangle, add up the length and width and double it.
So, the perimeter of this rectangle is (6 + 3) x 2 = 18 cm. The answer is 18 cm.
Q: Sam has 12 marbles. He gives 1/4 of them to his sister. How many marbles does Sam have left?

A: The answer is 9.

LLM

A: He gives (1 / 4) x 12 = 3 marbles. So Sam is left with 12 ‚Äì 3 = 9 marbles.
The answer is 9.

: Task description

: Demonstration

: Chain-of-Thought

: Query

Fig. 11: A comparative illustration of in-context learning (ICL) and chain-of-thought (CoT) prompting. ICL prompts LLMs with a natural language description, several demonstrations, and a test query, while CoT prompting involves a series of intermediate reasoning steps in prompts.

prediction of the output yÀÜk+1 generated from LLMs can be formulated as follows38:

LLM I, f (x1, y1), . . . , f (xk, yk), f (xk+1, ) ‚Üí yÀÜk+1.

demonstrations

input answer

(6)

where the actual answer yk+1 is left as a blank to be

predicted by the LLM. Since the performance of ICL heavily

relies on demonstrations, it is important to properly design

them in the prompts. According to the construction process

in Equation (6), we focus on three major aspects of for-

matting demonstrations in the prompts, including how to

select examples that make up demonstrations, format each

example into the prompt with the function f (¬∑), and arrange

demonstrations in a reasonable order.

A comprehensive review of ICL has been presented in

the survey paper [50], and we suggest the readers refer-

ring to it for a more general, detailed discussion on this

topic. Compared with this survey, we specially focus on the

discussion of applying ICL to LLMs in two major aspects,

i.e., demonstration design and the underlying mechanism

of ICL. Also, ICL has a close connection with instruction

tuning (discussed in Section 5.1) in that both utilize nat-

ural language to format the task or instances. However,

instruction tuning needs to fine-tune LLMs for adaptation,

while ICL only prompts LLMs for utilization. Furthermore,

instruction tuning can enhance the ICL ability of LLMs to

perform target tasks, especially in the zero-shot setting (only

using task descriptions) [64].

6.1.2 Demonstration Design
Several studies have shown that the effectiveness of ICL is
38. When ICL was introduced in the GPT-3‚Äôs paper [55], it was originally defined to be a combination of the task description and demonstration examples, wherein either component is dispensable. Following this definition, when a LLM is required to solve an unseen task by using only task descriptions, it can be also considered to perform ICL for task solving, whereas the ICL ability can be enhanced by instruction tuning.

highly affected by the design of demonstrations [304‚Äì306] Following the discussion in Section 6.1.1, we will introduce the demonstration design of ICL from three major aspects, i.e., demonstration selection, format, and order.
Demonstration Selection. The performance of ICL tends to have a large variance with different demonstration examples [307], so it is important to select a subset of examples that can effectively leverage the ICL capability of LLMs. There are two main demonstration selection approaches, namely heuristic and LLM-based approaches:
‚Ä¢ Heuristic approaches. Due to their simplicity and low costs, existing work widely adopts heuristic methods to select demonstrations. Several studies employ a k-NN based retriever to select examples that are semantically relevant to the query [307, 308]. However, they perform the selection individually for each example, rather than evaluating the example set as a whole. To resolve this issue, diversitybased selection strategies are proposed to choose the most representative set of examples for specific tasks [309, 310]. Furthermore, in [311], both relevance and diversity are taken into consideration when selecting demonstrations.
‚Ä¢ LLM-based approaches. Another line of work selects demonstrations by making use of LLMs. For example, LLMs can be utilized to directly measure the informativeness of each example according to the performance gain after adding the example [312]. In addition, EPR [313] proposes a two-stage retrieval approach that first recalls similar examples with an unsupervised method (e.g., BM25) and then ranks them using a dense retriever (trained with positive and negative examples labeled by LLMs). As an alternative approach, the task of demonstration selection can be formulated into a RL problem, where LLMs serve as the reward function to provide feedback for training the policy model [314]. Since LLMs perform well for text annotation [315], some recent studies employ LLM itself as the demonstration generator without human intervention [316].
To summarize, as discussed in [317], the selected demon-

36

stration examples in ICL should contain sufficient information about the task to solve as well as be relevant to the test query, for the above two selection approaches.
Demonstration Format. After selecting task examples, the next step is to integrate and format them into a natural language prompt for LLMs. A straightforward method is to instantiate a pre-defined template with the corresponding input-output pairs [36]. To construct more informative templates, recent studies consider adding task descriptions [64] or enhancing the reasoning capability of LLMs with chainof-thought prompts [33]. For instance, in [240], the authors collect a large-scale dataset with task descriptions written by humans. After tuning with this dataset, the performance on seen tasks can be boosted, and LLMs can also generalize to unseen tasks to some extent. To reduce the annotation costs, a semi-automated approach has been proposed in [125] by employing a seed set consisting of human-written task descriptions to guide LLMs to generate task descriptions for new tasks. Since it is costly to manually annotate demonstration formats for different tasks, some work also studies how to automatically generate high-quality ones. As two representative methods, Auto-CoT [318] leverages LLMs with the zero-shot prompt ‚ÄúLet‚Äôs think step by step‚Äù for generating intermediate reasoning steps, while least-tomost prompting [303] first queries LLMs to perform problem decomposition and then utilizes LLMs to sequentially solve sub-problems based on the intermediate answers to previously solved ones.
Demonstration Order. LLMs are shown to sometimes suffer from the recency bias, i.e., they are prone to repeat answers that are near the end of demonstrations [306]. Thus, it is important to arrange demonstrations (i.e., task examples) in a reasonable order. Early work proposes several heuristic methods to quickly find a good order. For example, demonstrations can be directly organized according to their similarity to the query in the embedding space [307]: the more similar, the closer to the end. In addition, global and local entropy metrics can be used to score different demonstration orders [305]. To integrate more task information, some recent studies propose to minimize the code length required to compress and transmit task labels, which is inspired by information theory [319]. However, these methods need additional labeled data as the validation set to evaluate the performance of specific demonstration orders. To eliminate this need, the authors in [305] propose to sample the validation data from the LLM itself.
6.1.3 Underlying Mechanism
After pre-training, LLMs can exhibit intriguing ICL capability without being updated. In what follows, we discuss two key questions about the ICL ability of LLMs, i.e., ‚Äúhow does pre-training affect the ICL ability‚Äù and ‚Äúhow do LLMs perform ICL during inference‚Äù.
How Pre-Training Affects ICL? ICL is first proposed in GPT-3 [55], and it has been shown that the ICL ability becomes more significant with a larger model size. Further, some studies reveal that small-scale PLMs can also demonstrate a strong ICL ability by continual pre-training [320] or fine-tuning [321] on specially designed training tasks,

which typically involve additional task examples in the input during the training process. It suggests that the design of training tasks is an important influence factor on the ICL capability of LLMs. Besides training tasks, recent studies have also investigated the relationship between ICL and pre-training corpora [317, 322]. For example, ICL can be theoretically explained as the product of pre-training on documents that exhibit long-range coherence [317]. Further, another study [322] theoretically analyzes that when scaling parameters and data, LLMs based on next-word prediction can emerge the ability of ICL by learning from the compositional structure (e.g., how words and phrases are combined to form larger linguistic units like sentences) present in language data.
How LLMs Perform ICL? At the inference stage, researchers focus on analyzing how the ICL capability operates based on given demonstrations since no explicit learning or updating is involved. According to the discussion in [323], there are two main ways for LLMs to utilize demonstrations: task recognition and task learning.
‚Ä¢ Task recognition. In the first way, LLMs recognize the task from demonstrations and utilize the prior knowledge obtained from pre-training to solve new test tasks. A Probably Approximately Correct (PAC) framework [324] has been proposed to assess the learnability of ICL. It assumes that there exists a latent variable representing the task in the pretraining data, and LLMs have been shown to be capable of capturing this variable from demonstrations, enabling them to recognize the task in ICL. Also, the interpretation of ICL as task recognition is supported by several empirical studies [304, 325]. For example, it has been observed that replacing the inputs or labels of demonstrations with random ones sampled from the input or label space does not seriously hurt the performance of LLMs, indicating that LLMs mainly recognize the target task from demonstrations instead of learning from them [304, 323]. Similarly, LLMs can exhibit decent performance even if the prompt template is irrelevant or misleading [325].
‚Ä¢ Task learning. In the second way, LLMs learn new tasks unseen in the pre-training stage only through demonstrations. Specially, task learning is analyzed mainly from the perspective of gradient descent and considered as implicit fine-tuning [60, 326]. Then, ICL can be explained as follows: by means of forward computation, LLMs generate metagradients with respect to demonstrations and implicitly perform gradient descent via the attention mechanism. Experiments also show that certain attention heads in LLMs are capable of performing task-agnostic atomic operations (e.g., copying and prefix matching), which are closely related to the ICL ability [327]. Furthermore, some studies abstract ICL as an algorithm learning process [328]. For example, the authors in [328] find that LLMs essentially encode implicit models through their parameters during pre-training. With the examples provided in ICL, LLMs can implement learning algorithms such as gradient descent or directly compute the closed-form solution to update these models during forward computation. Under this explanation framework, it has been shown that LLMs can effectively learn simple linear functions and even some complex functions like decision trees with ICL [328].

37

As discussed in a recent study [323], LLMs exhibit the abilities of both task recognition and task learning in ICL, but the two abilities seem to be possessed with different model scales. As shown in the experiments [323], the ability of task recognition is easier to obtain, and even a small LM with only 350M parameters can exhibit this ability, while task learning can only emerge for LLMs with at least 66B parameters. Another study [329] also supports this finding with specially designed experiments. They set up the tasks with flipped and semantically unrelated labels in the experiment, which require task learning when performing ICL. The results suggest that small LMs tend to disregard the labels and mainly depend on their prior knowledge to accomplish the task, while LLMs have the ability to surpass their prior knowledge and acquire new knowledge from demonstrations, resulting in better outcomes. Furthermore, to improve the task learning ability, Meta-In-Context Learning [330] proposes to include multiple related tasks instead of just a single one in the prompt. In addition, Symbol Tuning [331] fine-tunes LLMs on demonstrations with semantically unrelated labels (e.g., foo/bar instead of positive/negative for sentiment analysis), forcing LLMs to learn the task from demonstrations instead of relying on prior knowledge.
6.2 Chain-of-Thought Prompting
Chain-of-Thought (CoT) [33] is an improved prompting strategy to boost the performance of LLMs on complex reasoning tasks, such as arithmetic reasoning [332], commonsense reasoning [333], and symbolic reasoning [33]. Instead of simply constructing the prompts with input-output pairs as in ICL, CoT incorporates intermediate reasoning steps that can lead to the final output into the prompts. In the following, we will elaborate on the usage of CoT with ICL and discuss when and why CoT prompting works.
6.2.1 In-context Learning with CoT
Typically, CoT can be used with ICL in two major settings, namely the few-shot and zero-shot settings, as introduced below.
Few-shot CoT. Few-shot CoT is a special case of ICL, which augments each demonstration ‚ü®input, output‚ü© as ‚ü®input, CoT, output‚ü© by incorporating the CoT reasoning steps. To apply this strategy, we next discuss two key issues, i.e., how to design appropriate CoT prompts and how to utilize the generated CoTs for deriving the final answer.
‚Ä¢ CoT prompt design. It is critical to design appropriate CoT prompts for effectively eliciting the complex reasoning abilities of LLMs. As a direct approach, it is shown that using diverse CoTs (i.e., multiple reasoning paths for each problem) can effectively enhance their performance [334]. Another intuitive idea is that prompts with more complex reasoning paths are more likely to elicit the reasoning ability of LLMs [335], which can result in higher accuracy in generating correct answers. However, all these approaches rely on annotated CoT datasets, which limits their use in practice. To overcome this limitation, Auto-CoT [318] proposes to utilize Zero-shot-CoT [336] (detailed in the following part ‚ÄúZero-shot CoT‚Äù) to generate CoT reasoning paths by specially prompting LLMs, thus eliminating manual efforts. In

order to boost the performance, Auto-CoT further divides the questions in the training set into different clusters and then chooses the questions that are closest to the centroid of each cluster, which is supposed to well represent the questions in the training set. Although few-shot CoT can be considered as a special prompt case of ICL, the ordering of demonstrations seems to have a relatively small impact compared to the standard prompt in ICL: reordering the demonstrations only results in a performance variation of less than 2% in most tasks [33].
‚Ä¢ Enhanced CoT strategies. In addition to enriching the contextual information, CoT prompting also provides more options to infer the answer given a question. Existing studies mainly focus on generating multiple reasoning paths, and try to find a consensus among the derived answers [337, 338]. For instance, self-consistency [337] is proposed as a new decoding strategy when generating CoT and the final answer. It first generates several reasoning paths and then takes an ensemble over all the answers (e.g., selecting the most consistent answer by voting among these paths). Selfconsistency boosts the performance in CoT reasoning by a large margin, and can even improve some tasks where CoT prompting is usually worse than standard prompting (e.g., closed-book question answering and natural language inference). Further, the authors in [338] expand the selfconsistency strategy to a more general ensemble framework (extending to ensemble on the prompts), and they find that diverse reasoning paths are the key to the performance improvement in CoT reasoning. The above methods can be easily integrated into CoT prompting to enhance the performance without additional training. In contrast, other studies train a scoring model to measure the reliability of the generated reasoning paths [334] or continually train LLMs on the reasoning paths generated by themselves [339] to improve the performance.
Zero-shot CoT. Different from few-shot CoT, zero-shot CoT does not include human-annotated task demonstrations in the prompts. Instead, it directly generates reasoning steps and then employs the generated CoTs to derive the answers. Zero-shot CoT is first proposed in [336], where the LLM is first prompted by ‚ÄúLet‚Äôs think step by step‚Äù to generate reasoning steps and then prompted by ‚ÄúTherefore, the answer is‚Äù to derive the final answer. They find that such a strategy drastically boosts the performance when the model scale exceeds a certain size, but is not effective with small-scale models, showing a significant pattern of emergent abilities. In order to unlock the CoT ability on more tasks, Flan-T5 and Flan-PaLM [64] further perform instruction tuning on CoT annotations and the zero-shot performance on unseen tasks has been improved.
6.2.2 Further Discussion on CoT
In this part, we present discussions regarding two fundamental questions related to CoT, i.e., ‚Äúwhen does CoT work for LLMs‚Äù and ‚Äúwhy can LLMs perform CoT reasoning‚Äù.
When CoT works for LLMs? Since CoT is an emergent ability [31], it only has a positive effect on sufficiently large models (typically containing 10B or more parameters [33]) but not on small models. Moreover, since CoT augments the standard prompting with intermediate reasoning steps,

38

it is mainly effective for the tasks that require step-bystep reasoning [33], e.g., arithmetic reasoning, commonsense reasoning, and symbolic reasoning. Whereas, for other tasks that do not rely on complex reasoning, CoT might lead to worse performance than standard prompting [338], e.g., MNLI-m/mm, SST-2, and QQP from GLUE [193]. Interestingly, it seems that the performance gain brought by CoT prompting could be significant only when standard prompting yields poor results [33].
Why LLMs Can Perform CoT Reasoning? As the second question, we discuss the underlying mechanism of CoT in the following two aspects.
‚Ä¢ The source of CoT ability. Regarding the source of CoT capability, it is widely hypothesized that it can be attributed to training on code since models trained on it show a strong reasoning ability [47, 340]. Intuitively, code data is well organized with algorithmic logic and programming flow, which may be useful to improve the reasoning performance of LLMs. However, this hypothesis still lacks publicly reported evidence of ablation experiments (with and without training on code). In addition, instruction tuning seems not to be the key reason to obtain the CoT ability, since it has been empirically shown that instruction tuning on non-CoT data does not improve the performance on held-out CoT benchmarks [64].
‚Ä¢ The effect of prompting components. The major distinction between CoT prompting and standard prompting is the incorporation of reasoning paths prior to the final answer. Thus, some researchers investigate the effects of different components in the reasoning paths. Specifically, a recent study identifies three key components in CoT prompting, namely symbols (e.g., numerical quantities in arithmetic reasoning), patterns (e.g., equations in arithmetic reasoning), and text (i.e., the rest of tokens that are not symbols or patterns) [341]. It is shown that the latter two parts (i.e., patterns and text) are essential to the model performance, and removing either one would lead to a significant performance drop. However, the correctness of symbols and patterns does not seem critical. Further, there exists a symbiotic relationship between text and patterns: the text helps LLMs to generate useful patterns, and patterns aid LLMs to understand tasks and generate texts that help solve them [341].
In summary, CoT prompting provides a general yet flexible approach to eliciting the reasoning ability of LLMs. There are also some preliminary attempts to extend this technique to solve multimodal [342] and multilingual tasks [343].
6.3 Planning for Complex Task Solving
Prompting with ICL and CoT is a conceptually simple yet general approach to solving various tasks. However, this approach struggles with complex tasks like mathematical reasoning [344] and multi-hop question answering [345]. As an enhanced approach, prompt-based planning has been proposed to break down complex tasks into smaller subtasks and generate a plan of actions to accomplish the task.
6.3.1 The Overall Framework
In this part, we first formulate the general planning paradigm of LLMs for solving complex tasks.

Planning

Task

Framework

Result

Task Planner (LLM)
Feedback

Plan (generate & refine)
Environment

Plan Executor Action

Internal

External

LLM

Tool

World

(e.g., Code Interpreter) (e.g., Minecraft)

‚Ä¶ Others

Fig. 12: An illustration of the formulation for prompt based planning by LLMs for solving complex tasks.

In this paradigm, there are typically three components: task planner, plan executor, and environment39. Specifically, task planner, which is played by LLMs, aims to generate the whole plan to solve a target task. The plan can be present in different forms, e.g., an action sequence in the form of natural language [303] or an executable program written in programming language [346]. Then, plan executor is responsible for executing the actions in the plan. It can be implemented by models like LLMs for textual tasks [347] or by objects like robots for embodied tasks [348]. Furthermore, environment refers to where the plan executor carries out the actions, which can be set differently according to specific tasks, e.g., the LLM itself [349] or external virtual world like Minecraft [350]. It provides feedback about the execution result of the action to the task planner, either in the form of natural language [351] or from other multimodal signals [352].
For solving a complex task, the task planner first needs to clearly understand the task goal and generate a reasonable plan based on the reasoning of LLMs (See Section 6.3.2). Then, the plan executor acts according to the plan in the environment and the environment will produce feedback for the task planner (See Section 6.3.3). The task planner can further incorporate the feedback obtained from the environment to refine its initial plan and iteratively perform the above process to get better results as the task solution (See Section 6.3.4).
Next, we will introduce the three key steps in planning based task solving.
6.3.2 Plan Generation
Plan generation focuses on directly generating action sequences by prompting LLMs. Based on the format of the generated plans, existing work can be divided into two groups: text-based and code-based approaches.
39. Despite the similarity with RL, our formulation decouples the planning and execution phases, whereas in RL, they are typically interleaved in the agent. This paradigm is defined in a general yet slightly loose way, and it mainly aims to help readers understand the key idea underlying the planning approaches of LLMs.

39

Text-based Approaches. It is straightforward for LLMs to generate plans in the form of natural language. In this approach, LLMs are prompted to generate a sequence of actions for the plan executor to perform and solve the complex task. For example, Plan-and-Solve [347] adds explicit instructions like ‚Äúdevise a plan‚Äù to directly prompt the LLM for planning in a zero-shot manner, while Selfplanning [353] and DECOMP [354] add demonstrations in the prompt to guide the LLM to devise a plan through ICL. Following this way, some work further considers incorporating extra tools or models when planning. For example, ToolFormer [71] first annotates a pre-training corpus with potential API calls using LLMs, and then fine-tunes LLMs on it, so that LLMs can learn when and how to call APIs and incorporate the results returned by APIs during generation. HuggingGPT [355] introduces the models available in HuggingFace and regards LLMs as the controller to select suitable models based on their descriptions and aggregate their results as the final solution.
Code-based Approaches. Although text-based approaches sound intuitive, they cannot guarantee faithful execution of the plan, which may lead to failure even when the plan is sound. To address this issue, code-based approaches have been proposed to generate more verifiable plans in the form of executable code in programming languages, e.g., Python or PDDL. In this way, LLMs are first prompted to generate the program and then utilize a deterministic solver to execute it. For example, Faithful CoT [356] and PAL [346] decompose a reasoning task into two stages: at the first stage, the LLM generates a plan conditioned on the query; at the second stage, a deterministic solver executes the plan to derive the final answer. Furthermore, code-based approaches can be applied to embodied agents in a similar way. For example, PROGPROMPT [348] and LLM+P [357] first utilize LLMs to generate plans in the form of python functions or PDDL files, and then leverage a virtual agent or classical planner to solve the problem according to the code-based plans.
6.3.3 Feedback Acquisition
After executing the generated plan, the environment would produce the feedback signal to the LLM-based task planner, which can be utilized to refine its initial plan for better results. In existing work, there are typically two sources of feedback from the environment, depending on their relationship with the LLM-based task planner: internal (i.e., the LLM itself) and external (e.g., tools or virtual worlds) feedback.
Internal feedback. The LLM itself can be utilized as a feedback provider. One straightforward way is to directly evaluate the quality of the generated plans through prompting. For example, RAP [358] evaluate the likelihood that each candidate plan can lead to task success, while Tree of Thoughts [349] proposes to vote across plans by making comparisons between them. Further, LLMs can provide feedback based on the intermediate results from the plan executor. For example, Reflexion [351] utilizes LLMs to transform sparse result signals (e.g., success or failure) into concrete text-based feedback (e.g., ‚ÄúYou should recommend comedies that the user mentions in the query instead of horror

movies‚Äù) and stores this feedback in long-term memory for future planning.
External feedback. In addition to LLMs, external objects can also provide feedback signals. For example, tools like code interpreters are widely used in programming tasks to provide real-time error messages [351], models like stable diffusion [359] can be used in multimodal tasks to provide visual perception [352], and virtual worlds like Minecraft can provide immersive experiences [350]. Furthermore, some work explores multi-agent collaboration in simulated environments [360], where each agent receives feedback not only from interaction with the environment but also from communication with other agents.
6.3.4 Plan Refinement
With access to feedback from the environment, the task planner can accordingly refine its current plan and iteratively go through the ‚Äúplanning ‚Äì execution ‚Äì refinement‚Äù loop for better results. In this part, we summarizes three major refinement approaches in existing work.
Reasoning. The feedback data from the environment may not be directly suitable to be utilized by LLMs for plan refinement, e.g., containing irrelevant information or taking a non-language form. To solve this, some work adds the explicit reasoning process to extract critical information from feedback [361, 362]. For example, React [361] prompts LLMs with demonstrations to generate reasoning traces over feedback. It has been widely used in autonomous agent projects, such as AutoGPT40, which can automatically reason over the observed feedback to revise the initial plan for solving various user requests. However, these approaches typically fix the order of reasoning and planning. To support flexible switching between the two processes for better performance, ChatCoT [362] further unifies the toolaugmented reasoning process into a multi-turn conversation between the LLM-based task planner and the tool-based environment.
Backtracking. Early methods mainly consider planning forward actions while maintaining the existing plan, thus likely leading to local optimal plans based on a short-term evaluation. To solve this, Tree of Thoughts [349] allows backtracking with search algorithms like breadth-first and depthfirst search to make global planning. It refines the plan step by step by backtracking to the last state in the initial plan and choosing the next unexplored action. Furthermore, some studies [352, 363] utilize feedback signals to revise the entire plan. For example, DEPS [363] selects a better plan according to feedback signals, while TIP [352] adds feedback signals to prompts for the LLM-based planner to revise each step in the initial plan.
Memorization. In order to handle long-horizon tasks, it has become a key approach to aid plan refinement with longterm memory. For example, Reflexion [351] stores the feedback from self-reflection into the memory, so previous feedback can be retrieved for plan refinement. Further, the skill library mechanism [350, 364] is proposed to store successful plans in the library, which can be reused and synthesized
40. https://github.com/Significant-Gravitas/Auto-GPT

40

as complex plans for novel tasks. To implement the longterm memory mechanism, tools like vector databases (e.g., milvus [365]) can be used to encode plans or feedbacks into high-dimensional vectors for efficient storage and retrieval at a large scale.
7 CAPACITY EVALUATION
To examine the effectiveness and superiority of LLMs, a surge of tasks and benchmarks have been proposed for conducting empirical ability evaluation and analysis. In this section, we first introduce three types of basic ability evaluation of LLMs for language generation and understanding, then present several advanced ability evaluations of LLMs with more complicated settings or goals, and finally discuss existing benchmarks and empirical analyses.
7.1 Basic Ability Evaluation
In this part, we mainly focus on three basic types of ability evaluation for LLMs, i.e., language generation, knowledge utilization, and complex reasoning. It is noted that we do not intend to have complete coverage of all the related tasks, but instead only focus on the most widely discussed or studied tasks for LLMs. Next, we introduce these tasks in detail.
7.1.1 Language Generation
According to the task definition, existing tasks about language generation can be roughly categorized into language modeling, conditional text generation, and code synthesis tasks. Note that code synthesis is not a typical NLP task, we include it for discussion because it can be directly solved by a number of LLMs (trained on code data) in a similar generation approach as natural language text.
Language Modeling. As the most fundamental ability of LLMs, language modeling aims to predict the next token based on the previous tokens [15], which mainly focuses on the capacity of basic language understanding and generation. For evaluating such an ability, typical language modeling datasets that existing work uses include Penn Treebank [366], WikiText-103 [367], and the Pile [142], where the metric of perplexity is commonly used for evaluating the model performance under the zero-shot setting. Empirical studies [55, 83] show that LLMs bring substantial performance gains over the previous state-of-the-art methods on these evaluation datasets. To better test the modeling capacity of long-range dependencies in text, the LAMBADA dataset [183] has been introduced, where LLMs are required to predict the last word of sentences based on a paragraph of context. Then, the accuracy and perplexity of the predicted last words are employed to evaluate LLMs. As shown in existing work, the performance on the language modeling tasks typically follows the scaling law [30], which means that scaling language models would improve the accuracy and reduce the perplexity.
Conditional Text Generation. As an important topic in language generation, conditional text generation [48] focuses on generating texts satisfying specific task demands based on the given conditions, typically including machine translation [433], text summarization [376], and question

answering [386]. To measure the quality of the generated text, automatic metrics (e.g., Accuracy, BLEU [434] and ROUGE [435]) and human ratings have been typically used for evaluating the performance. Due to the powerful language generation capabilities, LLMs have achieved remarkable performance on existing datasets and benchmarks. For instance, GPT-4 exhibits comparable performance as commercial translation products, even for the translation task of languages that are with significant linguistic distance [436]. On news summarization tasks (i.e., CNN/DM and XSUM), LLMs also demonstrate comparable performance with human freelance writers [437]. Despite the rapid progress on model capacity, there are increasing concerns on the feasibility of existing automatic metrics to faithfully assess the performance of LLMs in conditional text generation tasks [437‚Äì439]. As the alternatives to automatic metrics, recent studies also propose to incorporate LLMs as generation evaluators to examine the quality of the generated content [120, 440, 441]. Moreover, researchers also explore more challenging language generation tasks for LLMs, such as structured data generation [442] and long text generation [46, 443, 444].
Code Synthesis. In addition to generating high-quality natural language text, existing LLMs also show strong abilities to generate formal language, especially computer programs (i.e., code) that satisfy specific conditions, called code synthesis [445]. Unlike natural language generation, as the generated code can be directly checked by execution with corresponding compilers or interpreters, existing work mostly evaluates the quality of the generated code from LLMs by calculating the pass rate against the test cases, i.e., pass@k41. Recently, several code benchmarks focusing on functional correctness are proposed to assess the code synthesis abilities of LLMs, such as APPS [380], HumanEval [89], and MBPP [164]. Typically, they consist of diverse programming problems, with text specification and test cases for correctness checking. To improve such an ability, it is key to fine-tuning (or pre-training) LLMs on code data, which can effectively adapt LLMs to code synthesis tasks [77]. In addition, existing work has proposed new strategies to generate code, e.g., sampling multiple candidate solutions [164] and planning-guided decoding [446], which can be considered as the imitation of bug-fixing and code-planning processes by programmers. Impressively, LLMs have recently shown competitive performance with humans by achieving a ranking of the top 28% among users on the programming contest platform Codeforces [98]. Further, GitHub Copilot has been released to assist programming in coding IDEs (e.g., Visual Studio and JetBrains IDEs), which can support a variety of languages including Python, JavaScript, and Java. A viewpoint article entitled ‚ÄúThe End of Programming‚Äù [447] in Communications of the ACM has discussed the impact of AI programming in the field of computer science, emphasizing an important shift towards the highly adaptive LLM as a new atomic unit of computation.
Major Issues. Although LLMs have achieved splendid performance in generating human-like text, they are susceptible
41. Given k programs generated by the LLM, pass@k is computed as 1 when at least one program passes all test cases, or else 0

41
TABLE 9: Basic evaluation tasks and corresponding representative datasets of LLMs.

Task

Dataset

Language Modeling

Penn Treebank [366], WikiText-103 [367], the Pile [142], LAMBADA [183]

Language Generation Conditional Text Generation

WMT‚Äô14,16,19,20,21,22 [368‚Äì373], Flores-101 [374], DiaBLa [375], CNN/DailyMail [376], XSum [377], WikiLingua [378], OpenDialKG [379]

Code Synthesis

APPS [380], HumanEval [89], MBPP [164], CodeContest [98], MTPB [77], DS-1000 [381], ODEX [382]

Closed-Book QA

Natural Questions [383], ARC [384], TruthfulQA [385], Web Questions [386], TriviaQA [387], PIQA [388], LC-quad2.0 [389], GrailQA [390], KQApro [391],
CWQ [392], MKQA [393], ScienceQA [394]

Knowledge Utilization

Open-Book QA

Natural Questions [383], OpenBookQA [395], ARC [384], Web Questions [386], TriviaQA [387], MS MARCO [396], QASC [397], SQuAD [398], WikiMovies [399]

Knowledge Completion

WikiFact [400], FB15k-237 [401], Freebase [402], WN18RR [403], WordNet [404], LAMA [405], YAGO3-10 [406], YAGO [407]

Knowledge Reasoning

CSQA [333], StrategyQA [408], HotpotQA [409], ARC [384], BoolQ [410], PIQA [388] SIQA [411], HellaSwag [412], WinoGrande [413], OpenBookQA [395], COPA [414],
ScienceQA [394], proScript [415], ProPara [416], ExplaGraphs [417], ProofWriter [418], EntailmentBank [419], ProOntoQA [420]

Complex Reasoning

Symbolic Reasoning

CoinFlip [33], ReverseList [33], LastLetter [33], Boolean Assignment [421], Parity [421], Colored Object [265], Penguins in a Table [265], Repeat Copy [346], Object Counting [346]

Mathematical Reasoning

MATH [264], GSM8k [422], SVAMP [423], MultiArith [424], ASDiv [332], MathQA [425], AQUA-RAT [426], MAWPS [427], DROP [428], NaturalProofs [429],
PISA [430], miniF2F [431], ProofNet [432]

to suffering from two major issues in language generation as discussed below.
Unreliable Generation Evaluation
LLMs have been capable of generating texts with a comparable quality to human-written texts, which however might be underestimated by automatic reference-based metrics. As an alternative evaluation approach, LLMs can serve as language generation evaluators to evaluate a single text, compare multiple candidates, and improve existing metrics. However, this evaluation approach still needs more inspections and examinations in real-world tasks.
‚Ä¢ Unreliable generation evaluation. With the advancement of language generation ability of LLMs, existing studies find that the generated texts from LLMs have reached a comparable quality to the reference texts on a variety of text generation tasks. However, due to the intrinsic weakness of existing evaluation benchmarks, there exists pronounced inconsistency between human evaluation and automatic reference-based metrics [437‚Äì439, 448]. For example, in OpenDialKG [379], ChatGPT underperforms a fine-tuned GPT-2 on BLEU and ROUGE-L metrics, while earning more favor from human judgment [448]. Furthermore, existing work argues that even human evaluation may not be robust enough [437, 438, 449, 450]. In some cases, it is difficult to achieve a high level of consensus among human annotators [438], and there is also a large gap between the annotation quality of crowdworkers and experts [449, 450]. Thus, how to conduct reliable evaluation for language generation tasks in the era of LLMs has become a fundamental yet challenging research topic. Recently, increasing research

work proposes to leverage LLMs to improve the evaluation quality of the generated texts. Specially, LLMs can be used to improve the evaluation quality of existing metrics. For example, Para-Ref [451] augments various automatic metrics by leveraging LLMs to paraphrase existing references into semantically equivalent references with diverse expressions. Further, LLMs are widely employed as the evaluators of text generation in a reference-free manner, including evaluating a single prediction [440, 441, 452] or comparing several candidates [120, 453‚Äì455]. Nevertheless, LLMs may expose bias (e.g., order bias or preference for LLM-generated texts over human-written texts) as language generation evaluators, demonstrating disparities when compared to human evaluation [441, 456, 457].
‚Ä¢ Underperforming specialized generation. Although LLMs have learned general language patterns to generate coherent text, their proficiency in generation might be constrained when dealing with a specialized domain or task. For instance, a language model that has been trained on general web articles may face challenges when generating a medical report which involves many medical jargon and methods. Intuitively, domain knowledge should be critical for model specialization. However, it is not easy to inject such specialized knowledge into LLMs. As discussed in recent analyses [47, 458], when LLMs are trained to exhibit some specific ability that allows them to excel in some areas, they might struggle in others. Such an issue is related to catastrophic forgetting [459, 460] in training neural networks, which refers to the conflict phenomenon of integrating new and old knowledge. Similar cases also occur in human alignment of LLMs, where ‚Äúalignment tax‚Äù [61] (e.g., a potential loss in the in-context learning ability) has to be paid for aligning to human values and needs. Moreover, due to the limitations of sequence modeling architecture, LLMs still face challenges in the understanding and generation

42

of structured data. Consequently, they often fall behind task-specific models on complex structured data tasks, such as knowledge-base question answering and semantic parsing [442, 461]. Therefore, it is important to develop effective model specialization methods that can flexibly adapt LLMs to various task scenarios, meanwhile retaining the original abilities as possible.
Underperforming Specialized Generation
LLMs may fall short in mastering generation tasks that require domain-specific knowledge or generating structured data. It is non-trivial to inject specialized knowledge into LLMs, meanwhile maintaining the original abilities of LLMs.
7.1.2 Knowledge Utilization
Knowledge utilization is an important ability of intelligent systems to accomplish knowledge-intensive tasks (e.g., commonsense question answering and fact completion) based on supporting factual evidence. Concretely, it requires LLMs to properly utilize the rich factual knowledge from the pretraining corpus or retrieve external data when necessary. In particular, question answering (QA) and knowledge completion have been two commonly used tasks for evaluating this ability. According to the test tasks (question answering or knowledge completion) and evaluation settings (with or without external resources), we categorize existing knowledge utilization tasks into three types, namely closed-book QA, open-book QA42, and knowledge completion.
Closed-Book QA. Closed-book QA tasks [462] test the acquired factual knowledge of LLMs from the pre-training corpus, where LLMs should answer the question only based on the given context without using external resources. For evaluating this ability, there are several datasets that can be leveraged, including Natural Questions [383], Web Questions [386], and TriviaQA [387], where the accuracy metric is widely adopted. Empirical results have revealed that LLMs can perform well in this setting and even match the performance of state-of-the-art open-domain QA systems [56]. Also, the performance of LLMs on closed-book QA tasks shows a scaling law pattern in terms of both model size and data size: scaling the parameters and training tokens can increase the capacity of LLMs and help them learn (or memorize) more knowledge from the pre-training data [56]. Further, under a similar parameter scale, LLMs with more pre-training data relevant to the evaluated tasks would achieve better performance [72]. Also, the closed-book QA setting provides a testbed for probing the accuracy of the factual knowledge encoded by LLMs. However, as shown in existing work [55], LLMs might perform less well on QA tasks relying on fine-grained knowledge, even when it exists in the pre-training data.
42. In this part, open-book QA refers to the QA tasks that require to extract and utilize useful information from external knowledge resources, as the antithesis of closed-book QA (only using the encoded information from pre-training corpus). Note that there is a dataset also named OpenBookQA [395], which follows the settings of open-book QA tasks by extracting and utilizing external science facts.

Open-Book QA. Unlike closed-book QA, in open-book QA tasks, LLMs can extract useful evidence from the external knowledge base or document collections, and then answer the question based on the extracted evidence [463‚Äì466]. Typical open-book QA datasets (e.g., Natural Questions [383], OpenBookQA [395], and SQuAD [398]) have overlap with closed-book QA datasets, but they incorporate external data sources, e.g., Wikipedia. The metrics of accuracy and F1 score are widely used in open-book QA tasks for evaluation. To select relevant knowledge from external resources, LLMs are often paired with a text retriever (or even a search engine), which is trained independently or jointly with LLMs [72, 463, 467]. Also, previous work [468‚Äì470] has indicated that retrievers can assist LLMs in verifying and rectifying the reasoning path. In evaluation, existing studies mainly focus on testing how LLMs utilize the extracted knowledge to answer the question and show that the retrieved evidence can largely improve the accuracy of the generated answers, even enabling a smaller LLM to outperform 10√ó larger ones [463, 467]. Further, open-book QA tasks can be also employed to evaluate the recency of knowledge information. Pre-training or retrieving from outdated knowledge resources may cause LLMs to generate incorrect answers for time-sensitive questions [463].
Knowledge Completion. In knowledge completion tasks, LLMs might be (to some extent) considered as a knowledge base [405], which can be leveraged to complete or predict the missing parts of knowledge units (e.g., knowledge triples). Such tasks can probe and evaluate how much and what kind of knowledge LLMs have learned from the pre-training data. Existing knowledge completion tasks can be roughly divided into knowledge graph completion tasks (e.g., FB15k237 [401] and WN18RR [403]) and fact completion tasks (e.g., WikiFact [400]), which aim to complete the triples from a knowledge graph and incomplete sentences about specific facts, respectively. Empirical studies have revealed that it is difficult for existing LLMs to accomplish knowledge completion tasks related to specific relation types [340]. As shown in the evaluation results on WikiFact, LLMs perform well on several frequent relations that occur in the pre-training data (e.g., currency and author), while not well on rare ones (e.g., discoverer_or_inventor and place_of_birth). Interestingly, under the same evaluation settings (e.g., in-context learning), InstructGPT (i.e., text-davinci-002) outperforms GPT-3 in all subsets of WikiFact.
Major Issues. Although LLMs have achieved key progress in capturing and utilizing knowledge information, they suffer from two major issues as discussed below.
‚Ä¢ Hallucination. In generating factual texts, a challenging issue is hallucination generations [448], where the generated information is either in conflict with the existing source (intrinsic hallucination) or cannot be verified by the available source (extrinsic hallucination), which are illustrated by two examples in Figure 13. Hallucination widely occurs in existing LLMs, even the most superior LLMs such as GPT-4 [46]. Furthermore, existing work shows that LLMs encounter difficulties in recognizing the hallucinated content in text [471], even the powerful ChatGPT. Additionally,

Bob‚Äôs wife is Amy. Bob‚Äôs daughter is Cindy. Who is Cindy to Amy?

43 Explain RLHF for LLMs.

Cindy is Amy‚Äôs daughter-in-law. (a) Intrinsic hallucination

RLHF stands for "Rights, Limitations, Harms, and Freedoms" and is a framework for ‚Ä¶‚Ä¶ models like LLMs (Large Language Models).
(b) Extrinsic hallucination

Fig. 13: Examples of intrinsic and extrinsic hallucination for a public LLM (access date: March 19, 2023). As an example of intrinsic hallucination, the LLM gives a conflicting judgment about the relationship between Cindy and Amy, which contradicts the input. For extrinsic hallucination, in this example, the LLM seems to have an incorrect understanding of the meaning of RLHF (reinforcement learning from human feedback), though it can correctly understand the meaning of LLMs (in this context).

beyond language tasks, a recent study has shown that large vision-language models (LVLM) also face challenges with hallucination, i.e., generating objects that are not present in the accompanying images [472]. In essence, LLMs seem to ‚Äúunconsciously‚Äù utilize the knowledge in task solving, which still lack an ability to accurately control the use of internal or external knowledge. Hallucinations would mislead LLMs to generate undesired outputs and mostly degrade the performance, leading to potential risks when deploying LLMs in real-world applications. To alleviate this problem, alignment tuning strategies (as discussed in Section 5.2) have been widely utilized in existing work [61], which rely on tuning LLMs on high-quality data or using human feedback. Moreover, the integration of external tools for the provision of credible information sources can help alleviate the hallucination issue [72, 469, 471]. Another line of research work leverages uncertainty estimation of LLMs to identify hallucinations [473, 474]. For instance, considering that hallucinated facts are prone to exhibit inconsistency across different sampled outputs, SelfCheckGPT [474] detects hallucination by measuring information inconsistency within sampled outputs. For the evaluation of the hallucination problem, a set of hallucination detection tasks have been proposed, e.g., TruthfulQA [385] for detecting human falsehood mimicked by models. More recently, HaluEval [471] creates a large-scale LLM-generated and human-annotated hallucinated samples to evaluate the ability of language models to recognize hallucination in both task-specific and general scenarios.
Hallucination
LLMs are prone to generate untruthful information that either conflicts with the existing source or cannot be verified by the available source. Even the most powerful LLMs such as ChatGPT face great challenges in migrating the hallucinations the generated texts. This issue can be partially alleviated by special approaches such as alignment tuning and tool utilization.
‚Ä¢ Knowledge recency. As another major challenge, LLMs would encounter difficulties when solving tasks that require

the latest knowledge beyond the training data. To tackle this issue, a straightforward approach is to regularly update LLMs with new data. However, it is very costly to fine-tune LLMs, and also likely to cause the catastrophic forgetting issue when incrementally training LLMs. Therefore, it is necessary to develop efficient and effective approaches that can integrate new knowledge into existing LLMs, making them up-to-date. Existing studies have explored how to utilize the external knowledge source (e.g., search engine) to complement LLMs, which can be either jointly optimized with LLMs [463] or used as a plug-and-play module [469]. For instance, ChatGPT utilizes a retrieval plugin to access up-to-date information sources [475]. By incorporating the extracted relevant information into the context [476‚Äì478], LLMs can acquire new factual knowledge and perform better on relevant tasks. However, such an approach seems to be still at a superficial level. In addition, existing studies also explore editing parameters of language models to update intrinsic knowledge [479‚Äì481]. Nevertheless, previous work [482] has shown that several parameter editing methods perform not well on LLMs, though they can improve the performance of small language models. Therefore, it is still difficult to directly amend intrinsic knowledge or inject specific knowledge into LLMs, which remains an open research problem [479, 480].
Knowledge Recency
The parametric knowledge of LLMs is hard to be updated in a timely manner. Augmenting LLMs with external knowledge sources is a practical approach to tackling the issue. However, how to effectively update knowledge within LLMs remains an open research problem.
7.1.3 Complex Reasoning
Complex reasoning refers to the ability of understanding and utilizing supporting evidence or logic to derive conclusions or make decisions [51, 52]. According to the type of involved logic and evidence in the reasoning process, we consider dividing existing evaluation tasks into three

44

major categories, namely knowledge reasoning, symbolic reasoning, and mathematical reasoning.
Knowledge Reasoning. The knowledge reasoning tasks rely on logical relations and evidence about factual knowledge to answer the given question. Existing work mainly uses specific datasets to evaluate the reasoning capacity of the corresponding type of knowledge, e.g., CSQA [333]/StrategyQA [408] for commonsense knowledge reasoning and ScienceQA [394] for science knowledge reasoning. In addition to the accuracy of the predicted results, existing work [394] has also evaluated the quality of the generated reasoning process, via automatic metrics (e.g., BLEU) or human evaluation. Typically, these tasks require LLMs to perform step-by-step reasoning based on factual knowledge, until reaching the answer to the given question. To elicit the step-by-step reasoning ability, chain-ofthought (CoT) prompting strategy [33] has been proposed for enhancing the complex reasoning capacity of LLMs. As discussed in Section 6.2, CoT involves the intermediate reasoning steps, which can be manually created [33] or automatically generated [483], into the prompts to guide LLMs to perform multi-step reasoning. Such a way largely improves the reasoning performance of LLMs, leading to new state-of-the-art results on several complex knowledge reasoning tasks [33, 56, 345]. Further, after reformulating knowledge reasoning tasks into code generation tasks, researchers have found that the performance of LLMs can be further improved [167], especially with the LLMs pretrained on code. However, due to the complexity of knowledge reasoning tasks, the performance of current LLMs still lags behind human results on tasks such as commonsense reasoning [33, 56, 484]. As a common type of mistakes, LLMs might generate inaccurate intermediate steps, leading to a wrong final result. To address this issue, existing work has proposed special decoding or ensemble strategies to improve the accuracy of the whole reasoning chain [334, 337].
Symbolic Reasoning43. The symbolic reasoning tasks mainly focus on manipulating the symbols in a formal rule setting to fulfill some specific goal [51], where the operations and rules may have never been seen by LLMs during pretraining. Existing work [33, 303, 336] commonly evaluates LLMs on the task of last letter concatenation and coin flip, where the evaluation examples require the same reasoning steps as the in-context examples (called in-domain test) or more steps (called out-of-domain test). For an example of the out-of-domain test, LLMs could only see the examples with two words in context, but it requires LLMs to concatenate the last letters of three or more words. Typically, the accuracy of the generated symbols is adopted to evaluate the performance of LLMs on these tasks. Thus, LLMs need to understand the semantic relations among the symbolic operations and their composition in complex scenarios. However, under the out-of-domain setting, as LLMs have not seen the complex compositions of symbolic operations and rules (e.g., twice the number of operations in context examples), it is hard for LLMs to capture their accurate
43. Following [33], we mainly discuss symbolic reasoning tasks specially designed for evaluating LLMs. We do not consider symbolic reasoning methods in traditional NLP tasks, such as deducing logical rules from the knowledge graphs in KBQA.

meanings. To solve this issue, existing studies incorporate scratchpad [421, 485] and tutor [486] strategies to help LLMs better manipulate symbolic operations, for generating longer and more complex reasoning processes. Another line of research work utilizes the formal programming language to represent the symbolic operations and rules, which requires LLMs to generate code and perform the reasoning process by executing it with external interpreters. Such a way can decompose the complex reasoning process into code synthesis and program execution for LLMs and interpreters, respectively, leading to a simplified reasoning process with yet more accurate results [346].
Mathematical Reasoning. The mathematical reasoning tasks need to comprehensively utilize mathematical knowledge, logic, and computation for solving problems or generating proof statements. Existing mathematical reasoning tasks can be mainly categorized into math problem solving and automated theorem proving. For math problem solving tasks, SVAMP [423], GSM8k [422] and MATH [264] datasets are commonly used for evaluation, where LLMs need to generate accurate concrete numbers or equations to answer the mathematical problem. As these tasks also require multi-step reasoning, the CoT prompting strategy has been widely adopted for LLMs to improve the reasoning performance [33]. As another practical strategy, continually pre-training LLMs on large-scale mathematical corpora can largely boost their performance on mathematical reasoning tasks [35, 159, 487]. Further, since math problems in different languages share the same mathematical logic, researchers also propose a multilingual math word problem benchmark [343] to evaluate the multilingual mathematical reasoning capacity of LLMs. As another challenging task, automated theorem proving (ATP) [429, 431, 488] requires the reasoning model to strictly follow the reasoning logic and mathematical skills. To evaluate the performance on this task, PISA [430] and miniF2F [431] are two typical ATP datasets with the proof success rate as the evaluation metric. As a typical approach, existing work on ATP utilizes LLMs to aid the search for proofs using an interactive theorem prover (ITP), such as Lean, Metamath, and Isabelle [489‚Äì 491]. A major limitation of ATP research is the lack of related corpora in formal language. To tackle it, several studies utilize LLMs to convert informal statements into formal proofs for augmenting new data [492] or generate drafts and proof sketches to reduce the search space of the proofs [493].
Major Issues. In spite of the advancements, LLMs still have several limitations in solving complex reasoning tasks.
‚Ä¢ Reasoning inconsistency. With improved reasoning strategies (e.g., CoT prompting), LLMs can solve some complex reasoning tasks, by performing step-by-step reasoning based on the supporting logic and evidence. Despite the effectiveness, the reasoning inconsistency issue often occurs in the decomposed reasoning process. Concretely, LLMs may generate the correct answer following an invalid reasoning path, or produce a wrong answer after a correct reasoning process [33, 356], leading to inconsistency between the derived answer and the reasoning process. To alleviate this problem, existing work has proposed to guide the whole generation process of LLMs via external tools or

45

models [334, 446, 494], to re-check the reasoning process and final answer for correcting the potential errors [495‚Äì497] or fine-tune LLMs with process-based feedback [498, 499]. For instance, Tree of Thoughts (ToT) [494] empowers LLMs to engage in the decision-making process by concurrently exploring and self-evaluating various reasoning paths. To refine the reasoning processes, Self-Refine [495] elicits feedback from LLMs on self-generated solutions, enabling the iterative refinement of solutions based on the feedback. Moreover, several studies improve the consistency in the reasoning chain of LLMs through the integration of processbased supervision during training [498, 499]. As a promising solution, recent approaches reformulate the complex reasoning tasks into code generation tasks, where the strict execution of the generated code ensures the consistency between the reasoning process and the outcome. Also, it has been revealed that there might exist inconsistency between tasks with similar inputs, where small changes in the task description may cause the model to produce different results [49, 423]. To mitigate this problem, selfconsistency [337] adopts the ensemble of multiple reasoning paths to enhance the decoding process of LLMs.
Reasoning Inconsistency
LLMs may generate the correct answer following an invalid reasoning path, or produce a wrong answer after a correct reasoning process, leading to inconsistency between the derived answer and the reasoning process. The issue can be alleviated by fine-tuning LLMs with process-level feedback, using an ensemble of diverse reasoning paths, and refining the reasoning process with selfreflection or external feedback.
‚Ä¢ Numerical computation. For complex reasoning tasks, LLMs still face difficulties in the involved numerical computation, especially for the symbols that are seldom encountered during pre-training, such as arithmetic with large numbers [49, 486, 500]. To tackle this issue, a direct way is to tune LLMs on synthesized arithmetic problems [259, 501]. Also, a surge of studies improve the numerical computation performance by tracing intermediate calculation steps in training and inference stages [259, 485, 502], e.g., scratchpad tracing. In addition, existing work [71] has also incorporated external tools (e.g., calculator), especially for handling arithmetic operations. More recently, ChatGPT has provided a plugin mechanism to use external tools [475]. In this way, LLMs need to learn how to properly manipulate the tools. For this purpose, researchers have augmented the examples using tools (even the LLM itself) for tuning the LLM [71, 503], or devised instructions and exemplars for in-context learning [346]. In addition to the aid of external tools, recent studies find that tokenizing digits into individual tokens (e.g., LLaMA and Galactica tokenizers) is a useful approach to enhancing the inherent arithmetic ability of LLMs [259, 500]. One possible explanation is that subword tokenization techniques can result in inconsistent sequences when tokenizing numbers. For instance, with a subword tokenizer the integer 7481 may be tokenized as 7 481, while 74815 may be tokenized as 748 15 (the

same numerical substrings with different splits) [259]. As a comparison, digit-based tokenization for numbers can avoid such an inconsistency, thus likely improving the numerical computation ability of LLMs.
Numerical Computation
LLMs face difficulties in numerical computation, especially for the symbols that are seldom encountered during pre-training. In addition to using mathematical tools, tokenizing digits into individual tokens is also an effective design choice for improving the arithmetic ability of LLMs.
7.2 Advanced Ability Evaluation
In addition to the above basic evaluation tasks, LLMs also exhibit some superior abilities that require special considerations for evaluation. In this part, we discuss several representative advanced abilities and the corresponding evaluation approaches, including human alignment, interaction with the external environment, and tool manipulation. Next, we discuss these advanced abilities in detail.
7.2.1 Human Alignment
It is desired that LLMs could well conform to human values and needs, i.e., human alignment, which is a key ability for the broad use of LLMs in real-world applications.
To evaluate this ability, existing studies consider multiple criteria for human alignment, such as helpfulness, honesty, and safety [46, 243, 268]. For helpfulness and honesty, adversarial question answering tasks (e.g., TruthfulQA [385]) can be utilized to examine LLM‚Äôs ability in detecting possible falsehood in the text [46, 72]. Furthermore, harmlessness can be also evaluated by several existing benchmarks, e.g., CrowS-Pairs [504] and Winogender [505]. Despite the automatic evaluation with the above datasets, human evaluation is still a more direct way to effectively test the human alignment ability of LLMs. OpenAI invites many experts in domains related to AI risks to evaluate and improve the behaviors of GPT-4 when encountering risky contents [46]. In addition, for other aspects of human alignment (e.g., truthfulness), several studies propose to use specific instructions and devise annotation rules to guide the annotation process [72]. Empirical studies have revealed that these strategies can greatly improve the human alignment ability of LLMs [243]. For instance, after alignment tuning on data collected through interactions with experts, the incorrect behavior rate of GPT-4 can be largely reduced when it deals with sensitive or disallowed prompts. In addition, highquality pre-training data can reduce the effort required for alignment [46]. For instance, Galactica is potentially more harmless due to the less biased contents in the scientific corpus [35].
7.2.2 Interaction with External Environment
In addition to standard evaluation tasks, LLMs have the ability to receive feedback from the external environment and perform actions according to the behavior instruction, e.g., generating action plans in natural language to manipulate agents [525, 526]. Such an ability is also emergent in

46
TABLE 10: Representative advanced abilities and corresponding representative datasets for evaluating LLMs.

Ability Human Alignment
Interaction with External Environment
Tool Manipulation

Category

Dataset

Honestness

TruthfulQA [385], HaluEval [471]

Helpfulness

HH-RLHF [243]

Harmlessness

HH-RLHF [243], Crows-Pairs [504] WinoGender [505], RealToxicityPrompts [506]

Household

VirtualHome [507], BEHAVIOR [508], ALFRED [509],ALFWorld [510]

Website Environment

WebShop [511], Mind2Web [512]

Open World

MineRL [513], MineDojo [514]

Search Engine

HotpotQA [409], TriviaQA [387], Natural Questions [383]

Code Executor

GSM8k [422], TabMWP [515], Date Understanding [265]

Calculator

GSM8k [422], MATH [264], CARP [516]

Model Interface

GPT4Tools [517], Gorilla [518]

Data Interface

WebQSP [519], MetaQA [520], WTQ [521] WikiSQL [522], TabFact [523], Spider [524]

LLMs that can generate detailed and highly realistic action plans, while smaller models (e.g., GPT-2) tend to generate shorter or meaningless plans [525].
To test this ability, several embodied AI environments and benchmarks can be used for evaluation, described as follows. VirtualHome [507] builds a 3D simulator for household tasks such as cleaning and cooking, in which the agent can execute natural language actions generated by LLMs. ALFRED [509] includes more challenging tasks that require LLMs to accomplish compositional targets. BEHAVIOR [508] focuses on everyday chores in simulation environments and requires LLMs to generate complex solutions, e.g., changing the internal status of objects. Apart from restricted environments such as household tasks, a line of research work investigates the proficiency of LLMbased agents to explore open-world environments, such as Minecraft and the Internet [527, 528]. Voyager [528] introduces an automatic curriculum module that enables LLMs to continuously acquire new skills based on feedback from the environment. GITM [527] focuses on solving various challenges in Minecraft based on LLM, through task decomposition, planning, and invocation of interfaces. Based on the generated action plans or task completions, existing work either adopts the regular metrics (e.g., executability and correctness of the generated action plans) [525] in the benchmark or directly conducts real-world experiments and measures the success rate [529], to evaluate such ability. It has been shown that LLMs are capable in interacting with the external environment and generating accurate action plans [530]. Recently, several improvement methods have been proposed to enhance the interaction ability of LLMs, e.g., designing code-like prompts [348] and providing realworld grounding [529].
In addition, recent work also explores multi-agent collaboration based on LLMs in simulated environments [360, 531, 532]. These studies simulate human social behaviors by instantiating multiple LLM-based agents with observations, planning, and memories in a sandbox environment. In controlled evaluation, the abilities of generative agents to search, plan, and think are evaluated by humans in an interview-like manner. Further, they also conduct descrip-

tive measurements on multiple agents within a simulated environment to examine emergent social behaviors.
7.2.3 Tool Manipulation
When solving complex problems, LLMs can turn to external tools if they determine it is necessary. By encapsulating available tools with API calls, existing work has involved a variety of external tools, e.g., search engine [72], calculator [71], and compiler [346], to enhance the performance of LLMs on several specific tasks. Recently, OpenAI has supported the use of plugins in ChatGPT [475], which can equip LLMs with broader capacities beyond language modeling. For example, the web browser plugin enables ChatGPT to access fresh information. Further, incorporating thirdparty plugins is particularly key for creating a prosperous ecosystem of applications based on LLMs.
To examine the ability of tool manipulation, existing work mostly adopts complex reasoning tasks for evaluation, such as mathematical problem solving (e.g., GSM8k [422] and SVAMP [423]) or knowledge question answering (e.g., TruthfulQA [385]), where the successful utilization of tools is very important for enhancing the required skills that LLMs are incapable in (e.g., numerical calculation). In this way, the evaluated performance on these tasks can reflect the ability of LLMs in tool manipulation. To teach LLMs to utilize tools, existing studies add exemplars using tools in context to elicit LLMs [346], or fine-tune LLMs on simulated data about tool utilization [71, 503]. It has been found that with the help of tools, LLMs become more capable of handling the issues that they are not good at, e.g., equation calculation and answering timely questions [71, 362]. However, as the number of available tools increases, the limited context length of LLMs may pose challenges in describing and demonstrating extensive tool APIs. To address this issue, existing work retrieves the usage of relevant tools, or encoding tool information as tokens within the embedding space [533‚Äì535].
In addition to existing tools developed by humans, LLMs possess the capability to make their own tools for specific tasks autonomously [536]. This enables the models to independently explore and manipulate these self-created

47

tools, thereby expanding their potential for autonomous exploration in solving a wide range of real-world tasks.
Summary. The above three abilities are of great value to the practical performance of LLMs: conforming to human values and preferences (human alignment), acting properly in real-world scenarios (interaction with the external environment), and expanding the ability scope (tool manipulation). In addition to the above three advanced abilities, LLMs might also show other abilities that are specially related to some tasks (e.g., data annotation [315]) or learning mechanisms (e.g., self-improvement [537]). It will be an open direction to discover, measure and evaluate these newly emerging abilities, so as to better utilize and improve LLMs.
7.3 Benchmarks and Empirical Evaluation
In the above, we have discussed the basic and advanced abilities of LLMs, and also introduced the evaluation approaches for different abilities. Next, we will introduce existing evaluation benchmarks and further present our empirical evaluation experiments based on specially selected tasks for different abilities.
7.3.1 Comprehensive Evaluation Benchmarks
Recently, several comprehensive benchmarks [264, 265, 340] have been released for the evaluation of LLMs. In this part, we introduce several widely used benchmarks, i.e., MMLU, BIG-bench, HELM, and a series of human exam benchmarks.
‚Ä¢ MMLU [264] is a versatile benchmark for large-scale evaluation of multi-task knowledge understanding, covering a wide range of knowledge domains from mathematics and computer science to humanities and social sciences. The difficulties of these tasks vary from basic to advanced. As shown in existing work, LLMs mostly outperform small models by a substantial margin on this benchmark [35, 56, 57, 64], which shows the scaling law in model size. More recently, GPT-4 achieves a remarkable record (86.4% in 5shot setting) in MMLU, which is significantly better than the previous state-of-the-art models [46].
‚Ä¢ BIG-bench [265] is a collaborative benchmark intended to probe existing LLMs from various aspects. It comprises 204 tasks that encompass a broad range of topics, including linguistics, childhood development, mathematics, commonsense reasoning, biology, physics, social bias, software development, and so on. By scaling the model size, LLMs can even outperform the average human performance under the few-shot setting on 65% of tasks in BIG-bench [56]. Considering the high evaluation cost of the entire benchmark, a lightweight benchmark BIG-bench-Lite has been proposed, which contains 24 small yet diverse and challenging tasks from BIG-bench. Additionally, the BIG-bench hard (BBH) benchmark [538] has been proposed to concentrate on investigating the currently unsolvable tasks of LLMs by selecting the challenging tasks in which LLMs exhibit inferior performance compared to humans. Since BBH becomes more difficult, small models mostly achieve performance close to random. As a comparison, CoT prompting can elicit the abilities of LLMs to perform step-by-step reasoning for enhancing the performance, even exceeding the average human performance in BBH.

‚Ä¢ HELM [340] is a comprehensive benchmark that currently implements a core set of 16 scenarios and 7 categories of metrics. It is built on top of many prior studies, conducting a holistic evaluation of language models. As shown in the experimental results of HELM, instruction tuning can consistently boost the performance of LLMs in terms of accuracy, robustness, and fairness. Further, for reasoning tasks, the LLMs that have been pre-trained on code corpus show superior performance.
‚Ä¢ Human-level test benchmarks aim to evaluate the comprehensive ability of LLMs with questions designed for testing humans, such as AGIEval [539], MMCU [540], M3KE [541], C-Eval [542] and Xiezhi [543]. These benchmarks encompass a wide range of domains, difficulty levels, and languages to provide a comprehensive evaluation of LLMs‚Äô general capabilities. Compared to publicly available models, models offering API services (e.g., GPT-4, ChatGPT, Claude) demonstrate superior performance compared to publicly available models on these evaluation benchmarks. As the bestperforming model in evaluations, GPT-4 surpasses average human performance in AGIEval [539]. However, it still lags behind the top human performance on these challenging benchmarks. Hence, there remains ample room for further enhancements in the overall abilities of LLMs, particularly for publicly accessible models.
The above benchmarks cover a variety of mainstream evaluation tasks and real-world human exam questions for the evaluation of LLMs. Also, there are several benchmarks that focus on evaluating specific abilities of LLMs, such as TyDiQA [544] for multilingual knowledge utilization and MGSM [343] for multilingual mathematical reasoning. To conduct the evaluation, one can select suitable benchmarks according to specific goals. In addition, there are also several open-source evaluation frameworks for researchers to evaluate LLMs on existing benchmarks or extend new tasks for customized evaluations, such as Language Model Evaluation Harness [545] and OpenAI Evals [46]. However, these benchmarks only focus on a few abilities or consider a simple mixture of multiple tasks that lack a comprehensive test for the basic and advanced abilities of LLMs.
7.3.2 Empirical Ability Evaluation
The above evaluation benchmarks are mainly employed to evaluate the overall abilities of LLMs. In this part, we conduct a fine-grained evaluation on the abilities discussed in Section 7.1 and Section 7.2. For each kind of ability, we select representative tasks and datasets for conducting evaluation experiments to examine the corresponding performance of LLMs. Next, we will first introduce the experimental settings for evaluation.
Evaluation Models. To conduct the evaluation, we consider representative LLMs from open-source models to closedsource API-accessing models as follows:
‚Ä¢ Open-source models. Existing open-source models can be categorized into base models and instruction-tuned models. Base models are only pre-trained on a large general-purpose corpus with the language modeling objective, but without further supervised fine-tuning. In our evaluation, we select three representative base models including LLaMA

48
TABLE 11: Evaluation on the eight abilities of LLMs with specially selected tasks. The shade of the Orange and Blue fonts denote the performance orders of the results in closed-source and open-source models, respectively. This table will be continuously updated by incorporating the results of more models.

Models

LBD‚Üë

Language Generation

Knowledge Utilization

WMT‚Üë

XSum‚Üë

HumanEval‚Üë TriviaQA‚Üë NaturalQ‚Üë WebQ‚Üë

ARC‚Üë

WikiFact‚Üë

ChatGPT Claude Davinci003 Davinci002

55.81 64.47 69.98 58.85

36.44 31.23 37.46 35.11

21.71 22.86 18.19 19.15

79.88 51.22 67.07 56.70

54.54 40.92 51.51 52.11

21.52 13.77 17.76 20.47

17.77 14.57 16.68 18.45

93.69 66.62 88.47 89.23

29.25 34.34 28.29 29.15

Vicuna (7B)

60.12

Alpaca (7B) 60.45

ChatGLM (6B) 33.34

18.06 21.52 16.58

13.59 8.74 13.48

17.07

28.58

9.17

13.41

17.14

3.24

13.42

13.42

4.40

6.64

16.96

3.00

49.75

9.20

55.39

26.95 26.05 16.01

LLaMA (7B) Falcon (7B) Pythia (12B) Pythia (7B)

66.78 66.89 60.49 50.96

13.84 4.05 5.43 3.68

8.77 10.00 8.87 8.23

15.24 10.37 14.63 9.15

34.62 28.74 15.73 10.16

7.92 10.78 1.99 1.77

11.12 8.46 4.72 3.74

4.88 4.08 11.66 11.03

19.78 23.91 20.57 15.75

Models

Knowledge Reasoning OBQA‚Üë HellaSwag‚Üë SocialIQA‚Üë

Symbolic Reasoning Mathematical Reasoning Interaction with Environment

C-Objects‚Üë Penguins‚Üë GSM8k‚Üë MATH‚Üë ALFW‚Üë

WebShop‚Üë

ChatGPT Claude Davinci003 Davinci002

81.20 81.80 74.40 69.80

61.43 54.95 62.65 47.81

73.23 73.23 69.70 57.01

53.20 59.95 64.60 62.55

40.27 47.65 61.07 67.11

78.47 70.81 57.16 49.96

33.78 20.18 17.66 14.28

58.96 32.09 65.67 76.87

45.12/15.60 50.02/30.40 64.08/32.40 29.66/15.20

Vicuna (7B)

30.00

Alpaca (7B) 28.60

ChatGLM (6B) 52.00

26.26 26.03 40.60

36.39 33.52 57.52

44.25

36.24

14.03

3.54

1.49

6.90/1.40

39.35

40.27

4.93

4.16

4.48

0.00/0.00

14.05

14.09

3.41

1.10

0.00

0.00/0.00

LLaMA (7B) Falcon (7B) Pythia (12B) Pythia (7B)

27.00 25.20 25.00 24.40

25.57 25.07 25.15 23.62

33.11 33.01 32.45 32.04

39.95

34.90

10.99

3.12

2.24

0.00/0.00

29.80

24.16

1.67

0.94

7.46

0.00/0.00

32.40

26.17

2.88

1.96

5.22

3.68/0.60

29.05

27.52

1.82

1.46

7.46

10.75/1.80

Models

TfQA‚Üë C-Pairs‚Üë

Human Alignment

WinoGender‚Üë

RTP‚Üì

Tool Manipulation HaluEval‚Üë HotpotQA‚Üë Gorilla-TH‚Üë Gorilla-TF‚Üë

Gorilla-HF‚Üë

ChatGPT Claude Davinci003 Davinci002

69.16 67.93 60.83 53.73

81.40 67.27 99.01 92.44

62.50/72.50/79.17 71.67/55.00/52.50 67.50/68.33/79.17 72.50/70.00/64.17

3.07 3.75 8.81 10.65

66.64 63.75 58.94 59.67

23.80 33.80 34.40 26.00

67.20 22.04 72.58 2.69

44.53 7.74 3.80 1.02

19.36 7.08 6.42 1.00

Vicuna (7B)

57.77

67.24 49.17/49.17/49.17

4.70

43.44

6.20

0.00

0.00

0.33

Alpaca (7B) 46.14

67.37 53.33/51.67/53.33

4.78

44.16

11.60

0.00

0.00

0.11

ChatGLM (6B) 63.53

50.20 47.50/47.50/46.67

2.89

41.82

4.00

0.00

0.00

0.00

LLaMA (7B) 47.86

68.50 54.17/52.50/51.67

5.94

14.18

1.60

0.00

0.00

0.11

Falcon (7B)

53.24

68.70 50.00/50.83/50.00

6.71

37.41

1.00

0.00

0.00

0.00

Pythia (12B) 54.47

65.98 49.17/48.33/49.17

6.59

27.09

0.40

0.00

0.00

0.00

Pythia (7B)

50.92

64.79 51.67/49.17/50.00 13.02

25.84

0.20

0.00

0.00

0.00

(7B) [57], Pythia (7B and 12B) [87], and Falcon (7B) [554]44. Instruction-tuned models are those fine-tuned using instructions (i.e., task datasets, daily chat, or synthetic instructions). In our experiments, we select three representative instruction-tuned models including Vicuna (7B) [120] Alpaca (7B) [119], and ChatGLM (6B) [83].
‚Ä¢ Closed-source models. In addition to the open-source models, there are also closed-source models that can only be accessed via APIs, which have gained much attention from both developers and researchers. Here, we select four representative closed-source models including text-davinci002/003 (short as Davinci002/003), ChatGPT, and Claude,
44. Experiments with larger models are still in schedule due to the limit of computational resources.

where the first three models are developed by OpenAI and the last one is developed by Anthropic.
Tasks and Datasets. Next, we set up the evaluation tasks and datasets for the abilities discussed in Section 7.1 and Section 7.2. We mainly evaluate the zero-shot performance of LLMs on these datasets. For more complex tasks that are hard to be solved in the zero-shot manner (e.g., mathematical reasoning and tool manipulation), we mainly report the 3-shot performance, considering the context length limit of open-source models.
‚Ä¢ Language generation. As discussed before, for language generation, we consider evaluating three kinds of tasks, i.e., language modeling, conditional text generation, and code synthesis. Specially, we select four commonly-used

49
TABLE 12: Prompt examples and their performance of ChatGPT on representative tasks. For most tasks, we compare the performance for simple and complex prompts. We also present the reported performance of supervised methods. ‚ÄúLG‚Äù, ‚ÄúKU‚Äù, ‚ÄúCR‚Äù, ‚ÄúSDG‚Äù, ‚ÄúIR‚Äù are short for ‚Äúlanguage generation‚Äù, ‚Äúknowledge utilization‚Äù, ‚Äúcomplex reasoning‚Äù, ‚Äústructured data generation‚Äù, ‚Äúinformation retrieval‚Äù. ‚Äú-‚Äù means there is no reported supervised result previously on this dataset.

LG KU
CR SDG
IR

Tasks Translation

Datasets WMT

Summarization

XSum

Closed-Book QA

ARC

Open-Book QA

OBQA

Fact Extraction

WikiF

Symbolic Reasoning C-Objects

Math Word Problems GSM8k

Code Synthesis HumanEval

Text-to-SQL

Spider

Recommendation MovieLens

Conversational Recommendation

ReDial

Instructions

I want you to act as a translator. Please translate the English sentence into Czech.

I want you to act as a translator. Translate the given English
sentence into Czech, and ensure that the translated sentence is semantically consistent with the given sentence. \n Sentence: {source sentence} \n Translation:

Please generate a one-sentence summary for the given document.

{document} Try your best to summarize the main content of the given document. And generate a short summary in 1 sentence for it.\n Summary:

Choose your answer to the question. {query} {options}

Choose a correct answer according to the given question, and output the corresponding id, do not answer other content except the answer id.

Choose your answer to the question: {question} {choices}. You must only output A, B, C, or D without any extra explanation. The answer is

Following is a question that requires multi-step reasoning, use of additional common and commonsense knowledge, and rich text comprehension. Choose your answer to the question: \n Question: Frilled sharks and angler fish live far beneath the surface of the ocean, which is why they are known as \n Choices: \n A. Deep sea animals \n B. fish \n C. Long Sea Fish \n D. Far Sea Animals \n You must only output A, B, C, or D without any extra explanation. The answer is

Complete the sentence with one or a few words.

Complete the given sentence with one entity name in Wikipedia (MUST be a noun) as short as possible, and ensure that the completed sentence conforms to the facts.

Problem: {problem}\n Answer:

You are an expert in reasoning problem. Here are some examples about symbolic reasoning. You can use the knowledge in examples and solve the last problem. You should follow the examples and generate the final answer without external solution or words.

Problem: {problem}\n Solution: Let‚Äôs think step by step.

Let‚Äôs use python to solve math problems. Here are three examples

how to do it,\n Q: Olivia has $23. She bought five bagels for $3

each. How much money does she have left?\n‚Äò‚Äò‚Äòdef solution():\n

"""Olivia has $23. She bought five bagels for $3 each. How

much money does she have left?"""\n

money_initial = 23\n

bagels = 5\n bagel_cost\n

bagel_cost = 3\n

money_spent = bagels *

money_left = money_initial - money_spent\n

result = money_left\n

return result‚Äò‚Äò‚Äò\n ...... \n How about

this question?\n Q:

I want you act as a code completer. Given a code snippet, your objective is to complete the code and ensure that it can achieve the described functionality.

### Complete sqlite SQL query only and with no explanation.\n #\n### Sqlite SQL tables, with their properties: \n#\n{table}\n# {foreign_key}\n#\n### {question}\n SELECT

I‚Äôve watched the following movies in the past in order: \n {user_his_text} \n\n Now there are {recall_budget} candidate movies that I can watch next: \n {candidate_text_order} \n Please rank these {recall_budget} movies by measuring the possibilities that I would like to watch next most, according to my watching history. Please think step by step. \n Note that my most recently watched movie is {recent_item}. Please show me your ranking results with order numbers. Split your output with line break. You MUST rank the
given candidate movies. You can not generate movies that are not in
the given candidate list.

Recommend 10 items that are consistent with user preference. The recommendation list can contain items that the dialog mentioned before. The format of the recommendation list is: no. title (year). Don‚Äôt mention anything other than the title of items in your recommendation list

ChatGPT 20.66
21.12

Supervised 41.40 [546]

21.71 23.01

42.08 [547]

85.19 85.86

92.00 [548]

81.20

82.20

87.20 [548]

29.25 31.21

34.20 [340]

53.20 ‚Äî
66.75

78.47 79.30

63.20 [549]

79.88 48.20 [550] 70.10 84.10 [551] 48.80 76.25 [552]

17.20 25.60 [553]

50

datasets, namely LAMBADA [183] (language modeling), WMT‚Äô22 [373] (machine translation), XSum [377] (text summarization), and HumanEval [89] (code synthesis) for evaluation. In WMT‚Äô22, we construct a new evaluation set by selecting 1000 examples for each language pair from the original large-scale test set to examine the average performance of LLMs in machine translation. We evaluate the zero-shot performance of LLMs on these datasets, and compute the accuracy of predicting words for LAMBADA, BLEU-4 for WMT‚Äô22, ROUGE-L for XSum, and pass@10 for HumanEval.
‚Ä¢ Knowledge utilization. To evaluate the ability of knowledge utilization, we select four question answering datasets (i.e., TriviaQA [387], Natural Questions [383], Web Questions [386], and ARC [384]), and a fact extraction dataset, WikiFact [400]. We also report the zero-shot performance of LLMs on these datasets, and compute accuracy for ARC and exact match for other datasets.
‚Ä¢ Complex reasoning. For complex reasoning, we evaluate the comparison models on OpenbookQA [395], HellaSwag [412], and SocialIQA [411] for knowledge reasoning; Colored Objects [265] and Penguins in the Table [265] for symbolic reasoning; GSM8k [422] and MATH [264] for mathematical reasoning. We compute the accuracy for OpenbookQA, HellaSwag, and SocialIQA; solve rate for Colored Objects and Penguins in the Table; and accuracy for GSM8k and MATH. For knowledge reasoning tasks, we evaluate the zero-shot performance, since they are all QA tasks that can be solved in a zero-shot setting. For complex symbolic reasoning and mathematical reasoning tasks, we leverage 3-shot in-context exemplars to better elicit LLMs to accomplish them. Following existing work [33, 346], we also utilize the chain-of-thought prompting strategy for better solving the mathematical reasoning tasks.
‚Ä¢ Human alignment. For human alignment, we select TruthfulQA [385] to measure whether a LLM is truthful in generating answers to questions, CrowS-Pairs [504] and WinoGender [505] to assess the stereotypes in LLMs, RealToxityPrompts [506] to evaluate the extent to which LLMs generate toxic language, and HaluEval [471] to test the ability of LLMs to recognize hallucination. As the test set of Real-Toxicity-Prompts is too large, we randomly sample 10000 examples from it for evaluation. We follow LLaMA [57] to report the zero-shot performance, and compute the accuracy of identifying a claim as true for TruthfulQA, accuracy of recognizing biased sentences (high perplexity) for CrowS-Pairs, coreference resolution accuracy (he/she/they) for WinoGender, toxicity score for RealToxityPrompts, and average accuracy of recognizing hallucinations for HaluEval. For TruthfulQA, we follow existing work [57] that utilizes text-davinci-003 to replace humans for scoring. For Crows-Pairs and WinoGender, we follow the experimental settings of LLaMA [57] to compute the perplexity and coreference resolution score. For RealToxityPrompts, we utilize the Perspective-API45 for toxicity evaluation.
‚Ä¢ Interaction with environment. To test this ability, we select ALFWorld [510] and WebShop [511] for evaluation, which simulate real-world scenarios such as household
45. https://perspectiveapi.com/

and e-commerce environments. We follow the setting of ReAct [361] that evaluate the 1-shot and 2-shot performance of LLMs on WebShop and ALFWorld respectively, and compute success rate for ALFWorld and average score/success rate for WebShop. Further, we also follow ReAct [361] to reduce the length of the input prompt and utilize line break as the EOS token.
‚Ä¢ Tool manipulation. For tool manipulation, we consider two kinds of tools including search engine and model interfaces. Therefore, we adopt two tool manipulation benchmarks, i.e., HotpotQA [409] and Gorilla [518]. HotpotQA requires LLMs to use search engine to retrieve documents from the web, and Gorilla to invoke model APIs from three hubs of TorchHub, TensorHub and HuggingFace. We compute exact match for HotpotQA and accuracy for Gorilla. For HotpotQA, we follow ReAct [361] to report the 3-shot performance. For Gorilla, we follow the code released by its paper [518], and evaluate the zero-shot performance.
Implementation Details. For each task and dataset, we evaluate the compared LLMs using the same prompts and results parsing method provided by existing work (i.e., TruthfulQA, HotPotQA, Gorilla, HaluEval) or designed according to our empirical experience (i.e., TriviaQA, Natural Questions, Web Questions, ARC, WikiFact, GSM8k, MATH, C-Objects, Penguins, LAMBADA, WMT‚Äô22, XSum, HumanEval, CrowS-Pairs, WinoGender, RealToxityPrompt). Specifically, all the experiments about closed-source models are based on invoking their official APIs, while for opensource models, we utilize their publicly available code and model parameters, and perform the inference on 8 A80080G GPUs. For TriviaQA, OpenbookQA, HellaSwag, and SocialIQA, we experiment on the development set since the test set is not publicly released. While for other datasets, we experiment on the test set. To reproduce our experiments, we also publicly release our experimental code and data in https://github.com/RUCAIBox/LLMSurvey/tree/ main/Experiments.
7.3.3 Results Analysis and Findings
We report the experimental results in Table 11, and analyze the results in the following.
Analysis of Closed-Source Models. We summarize our analysis and findings of the four closed-source models (i.e., ChatGPT, Claude, Davinci003 and Davinci002) as follows:
‚Ä¢ These four closed-source models achieve promising results as general-purpose task solvers, in which ChatGPT mostly performs the best. ChatGPT, Claude, Davinci003 and Davinci002 perform well in most of tasks, including complex tasks (e.g., GSM8k), which have shown great potential to be generalpurpose task solvers. Among them, ChatGPT exhibits a more superior model capacity on the evaluation tasks, winning the best on ten tasks. In some evaluation tasks, the performance gap between ChatGPT and other closedsource models is very large, especially for complex tasks e.g., 76.50 (ChatGPT) v.s. 49.73 (Davinci002) on GSM8k, and 79.88 (ChatGPT) v.s. 51.22 (Claude) on HumanEval.
‚Ä¢ ChatGPT and Davinci003 perform better on interaction with environment and tool manipulation tasks. On the two evaluation tasks, two OpenAI models, ChatGPT and Davinci003, perform better than other models by a large

51

margin, e.g., 38.81 (ChatGPT) v.s. 0.00 (Claude) on ALFW, and 24.6 (ChatGPT) v.s. 3.8 (Claude) on HotpotQA. A possible reason is that these two OpenAI models have been specially optimized towards these advanced abilities, e.g., OpenAI models support the use of plugins in ChatGPT.
‚Ä¢ All the comparison models perform not well on very difficult reasoning tasks. On MATH and HotpotQA, all models (including ChatGPT) perform not well. The two tasks are very difficult to solve, requiring accurate understanding of complex mathematical knowledge and performing multihop reasoning across documents, respectively. Further, these models also have a relatively weak performance on machine translation task (WMT). A possible reason is that WMT also contains many evaluation examples in minor languages, which might not be well covered in the pre-training data of these LLMs.
Analysis of Open-Source Models. Next, we continue to show our analysis and findings about six open-source models (i.e., Vicuna, Alpaca, ChatGLM, LLaMA, Pythia and Falcon) as follows:
‚Ä¢ Instruction-tuned models mostly perform better than the base models. Among all the compared open-source methods, the instruction-tuned models (i.e., Vicuna, Alpaca and ChatGLM) mostly perform better than non-instruction-tuned models (i.e., LLaMA, Pythia and Falcon). It indicates that instruction tuning is generally capable of improving the few-shot or zero-shot ability of LLMs in solving various tasks. However, after instruction tuning, Vicuna-7B and Alpaca-7B suffer from performance degradations on LAMBADA, a language modeling task. The reason may be that the instruction data mainly focuses on enabling LLMs to follow human instructions, which is not always useful for the general language generation task.
‚Ä¢ These small-sized open-source models perform not well on mathematical reasoning, interaction with environment, and tool manipulation tasks. On the tasks of mathematical reasoning, interaction with environment and tool manipulation, all these evaluated open-source models perform not well, including instruction-tuned ones. A possible reason is that the instruction data for fine-tuning these models is not specifically designed for these tasks. In addition, these closedsource models may have limited model capacities due to small model sizes.
‚Ä¢ The top-performing model varies on different human alignment tasks. For different human alignment tasks, we can see that these models achieve inconsistent performance rankings. For example, ChatGLM-6B performs the best among the compared open-source models on TruthfulQA, while Falcon-7B performs the best on CrowS-Pairs. A possible reason is that these tasks are designed with specific purposes for evaluating different aspects of human alignment, and these models exhibit varied performance on different tasks, even for the variants of the same model (e.g., Pythia-7B and Pythia-12B). More experiments and analysis on human alignment evaluation are needed to reveal more detailed findings.
‚Ä¢ As a more recently release model, Falcon-7B achieves a decent performance, especially on language generation tasks. For language generation tasks, Falcon-7B mostly performs better than other base models, e.g., 10.00 (Falcon-7B) v.s. 8.77

(LLaMA-7B) in LAMABDA. For other tasks (e.g., knowledge utilization and complex reasoning), Falcon-7B can also achieve comparable performance as LLaMA-7B. It has adopted a careful data pre-processing pipeline to filter lowquality and duplicate content from the web data, which mainly contributes to the excellent performance.
The readers should be note that these findings about open-source language models are limited to the model sizes. We will continually update this part by including the results of larger versions of these models, and also call for the support of computational resources for more experiments.
8 A PRACTICAL GUIDEBOOK OF PROMPT DESIGN
As discussed in Section 6, prompting is the major approach to utilizing LLMs for solving various tasks. Since the quality of prompts will largely influence the performance of LLMs in specific tasks, we set up a special section to discuss the prompt design in practice. In this section, we will first introduce the key components of prompts and discuss several principles for prompt design. Then, we evaluate ChatGPT with different prompts to show the results on several representative tasks. We are aware that there have been several existing papers [555, 556] and websites [557‚Äì 559] that present the suggestions and guidelines to design good prompts. As a comparison, we mainly aim to discuss the key factors (ingredients and principles) that are useful for prompt creation, and provide experimental results and analysis on popular tasks as the reference to the beginners.
8.1 Prompt Creation
The process of creating a suitable prompt is also called prompt engineering [556, 560]. A well-designed prompt is very helpful to elicit the abilities of LLMs for accomplishing specific tasks. In this part, we briefly summarize the key ingredients of prompts and discuss several basic principles of prompt design.
Key Ingredients. Typically, there are four key ingredients that depict the functionality of a prompt for eliciting the abilities of LLMs to complete the tasks, including task description, input data, contextual information, and prompt style. To have an intuitive understanding of our discussion, we also present three prompt examples for question answering, meta-review generation, and text-to-SQL in Table 14.
‚Ä¢ Task description. A task description is typically a specific instruction that LLMs are expected to follow. In general, one should clearly describe the task goal in natural language. For the tasks with special input or output format, detailed clarifications are often needed, and one can further utilize keywords to highlight the special settings for better guiding LLMs in task completion.
‚Ä¢ Input data. In common cases, it is straightforward to describe input data (e.g., an instance to be responded by LLMs) in natural language. For special input data, such as knowledge graph and table, it is necessary to apply an appropriate and convenient way to make them readable for LLMs. For structured data, linearization is commonly used to transform the original records (e.g., knowledge triples) into sequences [442] due to the simplicity. Further, the programming language (e.g., executable code) has also

52

been utilized to formulate the structured data, which can also support using external tools (e.g., program executor) to produce the precise results [561, 562].
‚Ä¢ Contextual information. In addition to the task description and input data, contextual or background information is also essential for specific tasks. For example, retrieved documents are highly useful for open-domain question answering as supporting evidence. Thus, it needs to include such information in a proper prompt pattern or expression format. Furthermore, in-context task exemplars are also helpful for eliciting LLMs to accomplish a complex task, which can better depict the task goal, the special output formats, and the mapping relation between input and output.
‚Ä¢ Prompt style. For different LLMs, it is important to design a suitable prompt style for eliciting their abilities to solve specific tasks. Overall, one should express the prompt as a clear question or detailed instruction that can be well understood and answered. In some cases, it is also useful to add the prefix or suffix to better guide LLMs. For example, using the prefix ‚ÄúLet us think step by step‚Äù can help elicit LLMs perform step-by-step reasoning, and using the prefix ‚ÄúYou are an expert on this task (or in this domain)‚Äù can boost the performance of LLMs in some specific tasks. Further, for chat-based LLMs (e.g., ChatGPT), instead of directly feeding a long or complex task prompt, it is suggested to decompose it into multiple prompts for the sub-tasks and then feed them into LLMs via a multi-turn conversation [362].
Design Principles. Based on the key ingredients of prompts, we summarize several critical design principles that can help create more effective prompts for solving various tasks.
‚Ä¢ Expressing the task goal clearly. Task descriptions should not be ambiguous or unclear, which likely lead to inaccurate or inappropriate responses. This highlights the need for clear and unambiguous directives when utilizing these models [61]. A clear and detailed description should contain various elements to explain a task, including task objective, input/output data (e.g., ‚ÄúGiven a long document, I want you to generate a concise summary.‚Äù), and the response constraints (e.g., ‚Äúthe length of the summary cannot exceed 50.‚Äù). By providing a well-clarified task description, LLMs can more effectively understand the target task and generate the desired output.
‚Ä¢ Decomposing into easy, detailed sub-tasks. To solve complex tasks, it is important to decompose the difficult task into several more easier, detailed sub-tasks for helping LLMs accomplish the goal step by step, which is closely related to the planning technique in Section 6.3. For example, following the suggestion [555], we can explicitly list the subtasks in the form of multiple numbered items (e.g., ‚ÄúBraid a coherent narrative by performing the following tasks: 1. ...; 2. ...; 3. ...‚Äù). By decomposing a target task into sub-tasks, LLMs can focus on solving easier sub-tasks and finally achieve more accurate results for complex tasks.
‚Ä¢ Providing few-shot demonstrations. As discussed in Section 6.1, LLMs can benefit from in-context learning for solving complex tasks, where the prompts contain a small number of task examples of the desired input-output pairs, i.e., few-shot demonstrations. Few-shot demonstrations can help LLMs learn the semantic mapping between input and output without parameter tuning. In practice, it is suggested

that one should generate a few high-quality demonstrations for the target task, which would highly benefit the final task performance.
‚Ä¢ Utilizing model-friendly format. Since LLMs are pretrained on specially constructed datasets, there are some prompt formats that can make LLMs better understand the instruction. For example, as the OpenAI documentation suggests, we can use ### or """ as a stop symbol to separate the instruction and context, which can be better understood by LLMs. As a general guideline, most existing LLMs perform a task better in English, thus it is useful to employ English instructions to solve difficult tasks based on machine translation.
Useful Tips. In addition to the design principles, we also present a collection of useful prompt tips based on existing work or our empirical experiences in Table 13. Note that these tips are suggested in a general manner, it does not indicate that they are the best prompts for the corresponding tasks. This part will be continuously updated with more guidelines or tips. We welcome readers to contribute to this collection of prompt tips. We present the detailed procedure to contribute to the prompt tips, at the link: https://github. com/RUCAIBox/LLMSurvey/tree/main/Prompts.
8.2 Results and Analysis
In the above subsection, we have discussed the general principles to design the prompts. This part presents concrete examples of prompts to solve a number of common tasks. Specially, these task prompts are mostly from existing papers, and the experiments are conducted by using the prompts based on ChatGPT for the corresponding tasks.
Experimental Setup. To conduct the experiments, we select a variety of tasks that span language generation, knowledge utilization, complex reasoning, structure data generation, and information retrieval. For each task, we manually write a prompt that follows general guidelines introduced in Section 8.1. Note that the tested prompts may not be the optimal for these tasks, since they mainly aim to help readers understand how to write an effective prompt for solving different tasks. Also, we add a simplified prompt as the comparison for most tasks. Following the experimental settings in Section 7.3.2, we examine the 3-shot performance of ChatGPT on complex reasoning tasks (Colored Objects and GSM8k), and zero-shot performance on other tasks.
Results Analysis. We report the experimental results in Table 12, where we also include the supervised performance in existing papers as reference.
‚Ä¢ Carefully designed prompts can boost the zero-shot or fewshot performance of ChatGPT. By comparing the results of using different prompts on the same task, we can see that using the carefully designed prompts can achieve better performance than the simpler ones. In the carefully designed prompts, we provide a more clearly expressed task description (e.g., WMT and WikiFact), or use a model-friendly format (e.g., GSM8k and OBQA). For example, for WikiFact task, the prompt with a more detailed task description leads to a performance increase from 29.25 to 31.21.
‚Ä¢ More complex tasks can benefit more from careful prompt engineering on ChatGPT. In the WikiFact and Colored Objects

53
TABLE 13: A collection of useful tips for designing prompts that are collected from online notes [555‚Äì558] and experiences from our authors, where we also show the related ingredients and principles (introduced in Section 8.1). We abbreviate principles as Prin. and list the IDs of the related principles for each prompt. ‚Éù1 : expressing the task goal clearly; ‚Éù2 : decomposing into easy, detailed sub-tasks; ‚Éù3 : providing few-shot demonstrations; ‚Éù4 : utilizing model-friendly format.

Ingredient Task Description
Input Data Contextual Information
Demonstration
Other Designs

Collected Prompts
T1. Make your prompt as detailed as possible, e.g., ‚ÄúSummarize the article into a short paragraph within 50 words. The major storyline and conclusion should be included, and the unimportant details can be omitted.‚Äù T2. It is helpful to let the LLM know that it is an expert with a prefixed prompt, e.g., ‚ÄúYou are a sophisticated expert in the domain of compute science.‚Äù T3. Tell the model more what it should do, but not what it should not do. T4. To avoid the LLM to generate too long output, you can just use the prompt: ‚ÄúQuestion: Short Answer: ‚Äù. Besides, you can also use the following suffixes, ‚Äúin a or a few words‚Äù, ‚Äúin one of two sentences‚Äù.
I1. For the question required factual knowledge, it is useful to first retrieve relevant documents via the search engine, and then concatenate them into the prompt as reference. I2. To highlight some important parts in your prompt, please use special marks, e.g., quotation (‚Äù‚Äù) and line break (\n). You can also use both of them for emphasizing.
C1. For complex tasks, you can clearly describe the required intermediate steps to accomplish it, e.g., ‚ÄúPlease answer the question step by step as: Step 1 - Decompose the question into several sub-questions, ¬∑ ¬∑ ¬∑ ‚Äù C2. If you want LLMs to provide the score for a text, it is necessary to provide a detailed description about the scoring standard with examples as reference. C3. When LLMs generate text according to some context (e.g., making recommendations according to purchase history), instructing them with the explanation about the generated result conditioned on context is helpful to improve the quality of the generated text.
D1. Well-formatted in-context exemplars are very useful to guide LLMs, especially for producing the outputs with complex formats. D2. For few-shot chain-of-thought prompting, you can also use the prompt ‚ÄúLet‚Äôs think step-by-step‚Äù, and the few-shot examples should be separated by ‚Äú\n‚Äù instead of full stop. D3. You can also retrieve similar examples in context to supply the useful task-specific knowledge for LLMs. To retrieve more relevant examples, it is useful to first obtain the answer of the question, and then concatenate it with the question for retrieval. D4. The diversity of the in-context exemplars within the prompt is also useful. If it is not easy to obtain diverse questions, you can also seek to keep the diversity of the solutions for the questions. D5. When using chat-based LLMs, you can decompose in-context exemplars into multi-turn messages, to better match the human-chatbot conversation format. Similarly, you can also decompose the reasoning process of an exemplars into multi-turn conversation. D6. Complex and informative in-context exemplars can help LLMs answer complex questions. D7. As a symbol sequence can typically be divided into multiple segments (e.g., i1, i2, i3 ‚àí‚Üí i1, i2 and i2, i3), the preceding ones can be used as in-context exemplars to guide LLMs to predict the subsequent ones, meanwhile providing historical information. D8. Order matters for in-context exemplars and prompts components. For very long input data, the position of the question (first or last) may also affect the performance. D9. If you can not obtain the in-context exemplars from existing datasets, an alternative way is to use the zero-shot generated ones from the LLM itself.
O1. Let the LLM check its generated results before draw the conclusion, e.g., ‚ÄúCheck whether the above solution is correct or not.‚Äù O2. If the LLM can not well solve the task, you can seek help from external tools by prompting the LLM to manipulate them. In this way, the tools should be encapsulated into callable APIs with detailed description about their functions, to better guide the LLM to utilize the tools. O3. The prompt should be self-contained, and better not include the information in the context with pronouns (e.g., it and they). O4. When using LLMs for comparing two or more examples, the order affects the performance a lot. O5. Before the prompt, assigning a role for the LLM is useful to help it better fulfill the following task instruction, e.g., ‚ÄúI want you to act as a lawyer‚Äù. O6. OpenAI models can perform a task better in English than other languages. Thus, it is useful to first translate the input into English and then feed it to LLMs. O7. For multi-choice questions, it is useful to constrain the output space of the LLM. You can use a more detailed explanation or just imposing constraints on the logits. O8. For sorting based tasks (e.g., recommendation), instead of directly outputting the complete text of each item after sorting, one can assign indicators (e.g., ABCD) to the unsorted items and instruct the LLMs to directly output the sorted indicators.

Prin. ‚Éù1 ‚Éù1 ‚Éù1 ‚Éù1
‚Éù4 ‚Éù4
‚Éù2 ‚Éù1 ‚Éù2
‚Éù3 ‚Éù1 ‚Éù3 ‚Éù3 ‚Éù4
‚Éù3 ‚Éù3
‚Éù3 ‚Éù2 ‚Éù3
‚Éù3 ‚Éù3
‚Éù2 ‚Éù4
‚Éù1 ‚Éù1 ‚Éù1 ‚Éù4 ‚Éù1 ‚Éù1

tasks, the designed prompts have greatly improved the performance of ChatGPT, i.e., from 23.61 to 28.47 on WikiFact and from 53.20 to 66.75 on Colored Objects. It indicates the necessity of prompt engineering for LLMs to perform well on complex tasks, since these tasks typically have specific output formats or require background knowledge. Our example prompts provide more detailed task description (e.g., output format and task goal), which can help ChatGPT better understand the complex task requirement for fulfilling it.
‚Ä¢ For mathematical reasoning tasks, it is more effective to

design specific prompts based on the format of programming language. For GSM8k, the designed prompt employs codeformatted few-shot demonstrations to convert this mathematical reasoning task into code generation task, which can leverage the strong code synthesis ability of ChatGPT for solving mathematical problems. Further, with the help of an external program executor, we are able to obtain more precise results instead of using LLMs for arithmetic operation. As we can see, the performance is boosted from 78.47 to 79.30 on GSM8k, indicating the usefulness of programming language in mathematical reasoning tasks.

54

‚Ä¢ In knowledge utilization and complex reasoning tasks, ChatGPT with proper prompts achieves comparable performance or even outperforms the supervised baselines methods. In knowledge utilization and complex reasoning tasks, ChatGPT with proper zero-shot or few-shot prompts can achieve comparable performance or even outperform the supervised methods, e.g., 31.21 (ChatGPT) v.s. 34.20 (supervised baseline) on WikiFact. Despite that, ChatGPT still performs worse than supervised baseline models on some specific tasks (e.g., ARC and WikiFact), since these supervised models have been specially optimized with task-specific data.
‚Ä¢ Through suitable prompt engineering, LLMs can handle some non-traditional NLP tasks. With the help of specific prompts, ChatGPT can also accomplish non-traditional NLP tasks, i.e., the general recommendation and conversational recommendation. A key point is that these tasks can be well expressed or described in natural language. However, the performance of ChatGPT is still far from the referenced performance in these tasks, as LLMs cannot directly fit these tasks, which require specific domain knowledge and task adaptation [255, 563].
9 APPLICATIONS
As LLMs are pre-trained on a mixture of source corpora, they can capture rich knowledge from large-scale pretraining data, thus having the potential to serve as domain experts or specialists for specific areas. In this section, we briefly review the recent progress on the applications of LLMs on several representative domains, including healthcare, education, law, finance, and scientific research.
Healthcare is a vital application field closely related to human life. Ever since the advent of ChatGPT, a number of studies have applied ChatGPT or other LLMs to the medical domain. It has been shown that LLMs are capable of handling a variety of healthcare tasks, e.g., biology information extraction [565], medical advice consultation [566], mental health analysis [567], and report simplification [568]. As the major technical approach, researchers typically design specific prompts or instructions to guide LLMs to perform a wide range of medical tasks. To further harness the power of LLMs in the healthcare domain, researchers propose to develop healthcare-related LLMs. Specifically, the Med-PaLM models [254, 569] achieves expert-level performance on the United States Medical Licensing Examination (USMLE), and earns greater approval from physicians in answering consumer‚Äôs medical questions. However, LLMs may fabricate medical misinformation [568, 570], e.g., misinterpreting medical terms and suggesting advice inconsistent with medical guidelines. In addition, it would also raise privacy concerns to upload the health information of patients [565] into a commercial server that support the LLM.
Education is also an important application domain where LLMs potentially exert significant influence. Existing work has found that LLMs can achieve student-level performance on standardized tests [46] in a variety of subjects of mathematics (e.g., physics, computer science) on both multiplechoice and free-response problems. In addition, empirical studies have shown that LLMs can serve as writing or reading assistant for education [571, 572]. A recent study [572]

reveals that ChatGPT is capable of generating logically consistent answers across disciplines, balancing both depth and breadth. Another quantitative analysis [571] shows that students utilizing ChatGPT (either keeping or refining the results from LLMs as their own answers) perform better than average students in some courses from the computer security field. Recently, several perspective papers [573, 574] also explore various application scenarios of LLMs in classroom teaching, such as teacher-student collaboration, personalized learning, and assessment automation. However, the application of LLMs in education may lead to a series of practical issues, e.g., plagiarism, potential bias in AIgenerated content, overreliance on LLMs, and inequitable access for non-English speaking individuals [575].
Law is a specialized domain that is built on professional domain knowledge. Recently, a number of studies have applied LLMs to solve various legal tasks, e.g., legal document analysis [576], legal judgment prediction [577], and legal document writing [578]. A recent study [579] has found that LLMs exhibit powerful abilities of legal interpretation and reasoning. Moreover, the latest GPT-4 model achieves a top 10% score in a simulated bar exam compared with human test-takers [46]. To further improve the performance of LLMs in the law domain, specially designed legal prompt engineering are employed to yield advanced performance in long legal document comprehension and complex legal reasoning [580, 581]. To summarize the progress, LLMs can act as helpful assistants to legal profession. Despite the progress, the use of LLMs in law raises concerns about legal challenges, including copyright issues [582], personal information leakage [583], or bias and discrimination [584].
Finance is an important field where LLMs have promising application prospects. LLMs have been employed on various finance related tasks, such as numerical claim detection [585], financial sentiment analysis [586], financial named entity recognition [587], and financial reasoning [588]. Despite the competitive zero-shot performance exhibited by general-purpose LLMs in the finance tasks, they still underperform domain-specific PLMs containing million-scale parameters [585]. To leverage the scaling effect of LLMs, researchers collect large-scale finance corpora for continually pre-training LLMs (e.g., BloombergGPT [258], XuanYuan 2.0 [589], and FinGPT [590]). BloombergGPT has demonstrated remarkable performance across a diverse range of financial tasks while maintaining competitive performance in general-purpose tasks [258]. Nevertheless, it is imperative to consider the potential risks in the application of LLMs in finance, as the generation of inaccurate or harmful content by LLMs could have significant adverse implications for financial markets [258]. Therefore, it needs more strict reviewing and monitoring on the use of LLMs in the financial field.
Scientific research is another promising field that LLMs can empower the development progress. Prior research demonstrates the effectiveness of LLMs in handling knowledge-intensive scientific tasks (e.g., PubMedQA [591], BioASQ [592]), especially for LLMs that are pre-trained on scientific-related corpora (e.g., Galactica [35], Minerva [159]). Given the excellent general abilities and broad scientific

55
TABLE 14: Example instructions collected from [555, 564]. The blue text denotes the task description, the red text denotes the contextual information, the green text denotes the demonstrations, and the gold text denotes the prompt style.
Use the provided articles delimited by triple quotes to answer questions. If the answer cannot be found in the articles, write ‚ÄúI could not find an answer.‚Äù Articles: ‚Äú‚Äú‚ÄúJoao Moutinho is a Portuguese footballer who last played as a central midfielder for Premier League club Wolverhampton Wanderers and the Portugal national team.‚Äù‚Äù‚Äù Question: Is the following sentence plausible? ‚ÄôJoao Moutinho was out at third.‚Äô Answer: Let‚Äôs think step by step. Joao Moutinho is a soccer player. Being out at third is part of baseball, not soccer. So the answer is No. ... <Demonstrations>
Articles: <insert articles, each delimited by triple quotes> Question: <insert question> Answer:
Prepare a meta-review by answering the following questions from the reviewer comments (provided after the questions). 1. Based on the reviewer‚Äôs comments, what are the core contributions made by this manuscript? 2. What are the common strengths of this work, as mentioned by multiple reviewers? 3. What are the common weaknesses of this work, as highlighted by multiple reviewers? 4. What suggestions would you provide for improving this paper? 5. What are the missing references mentioned by the individual reviews? The review texts are below: <insert three comments R1, R2, R3 from the reviewers> Meta-review: <insert meta-review> ... <Demonstrations>
Provide justification for your response in detail by explaining why you made the choices you actually made. A good output should be coherent, highlight major strengths/issues mentioned by multiple reviewers, be less than 400 words in length, and finally, the response should be in English only.
The review texts are below: <insert three comments R1, R2, R3 from the reviewers> Meta-review:
CREATE TABLE Highschooler ( ID int primary key, name text, grade int ); /* 3 example rows: SELECT * FROM Highschooler LIMIT 3; ID name grade 1234 Janie 8 5678 Mary 8 9012 Mike 9 */ Using valid SQLite, answer the following questions for the tables provided above. Question: What is Kyle‚Äôs id? SQL: SELECT ID FROM Highschooler WHERE name=‚ÄúKyle‚Äù; ... <Demonstrations>
Question: <insert question> SQL:

knowledge, LLMs hold significant potential as helpful assistants across various stages of the scientific research pipeline [593]. First, during the literature survey stage, LLMs can help conduct a comprehensive overview of the progress in a specific research field [594, 595]. Second, during the research idea generation stage, LLMs demonstrate the ability to generate intriguing scientific hypotheses [596]. Third, during the data analysis stage, LLMs can be employed to conduct automatic approaches to analyzing the data characteristics, including data exploration, visualization, and deriving analytical conclusions [597, 598]. Fourth, during the paper writing stage, researchers can also benefit from the assistance of LLMs in scientific writing [599, 600],

in which LLMs can offer valuable support for scientific writing through diverse means, such as summarizing the existing content and polishing the writing [601]. In addition, LLMs can aid in the automated paper review process, encompassing tasks such as error detection, checklist verification, and candidate ranking [602]. Despite these advances, there is much room for improving the capacities of LLMs to serve as helpful, trustworthy scientific assistants, to both increase the quality of the generated scientific content and reduce the harmful hallucinations.
Summary. In addition to the aforementioned work, the applications of LLMs have been also discussed in several other domains. For instance, in the psychologic domain,

56

some recent work has studied the human-like characteristics of LLMs, such as self-awareness, theory of mind (ToM), and affective computing [603, 604]. In particular, an empirical evaluation of ToM conducted on two classic false-belief tasks speculates that LLMs may have ToM-like abilities since the model in the GPT-3.5 series achieves comparable performance with nine-year-old children in ToM task [603]. In addition, another line of work has investigated applying LLMs into the software development domain, e.g., code suggestion [605], code summarization [606], and automated program repair [607]. To summarize, to assist humans by LLMs in real-world tasks has become a significant area of research. However, it also presents challenges. Ensuring the accuracy of LLM-generated content, addressing biases, and maintaining user privacy and data security are crucial considerations when applying LLMs to real-world scenarios.
10 CONCLUSION AND FUTURE DIRECTIONS
In this survey, we have reviewed the recent progress of large language models (LLMs), and introduced the key concepts, findings, and techniques for understanding and utilizing LLMs. We focus on the large-sized models (i.e., having a size larger than 10B) while excluding the contents of early pre-trained language models (e.g., BERT and GPT2) that have been well covered in the existing literature. In particular, our survey has discussed four important aspects of LLMs, i.e., pre-training, adaptation tuning, utilization, and evaluation. For each aspect, we highlight the techniques or findings that are key to the success of LLMs. Furthermore, we also summarize the available resources for developing LLMs and discuss important implementation guidelines for reproducing LLMs. This survey tries to cover the most recent literature about LLMs and provides a good reference resource on this topic for both researchers and engineers.
Next, we summarize the discussions of this survey, and introduce the challenges and future directions for LLMs, in the following aspects.
Theory and Principle. To understand the underlying working mechanism of LLMs, one of the greatest mysteries is how information is distributed, organized, and utilized through the very large, deep neural network. It is important to reveal the basic principles or elements that establish the foundation of the abilities of LLMs. In particular, scaling seems to play an important role in increasing the capacity of LLMs [31, 55, 59]. It has been shown that some emergent abilities would occur in an unexpected way (a sudden performance leap) when the parameter scale of language models increases to a critical size (e.g., 10B) [31, 33], typically including in-context learning, instruction following, and stepby-step reasoning. These emergent abilities are fascinating yet perplexing: when and how they are obtained by LLMs are not yet clear. Recent studies either conduct extensive experiments for investigating the effect of emergent abilities and the contributing factors to such abilities [307, 608, 609], or explain some specific abilities with existing theoretical frameworks [60, 317]. An insightful technical post also specially discusses this topic [47], taking the GPT-series models as the target. However, more formal theories and principles to understand, characterize, and explain the abilities or

behaviors of LLMs are still missing. Since emergent abilities bear a close analogy to phase transitions in nature [31, 58], cross-discipline theories or principles (e.g., whether LLMs can be considered as some kind of complex systems) might be useful to explain and understand the behaviors of LLMs. These fundamental questions are worth exploring for the research community, which are important for developing the next-generation LLMs.
Model Architecture. Due to the scalability and effectiveness, Transformer, consisting of stacked multi-head selfattention layers, has become the de facto architecture for building LLMs. Various strategies have been proposed to improve the performance of this architecture, such as neural network configuration and scalable parallel training (see discussions in Section 4.2.2). To enhance the model capacity (e.g., the multi-turn conversation ability), existing LLMs typically maintain a long context window, e.g., GPT-4-32k has an extremely large context length of 32,768 tokens. Thus, a practical consideration is to reduce the time complexity (originally to be quadratic costs) incurred by the standard self-attention mechanism. It is important to investigate the effect of more efficient Transformer variants in building LLMs [610], e.g., sparse attention has been used in GPT3 [55]. Besides, catastrophic forgetting has been a longstanding challenge for neural networks, which also has a negative impact on LLMs. When tuning LLMs with new data, the originally learned knowledge is likely to be damaged, e.g., fine-tuning a LLM according to some specific tasks will affect the general ability of LLMs. A similar case occurs when LLMs are aligned with human values (called alignment tax [61, 268]). Thus, it is necessary to consider extending existing architectures with more flexible mechanisms or modules that can effectively support data update and task specialization.
Model Training. In practice, it is very difficult to pretrain capable LLMs, due to the huge computation consumption and the sensitivity to data quality and training tricks [69, 83]. Thus, it becomes particularly important to develop more systemic, economical pre-training approaches for optimizing LLMs, considering the factors of model effectiveness, efficiency optimization, and training stability. More model checking or performance diagnosis methods (e.g., predictable scaling in GPT-4 [46]) should be developed in order to detect early abnormal issues during training. Furthermore, it also calls for more flexible mechanisms of hardware support or resource schedule, so as to better organize and utilize the resources in a computing cluster. Since it is very costly to pre-train a LLM from scratch, it is important to design a suitable mechanisms for continually pre-training or fine-tuning the LLM based on publicly available model checkpoints (e.g., LLaMA [57] and FlanT5 [64]). For this purpose, a number of technical issues have to be resolved, e.g., catastrophic forgetting and task specialization. However, to date, there still lack open-source model checkpoints for LLMs with complete pre-processing and training logs (e.g., the scripts to prepare the pre-training data) for reproduction. We believe that it will be of great value to report more technical details in open-source models for the research of LLMs. Furthermore, it is also important

57

to develop more improvement tuning strategies that effectively elicits the model abilities.
Model Utilization. Since fine-tuning is very costly in real applications, prompting has become the prominent approach to using LLMs. By combining task descriptions and demonstration examples into prompts, in-context learning (a special form of prompting) endows LLMs with the ability to perform well on new tasks, even outperforming full-data fine-tuned models in some cases. Furthermore, to enhance the ability of complex reasoning, advanced prompting techniques have been proposed, exemplified by the chain-ofthought (CoT) strategy, which includes the intermediate reasoning steps into prompts. However, existing prompting approaches still have several deficiencies described as follows. Firstly, it involves considerable human efforts in the design of prompts. It would be quite useful to automatically generate effective prompts for solving various tasks. Secondly, some complex tasks (e.g., formal proof and numerical computation) require specific knowledge or logic rules, which may not be well expressed in natural language or demonstrated by examples. Thus, it is important to develop more informative, flexible task formatting methods for prompts46. Thirdly, existing prompting strategies mainly focus on single-turn performance. It is useful to develop interactive prompting mechanisms (e.g., through natural language conversations) for solving complex tasks, which have been demonstrated to be very useful by ChatGPT.
Safety and Alignment. Despite their capacities, LLMs pose similar safety challenges as small language models. For example, LLMs exhibit a tendency to generate hallucinations [448], which are texts that seem plausible but may be factually incorrect. What is worse, LLMs might be elicited by intentional instructions to produce harmful, biased, or toxic texts for malicious systems, leading to the potential risks of misuse [55, 61]. To have a detailed discussion of the safety issues of LLMs (e.g., privacy, overreliance, disinformation, and influence operations), the readers can refer to the GPT3/4 technical reports [46, 55]. As the major approach to averting these issues, reinforcement learning from human feedback (RLHF) [61, 100] has been widely used by incorporating humans in the training loop for developing well-aligned LLMs. To improve the model safety, it is also important to include safety-relevant prompts during RLHF, as shown by GPT-4 [46]. However, RLHF heavily relies on high-quality human feedback data from professional labelers, making it difficult to be properly implemented in practice. Therefore, it is necessary to improve the RLHF framework for reducing the efforts of human labelers and seek a more efficient annotation approach with guaranteed data quality, e.g., LLMs can be employed to assist the labeling work. More recently, red teaming [115, 269] has been adopted for improving the model safety of LLMs, which utilizes the collected adversarial prompts to refine the LLMs (i.e., avoiding the attacks from red teaming). Furthermore, it is also meaningful to establish the proper learning mechanism for LLMs to obtain human feedback
46. It seems that an alternative approach to this issue is to invoke external tools, e.g., the plugins for ChatGPT, when the task is difficult to solve via text generation.

via chatting and directly utilize it for self-improvement.
Application and Ecosystem. As LLMs have shown a strong capacity in solving various tasks, they can be applied in a broad range of real-world applications (i.e., following taskspecific natural language instructions). As a remarkable progress, ChatGPT has potentially changed the way how humans access information, which has been implemented in the release of New Bing. In the near future, it can be foreseen that LLMs would have a significant impact on information-seeking techniques, including both search engines and recommender systems. Furthermore, the development and use of intelligent information assistants would be highly promoted with the technology upgrade from LLMs. In a broader scope, this wave of technical innovation would lead to an ecosystem of LLM-empowered applications (e.g., the support of plugins by ChatGPT), which has a close connection with human life. Lastly, the rise of LLMs sheds light on the exploration of artificial general intelligence (AGI). It is promising to develop more smart intelligent systems (possibly with multi-modality signals) than ever. However, in this development process, AI safety should be one of the primary concerns, i.e., making AI lead to good for humanity but not bad [40].
CODA
It is not an easy job to write this long survey and update its content with timely work. First of all, we would like to sincerely thank the support from the readers and our team members. We work very hard on this survey, and hope that it can present a comprehensive, timely reference for LLMs.
Survey Writing. This survey was planned during a discussion meeting held by our research team, and we aimed to summarize the recent advances of large language models as a highly readable report for our team members. The first draft was finished on March 13, 2023, in which our team members tried their best to include the related studies about LLMs in a relatively objective, comprehensive way. Then, we have extensively revised the writing and contents in several passes. Due to the space limit, we can only include a fraction of existing LLMs in Figure 2 and Table 1, by setting the selection criterion. However, we set a more relaxed criterion for model selection on our GitHub page (https://github.com/RUCAIBox/LLMSurvey), which will be regularly maintained. We release the initial version on March 31, 2023, and the major revision on June 29, 2023.
Seeking for Advice. Despite all our efforts, this survey is still far from perfect: we are likely to miss important references or topics, and might also have non-rigorous expressions or discussions. We will continuously update this survey, and improve the quality as much as we can. For us, survey writing is also a learning process for LLMs by ourselves. For readers with constructive suggestions to improve this survey, you are welcome to leave comments on the GitHub page of our survey or directly email our authors. We will make revisions following the received comments or suggestions in a future version, and acknowledge the readers who have contributed constructive suggestions in our survey.

58

Update log. In this part, we regularly maintain a update log for the submissions of this survey to arXiv:
‚Ä¢ First release on March 31, 2023: the initial version. ‚Ä¢ Update on April 9, 2023: add the affiliation information,
revise Figure 2 and Table 1 and clarify the corresponding selection criterion for LLMs, improve the writing, and correct some minor errors. ‚Ä¢ Update on April 11, 2023: correct the errors for library resources. ‚Ä¢ Update on April 12, 2023: revise Figure 2 and Table 1, and clarify the release date of LLMs. ‚Ä¢ Update on April 16, 2023: add a new Section 2.2 about the technical evolution of GPT-series models. ‚Ä¢ Update on April 24, 2023: add the discussion about scaling laws and add some explanations about the model sizes for emergent abilities (Section 2.1); add an illustrative figure for the attention patterns for different architectures in Figure 7, and add the detailed formulas in Table 4. ‚Ä¢ Update on April 25, 2023: revise some copy errors in figures and tables. ‚Ä¢ Update on April 27, 2023: add efficient tuning in Section 5.3. ‚Ä¢ Update on April 28, 2023: revise Section 5.3. ‚Ä¢ Update on May 7, 2023: revise Table 1, Table 2, and some minor points. ‚Ä¢ Update on June 29, 2023 (major revision):
‚Äì Section 1: add Figure 1 for the trends of published LLM papers in arXiv;
‚Äì Section 2: add Figure 3 for GPT‚Äôs evolution and the corresponding discussion;
‚Äì Section 3: add Figure 4 for LLaMA family and the corresponding discussion;
‚Äì Section 5: add latest discussion about the synthetic data formatting of instruction tuning in Section 5.1.1, the empirical analysis for instruction tuning in Section 5.1.4, parameter-efficient model adaptation in Section 5.3 and memory-efficient adaptation in Section 5.4;
‚Äì Section 6: add latest discussion about the underlying mechanism of ICL 6.1.3, planning for complex task solving in Section 6.3;
‚Äì Section 7: add Table 10 for representative datasets for evaluating advanced abilities of LLMs, and empirical ability evaluation in Section 7.3.2;
‚Äì Section 8: add prompt design; ‚Äì Section 9: add the discussions on applications of
LLMs in finance and scientific research domains;
Planning Content. We will regularly include new content into this survey, to make it more self-contained and upto-date. Here, we list several potential topics that might appear in the next major version(s): (1) more experiments with larger language models for both instruction tuning and ability evaluation; (2) more detailed prompting practice; (3) training recipe; (4) more theoretical analysis and discussion; (5) more discussions on applications.
Clarifications on Experiments. In this version, we have included a number experiments on instruction-tuning (Table 8), overall ability evaluation (Table 11), and prompt

engineering (Table 12). Due to the limit of computational resources, our experiments are not complete, limited to small-sized models or a few comparisons. Despite that, we feel that it might be meaningful to share the partial results to the public. We will try to include the missing results of larger models or more comparisons in the future versions. We also call for support of computing power for conducting more comprehensive experiments.
Chinese Version. We also provide a translated Chinese version (corresponding to the first release) of this survey paper at the link: https://github.com/RUCAIBox/LLMSurvey/ blob/main/assets/LLM Survey Chinese V1.pdf. Four volunteers contribute to check and revise the content, and they are Yiwen Hu, Xinming Hou, Yanbin Yin, and Zhanshuo Cao (in order of contribution). We will also continuously update the Chinese version, but it may not be as timely as the latest English version.
ACKNOWLEDGMENTS
The authors would like to thank Yankai Lin and Yutao Zhu for proofreading this paper. Since the first release of this paper, we have received a number of valuable comments from the readers. We sincerely thank the readers who have written to us with constructive suggestions and comments: Tyler Suard, Damai Dai, Liang Ding, Stella Biderman, Kevin Gray, Jay Alammar, Yubo Feng, Mark Holmstrom, Xingdong Liu, Il-Seok Oh, Yiting Liu, Shaojun Wang, Gaoyan Ou, and Todd Morrill.
In this version (June 29, 2023), we add a large number of experiments and prompt practices. These new contents are completed by a number of volunteers in our team. Here, we add a special part to thank all the students who have worked very hard on this part (also including the ones on our author list).
Contribution on Experiments. We would like to sincerely thank the following people for their hard work involved in experiments shown in Table 11.
‚Ä¢ Xiaoxue Cheng: implement the experiments for evaluation on Language Generation and HaluEval tasks.
‚Ä¢ Yuhao Wang: implement the experiments for evaluation on interaction with environment tasks.
‚Ä¢ Bowen Zheng: implement the experiments for evaluation on tool manipulation tasks.
Contribution on Tips. We list the following guys for their contributions on the corresponding numbers of provided tips for designing prompts in Table 13.
‚Ä¢ Xiaolei Wang: T3, O3 ‚Ä¢ Beichen Zhang: D2, D5 ‚Ä¢ Zhipeng Chen: D3, D4 ‚Ä¢ Junjie Zhang: D6 ‚Ä¢ Bowen Zheng: D7 ‚Ä¢ Zican Dong: D8 ‚Ä¢ Xinyu Tang: C2 ‚Ä¢ Yifan Du: T4 ‚Ä¢ Tianyi Tang: O6, O7, D9 ‚Ä¢ Yupeng Hou: O8, C3

REFERENCES
[1] S. Pinker, The Language Instinct: How the Mind Creates Language. Brilliance Audio; Unabridged edition, 2014.
[2] M. D. Hauser, N. Chomsky, and W. T. Fitch, ‚ÄúThe faculty of language: what is it, who has it, and how did it evolve?‚Äù science, vol. 298, no. 5598, pp. 1569‚Äì 1579, 2002.
[3] A. M. Turing, ‚ÄúComputing machinery and intelligence,‚Äù Mind, vol. LIX, no. 236, pp. 433‚Äì460, 1950.
[4] F. Jelinek, Statistical Methods for Speech Recognition. MIT Press, 1998.
[5] J. Gao and C. Lin, ‚ÄúIntroduction to the special issue on statistical language modeling,‚Äù ACM Trans. Asian Lang. Inf. Process., vol. 3, no. 2, pp. 87‚Äì93, 2004.
[6] R. Rosenfeld, ‚ÄúTwo decades of statistical language modeling: Where do we go from here?‚Äù Proceedings of the IEEE, vol. 88, no. 8, pp. 1270‚Äì1278, 2000.
[7] A. Stolcke, ‚ÄúSrilm-an extensible language modeling toolkit,‚Äù in Seventh international conference on spoken language processing, 2002.
[8] X. Liu and W. B. Croft, ‚ÄúStatistical language modeling for information retrieval,‚Äù Annu. Rev. Inf. Sci. Technol., vol. 39, no. 1, pp. 1‚Äì31, 2005.
[9] C. Zhai, Statistical Language Models for Information Retrieval, ser. Synthesis Lectures on Human Language Technologies. Morgan & Claypool Publishers, 2008.
[10] S. M. Thede and M. P. Harper, ‚ÄúA second-order hidden markov model for part-of-speech tagging,‚Äù in 27th Annual Meeting of the Association for Computational Linguistics, University of Maryland, College Park, Maryland, USA, 20-26 June 1999, R. Dale and K. W. Church, Eds. ACL, 1999, pp. 175‚Äì182.
[11] L. R. Bahl, P. F. Brown, P. V. de Souza, and R. L. Mercer, ‚ÄúA tree-based statistical language model for natural language speech recognition,‚Äù IEEE Transactions on Acoustics, Speech, and Signal Processing, vol. 37, no. 7, pp. 1001‚Äì1008, 1989.
[12] T. Brants, A. C. Popat, P. Xu, F. J. Och, and J. Dean, ‚ÄúLarge language models in machine translation,‚Äù in EMNLP-CoNLL 2007, Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, June 28-30, 2007, Prague, Czech Republic, J. Eisner, Ed. ACL, 2007, pp. 858‚Äì867.
[13] S. M. Katz, ‚ÄúEstimation of probabilities from sparse data for the language model component of a speech recognizer,‚Äù IEEE Trans. Acoust. Speech Signal Process., vol. 35, no. 3, pp. 400‚Äì401, 1987.
[14] W. A. Gale and G. Sampson, ‚ÄúGood-turing frequency estimation without tears,‚Äù J. Quant. Linguistics, vol. 2, no. 3, pp. 217‚Äì237, 1995.
[15] Y. Bengio, R. Ducharme, P. Vincent, and C. Janvin, ‚ÄúA neural probabilistic language model,‚Äù J. Mach. Learn. Res., vol. 3, pp. 1137‚Äì1155, 2003.
[16] T. Mikolov, M. Karafia¬¥t, L. Burget, J. Cernocky¬¥ , and S. Khudanpur, ‚ÄúRecurrent neural network based language model,‚Äù in INTERSPEECH 2010, 11th Annual Conference of the International Speech Communication Association, Makuhari, Chiba, Japan, September 26-30,

59
2010, T. Kobayashi, K. Hirose, and S. Nakamura, Eds. ISCA, 2010, pp. 1045‚Äì1048. [17] S. Kombrink, T. Mikolov, M. Karafia¬¥t, and L. Burget, ‚ÄúRecurrent neural network based language modeling in meeting recognition,‚Äù in INTERSPEECH 2011, 12th Annual Conference of the International Speech Communication Association, Florence, Italy, August 27-31, 2011. ISCA, 2011, pp. 2877‚Äì2880. [18] R. Collobert, J. Weston, L. Bottou, M. Karlen, K. Kavukcuoglu, and P. P. Kuksa, ‚ÄúNatural language processing (almost) from scratch,‚Äù J. Mach. Learn. Res., vol. 12, pp. 2493‚Äì2537, 2011. [19] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean, ‚ÄúDistributed representations of words and phrases and their compositionality,‚Äù in Advances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Information Processing Systems 2013. Proceedings of a meeting held December 5-8, 2013, Lake Tahoe, Nevada, United States, C. J. C. Burges, L. Bottou, Z. Ghahramani, and K. Q. Weinberger, Eds., 2013, pp. 3111‚Äì3119. [20] T. Mikolov, K. Chen, G. Corrado, and J. Dean, ‚ÄúEfficient estimation of word representations in vector space,‚Äù in 1st International Conference on Learning Representations, ICLR 2013, Scottsdale, Arizona, USA, May 2-4, 2013, Workshop Track Proceedings, Y. Bengio and Y. LeCun, Eds., 2013. [21] M. E. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark, K. Lee, and L. Zettlemoyer, ‚ÄúDeep contextualized word representations,‚Äù in Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2018, New Orleans, Louisiana, USA, June 1-6, 2018, Volume 1 (Long Papers), M. A. Walker, H. Ji, and A. Stent, Eds. Association for Computational Linguistics, 2018, pp. 2227‚Äì2237. [22] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin, ‚ÄúAttention is all you need,‚Äù in Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 49, 2017, Long Beach, CA, USA, 2017, pp. 5998‚Äì6008. [23] J. Devlin, M. Chang, K. Lee, and K. Toutanova, ‚ÄúBERT: pre-training of deep bidirectional transformers for language understanding,‚Äù in Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers), J. Burstein, C. Doran, and T. Solorio, Eds. Association for Computational Linguistics, 2019, pp. 4171‚Äì4186. [24] M. Lewis, Y. Liu, N. Goyal, M. Ghazvininejad, A. Mohamed, O. Levy, V. Stoyanov, and L. Zettlemoyer, ‚ÄúBART: denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension,‚Äù in Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020, 2020, pp. 7871‚Äì7880. [25] W. Fedus, B. Zoph, and N. Shazeer, ‚ÄúSwitch transformers: Scaling to trillion parameter models with simple and efficient sparsity,‚Äù J. Mach. Learn. Res, pp.

60

1‚Äì40, 2021. [26] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei,
I. Sutskever et al., ‚ÄúLanguage models are unsupervised multitask learners,‚Äù OpenAI blog, p. 9, 2019. [27] Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis, L. Zettlemoyer, and V. Stoyanov, ‚ÄúRoberta: A robustly optimized BERT pretraining approach,‚Äù CoRR, vol. abs/1907.11692, 2019. [28] V. Sanh, A. Webson, C. Raffel, S. H. Bach, L. Sutawika, Z. Alyafeai, A. Chaffin, A. Stiegler, A. Raja, M. Dey, M. S. Bari, C. Xu, U. Thakker, S. S. Sharma, E. Szczechla, T. Kim, G. Chhablani, N. V. Nayak, D. Datta, J. Chang, M. T. Jiang, H. Wang, M. Manica, S. Shen, Z. X. Yong, H. Pandey, R. Bawden, T. Wang, T. Neeraj, J. Rozen, A. Sharma, A. Santilli, T. Fe¬¥vry, J. A. Fries, R. Teehan, T. L. Scao, S. Biderman, L. Gao, T. Wolf, and A. M. Rush, ‚ÄúMultitask prompted training enables zero-shot task generalization,‚Äù in The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022. [29] T. Wang, A. Roberts, D. Hesslow, T. L. Scao, H. W. Chung, I. Beltagy, J. Launay, and C. Raffel, ‚ÄúWhat language model architecture and pretraining objective works best for zero-shot generalization?‚Äù in International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA, ser. Proceedings of Machine Learning Research, vol. 162, 2022, pp. 22 964‚Äì22 984. [30] J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess, R. Child, S. Gray, A. Radford, J. Wu, and D. Amodei, ‚ÄúScaling laws for neural language models,‚Äù CoRR, vol. abs/2001.08361, 2020. [31] J. Wei, Y. Tay, R. Bommasani, C. Raffel, B. Zoph, S. Borgeaud, D. Yogatama, M. Bosma, D. Zhou, D. Metzler, E. H. Chi, T. Hashimoto, O. Vinyals, P. Liang, J. Dean, and W. Fedus, ‚ÄúEmergent abilities of large language models,‚Äù CoRR, vol. abs/2206.07682, 2022. [32] M. Shanahan, ‚ÄúTalking about large language models,‚Äù CoRR, vol. abs/2212.03551, 2022. [33] J. Wei, X. Wang, D. Schuurmans, M. Bosma, E. H. Chi, Q. Le, and D. Zhou, ‚ÄúChain of thought prompting elicits reasoning in large language models,‚Äù CoRR, vol. abs/2201.11903, 2022. [34] J. Hoffmann, S. Borgeaud, A. Mensch, E. Buchatskaya, T. Cai, E. Rutherford, D. de Las Casas, L. A. Hendricks, J. Welbl, A. Clark, T. Hennigan, E. Noland, K. Millican, G. van den Driessche, B. Damoc, A. Guy, S. Osindero, K. Simonyan, E. Elsen, J. W. Rae, O. Vinyals, and L. Sifre, ‚ÄúTraining compute-optimal large language models,‚Äù vol. abs/2203.15556, 2022. [35] R. Taylor, M. Kardas, G. Cucurull, T. Scialom, A. Hartshorn, E. Saravia, A. Poulton, V. Kerkez, and R. Stojnic, ‚ÄúGalactica: A large language model for science,‚Äù CoRR, vol. abs/2211.09085, 2022. [36] P. Liu, W. Yuan, J. Fu, Z. Jiang, H. Hayashi, and G. Neubig, ‚ÄúPre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing,‚Äù ACM Comput. Surv., pp. 195:1‚Äì195:35, 2023.

[37] C. Zhou, Q. Li, C. Li, J. Yu, Y. Liu, G. Wang, K. Zhang, C. Ji, Q. Yan, L. He, H. Peng, J. Li, J. Wu, Z. Liu, P. Xie, C. Xiong, J. Pei, P. S. Yu, and L. Sun, ‚ÄúA comprehensive survey on pretrained foundation models: A history from BERT to chatgpt,‚Äù CoRR, vol. abs/2302.09419, 2023.
[38] X. Han, Z. Zhang, N. Ding, Y. Gu, X. Liu, Y. Huo, J. Qiu, Y. Yao, A. Zhang, L. Zhang, W. Han, M. Huang, Q. Jin, Y. Lan, Y. Liu, Z. Liu, Z. Lu, X. Qiu, R. Song, J. Tang, J. Wen, J. Yuan, W. X. Zhao, and J. Zhu, ‚ÄúPretrained models: Past, present and future,‚Äù AI Open, vol. 2, pp. 225‚Äì250, 2021.
[39] X. Qiu, T. Sun, Y. Xu, Y. Shao, N. Dai, and X. Huang, ‚ÄúPre-trained models for natural language processing: A survey,‚Äù CoRR, vol. abs/2003.08271, 2020.
[40] S. Altman, ‚ÄúPlanning for agi and beyond,‚Äù OpenAI Blog, February 2023.
[41] S. Bubeck, V. Chandrasekaran, R. Eldan, J. Gehrke, E. Horvitz, E. Kamar, P. Lee, Y. T. Lee, Y. Li, S. Lundberg, H. Nori, H. Palangi, M. T. Ribeiro, and Y. Zhang, ‚ÄúSparks of artificial general intelligence: Early experiments with gpt-4,‚Äù vol. abs/2303.12712, 2023.
[42] S. Huang, L. Dong, W. Wang, Y. Hao, S. Singhal, S. Ma, T. Lv, L. Cui, O. K. Mohammed, B. Patra, Q. Liu, K. Aggarwal, Z. Chi, J. Bjorck, V. Chaudhary, S. Som, X. Song, and F. Wei, ‚ÄúLanguage is not all you need: Aligning perception with language models,‚Äù CoRR, vol. abs/2302.14045, 2023.
[43] Y. Cao, S. Li, Y. Liu, Z. Yan, Y. Dai, P. S. Yu, and L. Sun, ‚ÄúA comprehensive survey of ai-generated content (aigc): A history of generative ai from gan to chatgpt,‚Äù arXiv preprint arXiv:2303.04226, 2023.
[44] D. Driess, F. Xia, M. S. Sajjadi, C. Lynch, A. Chowdhery, B. Ichter, A. Wahid, J. Tompson, Q. Vuong, T. Yu et al., ‚ÄúPalm-e: An embodied multimodal language model,‚Äù arXiv preprint arXiv:2303.03378, 2023.
[45] C. Wu, S. Yin, W. Qi, X. Wang, Z. Tang, and N. Duan, ‚ÄúVisual chatgpt: Talking, drawing and editing with visual foundation models,‚Äù arXiv preprint arXiv:2303.04671, 2023.
[46] OpenAI, ‚ÄúGpt-4 technical report,‚Äù OpenAI, 2023. [47] Y. Fu, H. Peng, and T. Khot, ‚ÄúHow does gpt obtain its
ability? tracing emergent abilities of language models to their sources,‚Äù Yao Fu‚Äôs Notion, Dec 2022. [48] J. Li, T. Tang, W. X. Zhao, and J. Wen, ‚ÄúPretrained language model for text generation: A survey,‚Äù in Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI 2021, Virtual Event / Montreal, Canada, 19-27 August 2021, Z. Zhou, Ed. ijcai.org, 2021, pp. 4492‚Äì4499. [49] P. Lu, L. Qiu, W. Yu, S. Welleck, and K. Chang, ‚ÄúA survey of deep learning for mathematical reasoning,‚Äù CoRR, vol. abs/2212.10535, 2022. [50] Q. Dong, L. Li, D. Dai, C. Zheng, Z. Wu, B. Chang, X. Sun, J. Xu, L. Li, and Z. Sui, ‚ÄúA survey for in-context learning,‚Äù CoRR, vol. abs/2301.00234, 2023. [51] J. Huang and K. C. Chang, ‚ÄúTowards reasoning in large language models: A survey,‚Äù CoRR, vol. abs/2212.10403, 2022. [52] S. Qiao, Y. Ou, N. Zhang, X. Chen, Y. Yao, S. Deng, C. Tan, F. Huang, and H. Chen, ‚ÄúReasoning with

61

language model prompting: A survey,‚Äù CoRR, vol. abs/2212.09597, 2022. [53] J. Zhou, P. Ke, X. Qiu, M. Huang, and J. Zhang, ‚ÄúChatgpt: potential, prospects, and limitations,‚Äù in Frontiers of Information Technology & Electronic Engineering, 2023, pp. 1‚Äì6. [54] W. X. Zhao, J. Liu, R. Ren, and J. Wen, ‚ÄúDense text retrieval based on pretrained language models: A survey,‚Äù CoRR, vol. abs/2211.14876, 2022. [55] T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger, T. Henighan, R. Child, A. Ramesh, D. M. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray, B. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever, and D. Amodei, ‚ÄúLanguage models are few-shot learners,‚Äù in Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan, and H. Lin, Eds., 2020. [56] A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts, P. Barham, H. W. Chung, C. Sutton, S. Gehrmann, P. Schuh, K. Shi, S. Tsvyashchenko, J. Maynez, A. Rao, P. Barnes, Y. Tay, N. Shazeer, V. Prabhakaran, E. Reif, N. Du, B. Hutchinson, R. Pope, J. Bradbury, J. Austin, M. Isard, G. Gur-Ari, P. Yin, T. Duke, A. Levskaya, S. Ghemawat, S. Dev, H. Michalewski, X. Garcia, V. Misra, K. Robinson, L. Fedus, D. Zhou, D. Ippolito, D. Luan, H. Lim, B. Zoph, A. Spiridonov, R. Sepassi, D. Dohan, S. Agrawal, M. Omernick, A. M. Dai, T. S. Pillai, M. Pellat, A. Lewkowycz, E. Moreira, R. Child, O. Polozov, K. Lee, Z. Zhou, X. Wang, B. Saeta, M. Diaz, O. Firat, M. Catasta, J. Wei, K. MeierHellstern, D. Eck, J. Dean, S. Petrov, and N. Fiedel, ‚ÄúPalm: Scaling language modeling with pathways,‚Äù CoRR, vol. abs/2204.02311, 2022. [57] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M. Lachaux, T. Lacroix, B. Rozie`re, N. Goyal, E. Hambro, F. Azhar, A. Rodriguez, A. Joulin, E. Grave, and G. Lample, ‚ÄúLlama: Open and efficient foundation language models,‚Äù CoRR, 2023. [58] B. A. Huberman and T. Hogg, ‚ÄúPhase transitions in artificial intelligence systems,‚Äù Artificial Intelligence, vol. 33, no. 2, pp. 155‚Äì171, 1987. [59] J. W. Rae, S. Borgeaud, T. Cai, K. Millican, J. Hoffmann, H. F. Song, J. Aslanides, S. Henderson, R. Ring, S. Young, E. Rutherford, T. Hennigan, J. Menick, A. Cassirer, R. Powell, G. van den Driessche, L. A. Hendricks, M. Rauh, P. Huang, A. Glaese, J. Welbl, S. Dathathri, S. Huang, J. Uesato, J. Mellor, I. Higgins, A. Creswell, N. McAleese, A. Wu, E. Elsen, S. M. Jayakumar, E. Buchatskaya, D. Budden, E. Sutherland, K. Simonyan, M. Paganini, L. Sifre, L. Martens, X. L. Li, A. Kuncoro, A. Nematzadeh, E. Gribovskaya, D. Donato, A. Lazaridou, A. Mensch, J. Lespiau, M. Tsimpoukelli, N. Grigorev, D. Fritz, T. Sottiaux, M. Pajarskas, T. Pohlen, Z. Gong, D. Toyama, C. de Masson d‚ÄôAutume, Y. Li, T. Terzi, V. Mikulik,

I. Babuschkin, A. Clark, D. de Las Casas, A. Guy, C. Jones, J. Bradbury, M. J. Johnson, B. A. Hechtman, L. Weidinger, I. Gabriel, W. S. Isaac, E. Lockhart, S. Osindero, L. Rimell, C. Dyer, O. Vinyals, K. Ayoub, J. Stanway, L. Bennett, D. Hassabis, K. Kavukcuoglu, and G. Irving, ‚ÄúScaling language models: Methods, analysis & insights from training gopher,‚Äù CoRR, vol. abs/2112.11446, 2021. [60] D. Dai, Y. Sun, L. Dong, Y. Hao, Z. Sui, and F. Wei, ‚ÄúWhy can GPT learn in-context? language models secretly perform gradient descent as meta-optimizers,‚Äù CoRR, vol. abs/2212.10559, 2022. [61] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. L. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray, J. Schulman, J. Hilton, F. Kelton, L. Miller, M. Simens, A. Askell, P. Welinder, P. F. Christiano, J. Leike, and R. Lowe, ‚ÄúTraining language models to follow instructions with human feedback,‚Äù CoRR, vol. abs/2203.02155, 2022. [62] J. Wei, M. Bosma, V. Y. Zhao, K. Guu, A. W. Yu, B. Lester, N. Du, A. M. Dai, and Q. V. Le, ‚ÄúFinetuned language models are zero-shot learners,‚Äù in The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022. [63] R. Thoppilan, D. D. Freitas, J. Hall, N. Shazeer, A. Kulshreshtha, H. Cheng, A. Jin, T. Bos, L. Baker, Y. Du, Y. Li, H. Lee, H. S. Zheng, A. Ghafouri, M. Menegali, Y. Huang, M. Krikun, D. Lepikhin, J. Qin, D. Chen, Y. Xu, Z. Chen, A. Roberts, M. Bosma, Y. Zhou, C. Chang, I. Krivokon, W. Rusch, M. Pickett, K. S. Meier-Hellstern, M. R. Morris, T. Doshi, R. D. Santos, T. Duke, J. Soraker, B. Zevenbergen, V. Prabhakaran, M. Diaz, B. Hutchinson, K. Olson, A. Molina, E. Hoffman-John, J. Lee, L. Aroyo, R. Rajakumar, A. Butryna, M. Lamm, V. Kuzmina, J. Fenton, A. Cohen, R. Bernstein, R. Kurzweil, B. Aguera-Arcas, C. Cui, M. Croak, E. H. Chi, and Q. Le, ‚ÄúLamda: Language models for dialog applications,‚Äù CoRR, vol. abs/2201.08239, 2022. [64] H. W. Chung, L. Hou, S. Longpre, B. Zoph, Y. Tay, W. Fedus, E. Li, X. Wang, M. Dehghani, S. Brahma, A. Webson, S. S. Gu, Z. Dai, M. Suzgun, X. Chen, A. Chowdhery, S. Narang, G. Mishra, A. Yu, V. Y. Zhao, Y. Huang, A. M. Dai, H. Yu, S. Petrov, E. H. Chi, J. Dean, J. Devlin, A. Roberts, D. Zhou, Q. V. Le, and J. Wei, ‚ÄúScaling instruction-finetuned language models,‚Äù CoRR, vol. abs/2210.11416, 2022. [65] J. Rasley, S. Rajbhandari, O. Ruwase, and Y. He, ‚ÄúDeepspeed: System optimizations enable training deep learning models with over 100 billion parameters,‚Äù in KDD, 2020, pp. 3505‚Äì3506. [66] M. Shoeybi, M. Patwary, R. Puri, P. LeGresley, J. Casper, and B. Catanzaro, ‚ÄúMegatron-lm: Training multi-billion parameter language models using model parallelism,‚Äù CoRR, vol. abs/1909.08053, 2019. [67] D. Narayanan, M. Shoeybi, J. Casper, P. LeGresley, M. Patwary, V. Korthikanti, D. Vainbrand, P. Kashinkunti, J. Bernauer, B. Catanzaro, A. Phanishayee, and M. Zaharia, ‚ÄúEfficient large-scale language model training on GPU clusters using

62

megatron-lm,‚Äù in International Conference for High Performance Computing, Networking, Storage and Analysis, SC 2021, St. Louis, Missouri, USA, November 14-19, 2021. ACM, 2021, p. 58. [68] V. Korthikanti, J. Casper, S. Lym, L. McAfee, M. Andersch, M. Shoeybi, and B. Catanzaro, ‚ÄúReducing activation recomputation in large transformer models,‚Äù CoRR, vol. abs/2205.05198, 2022. [69] T. L. Scao, A. Fan, C. Akiki, E. Pavlick, S. Ilic, D. Hesslow, R. Castagne¬¥, A. S. Luccioni, F. Yvon, M. Galle¬¥, J. Tow, A. M. Rush, S. Biderman, A. Webson, P. S. Ammanamanchi, T. Wang, B. Sagot, N. Muennighoff, A. V. del Moral, O. Ruwase, R. Bawden, S. Bekman, A. McMillan-Major, I. Beltagy, H. Nguyen, L. Saulnier, S. Tan, P. O. Suarez, V. Sanh, H. Laurenc¬∏on, Y. Jernite, J. Launay, M. Mitchell, C. Raffel, A. Gokaslan, A. Simhi, A. Soroa, A. F. Aji, A. Alfassy, A. Rogers, A. K. Nitzav, C. Xu, C. Mou, C. Emezue, C. Klamm, C. Leong, D. van Strien, D. I. Adelani, and et al., ‚ÄúBLOOM: A 176b-parameter open-access multilingual language model,‚Äù CoRR, vol. abs/2211.05100, 2022. [70] P. F. Christiano, J. Leike, T. B. Brown, M. Martic, S. Legg, and D. Amodei, ‚ÄúDeep reinforcement learning from human preferences,‚Äù in Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, I. Guyon, U. von Luxburg, S. Bengio, H. M. Wallach, R. Fergus, S. V. N. Vishwanathan, and R. Garnett, Eds., 2017, pp. 4299‚Äì 4307. [71] T. Schick, J. Dwivedi-Yu, R. Dess`ƒ±, R. Raileanu, M. Lomeli, L. Zettlemoyer, N. Cancedda, and T. Scialom, ‚ÄúToolformer: Language models can teach themselves to use tools,‚Äù CoRR, vol. abs/2302.04761, 2023. [72] R. Nakano, J. Hilton, S. Balaji, J. Wu, L. Ouyang, C. Kim, C. Hesse, S. Jain, V. Kosaraju, W. Saunders, X. Jiang, K. Cobbe, T. Eloundou, G. Krueger, K. Button, M. Knight, B. Chess, and J. Schulman, ‚ÄúWebgpt: Browser-assisted question-answering with human feedback,‚Äù CoRR, vol. abs/2112.09332, 2021. [73] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and P. J. Liu, ‚ÄúExploring the limits of transfer learning with a unified textto-text transformer,‚Äù J. Mach. Learn. Res., pp. 140:1‚Äì 140:67, 2020. [74] L. Xue, N. Constant, A. Roberts, M. Kale, R. AlRfou, A. Siddhant, A. Barua, and C. Raffel, ‚Äúmt5: A massively multilingual pre-trained text-to-text transformer,‚Äù in Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021, Online, June 6-11, 2021, 2021, pp. 483‚Äì498. [75] W. Zeng, X. Ren, T. Su, H. Wang, Y. Liao, Z. Wang, X. Jiang, Z. Yang, K. Wang, X. Zhang, C. Li, Z. Gong, Y. Yao, X. Huang, J. Wang, J. Yu, Q. Guo, Y. Yu, Y. Zhang, J. Wang, H. Tao, D. Yan, Z. Yi, F. Peng, F. Jiang, H. Zhang, L. Deng, Y. Zhang, Z. Lin, C. Zhang, S. Zhang, M. Guo, S. Gu, G. Fan, Y. Wang, X. Jin, Q. Liu, and Y. Tian, ‚ÄúPangu-Œ±:

Large-scale autoregressive pretrained chinese language models with auto-parallel computation,‚Äù CoRR, vol. abs/2104.12369, 2021. [76] Z. Zhang, Y. Gu, X. Han, S. Chen, C. Xiao, Z. Sun, Y. Yao, F. Qi, J. Guan, P. Ke, Y. Cai, G. Zeng, Z. Tan, Z. Liu, M. Huang, W. Han, Y. Liu, X. Zhu, and M. Sun, ‚ÄúCPM-2: large-scale cost-effective pre-trained language models,‚Äù CoRR, vol. abs/2106.10715, 2021. [77] E. Nijkamp, B. Pang, H. Hayashi, L. Tu, H. Wang, Y. Zhou, S. Savarese, and C. Xiong, ‚ÄúCodegen: An open large language model for code with mtulti-turn program synthesis,‚Äù arXiv preprint arXiv:2203.13474, 2022. [78] S. Black, S. Biderman, E. Hallahan, Q. Anthony, L. Gao, L. Golding, H. He, C. Leahy, K. McDonell, J. Phang, M. Pieler, U. S. Prashanth, S. Purohit, L. Reynolds, J. Tow, B. Wang, and S. Weinbach, ‚ÄúGptneox-20b: An open-source autoregressive language model,‚Äù CoRR, vol. abs/2204.06745, 2022. [79] Y. Wang, S. Mishra, P. Alipoormolabashi, Y. Kordi, A. Mirzaei, A. Naik, A. Ashok, A. S. Dhanasekaran, A. Arunkumar, D. Stap, E. Pathak, G. Karamanolakis, H. G. Lai, I. Purohit, I. Mondal, J. Anderson, K. Kuznia, K. Doshi, K. K. Pal, M. Patel, M. Moradshahi, M. Parmar, M. Purohit, N. Varshney, P. R. Kaza, P. Verma, R. S. Puri, R. Karia, S. Doshi, S. K. Sampat, S. Mishra, S. R. A, S. Patro, T. Dixit, and X. Shen, ‚ÄúSuper-naturalinstructions: Generalization via declarative instructions on 1600+ NLP tasks,‚Äù in Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022, 2022, pp. 5085‚Äì 5109. [80] Y. Tay, M. Dehghani, V. Q. Tran, X. Garc¬¥ƒ±a, J. Wei, X. Wang, H. W. Chung, D. Bahri, T. Schuster, H. Zheng, D. Zhou, N. Houlsby, and D. Metzler, ‚ÄúUl2: Unifying language learning paradigms,‚Äù 2022. [81] S. Zhang, S. Roller, N. Goyal, M. Artetxe, M. Chen, S. Chen, C. Dewan, M. T. Diab, X. Li, X. V. Lin, T. Mihaylov, M. Ott, S. Shleifer, K. Shuster, D. Simig, P. S. Koura, A. Sridhar, T. Wang, and L. Zettlemoyer, ‚ÄúOPT: open pre-trained transformer language models,‚Äù CoRR, vol. abs/2205.01068, 2022. [82] M. R. Costa-jussa`, J. Cross, O. C¬∏ elebi, M. Elbayad, K. Heafield, K. Heffernan, E. Kalbassi, J. Lam, D. Licht, J. Maillard, A. Sun, S. Wang, G. Wenzek, A. Youngblood, B. Akula, L. Barrault, G. M. Gonzalez, P. Hansanti, J. Hoffman, S. Jarrett, K. R. Sadagopan, D. Rowe, S. Spruit, C. Tran, P. Andrews, N. F. Ayan, S. Bhosale, S. Edunov, A. Fan, C. Gao, V. Goswami, F. Guzma¬¥n, P. Koehn, A. Mourachko, C. Ropers, S. Saleem, H. Schwenk, and J. Wang, ‚ÄúNo language left behind: Scaling human-centered machine translation,‚Äù CoRR, vol. abs/2207.04672, 2022. [83] A. Zeng, X. Liu, Z. Du, Z. Wang, H. Lai, M. Ding, Z. Yang, Y. Xu, W. Zheng, X. Xia, W. L. Tam, Z. Ma, Y. Xue, J. Zhai, W. Chen, P. Zhang, Y. Dong, and J. Tang, ‚ÄúGLM-130B: an open bilingual pre-trained model,‚Äù vol. abs/2210.02414, 2022. [84] N. Muennighoff, T. Wang, L. Sutawika, A. Roberts, S. Biderman, T. L. Scao, M. S. Bari, S. Shen, Z. X. Yong,

63

H. Schoelkopf, X. Tang, D. Radev, A. F. Aji, K. Almubarak, S. Albanie, Z. Alyafeai, A. Webson, E. Raff, and C. Raffel, ‚ÄúCrosslingual generalization through multitask finetuning,‚Äù CoRR, vol. abs/2211.01786, 2022. [85] S. Iyer, X. V. Lin, R. Pasunuru, T. Mihaylov, D. Simig, P. Yu, K. Shuster, T. Wang, Q. Liu, P. S. Koura, X. Li, B. O‚ÄôHoro, G. Pereyra, J. Wang, C. Dewan, A. Celikyilmaz, L. Zettlemoyer, and V. Stoyanov, ‚ÄúOPT-IML: scaling language model instruction meta learning through the lens of generalization,‚Äù CoRR, vol. abs/2212.12017, 2022. [86] Q. Zheng, X. Xia, X. Zou, Y. Dong, S. Wang, Y. Xue, Z. Wang, L. Shen, A. Wang, Y. Li et al., ‚ÄúCodegeex: A pre-trained model for code generation with multilingual evaluations on humaneval-x,‚Äù arXiv preprint arXiv:2303.17568, 2023. [87] S. Biderman, H. Schoelkopf, Q. Anthony, H. Bradley, K. O‚ÄôBrien, E. Hallahan, M. A. Khan, S. Purohit, U. S. Prashanth, E. Raff et al., ‚ÄúPythia: A suite for analyzing large language models across training and scaling,‚Äù arXiv preprint arXiv:2304.01373, 2023. [88] D. Lepikhin, H. Lee, Y. Xu, D. Chen, O. Firat, Y. Huang, M. Krikun, N. Shazeer, and Z. Chen, ‚ÄúGshard: Scaling giant models with conditional computation and automatic sharding,‚Äù in 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021, 2021. [89] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. de Oliveira Pinto, J. Kaplan, H. Edwards, Y. Burda, N. Joseph, G. Brockman, A. Ray, R. Puri, G. Krueger, M. Petrov, H. Khlaaf, G. Sastry, P. Mishkin, B. Chan, S. Gray, N. Ryder, M. Pavlov, A. Power, L. Kaiser, M. Bavarian, C. Winter, P. Tillet, F. P. Such, D. Cummings, M. Plappert, F. Chantzis, E. Barnes, A. HerbertVoss, W. H. Guss, A. Nichol, A. Paino, N. Tezak, J. Tang, I. Babuschkin, S. Balaji, S. Jain, W. Saunders, C. Hesse, A. N. Carr, J. Leike, J. Achiam, V. Misra, E. Morikawa, A. Radford, M. Knight, M. Brundage, M. Murati, K. Mayer, P. Welinder, B. McGrew, D. Amodei, S. McCandlish, I. Sutskever, and W. Zaremba, ‚ÄúEvaluating large language models trained on code,‚Äù CoRR, vol. abs/2107.03374, 2021. [90] Y. Sun, S. Wang, S. Feng, S. Ding, C. Pang, J. Shang, J. Liu, X. Chen, Y. Zhao, Y. Lu, W. Liu, Z. Wu, W. Gong, J. Liang, Z. Shang, P. Sun, W. Liu, X. Ouyang, D. Yu, H. Tian, H. Wu, and H. Wang, ‚ÄúERNIE 3.0: Large-scale knowledge enhanced pre-training for language understanding and generation,‚Äù CoRR, vol. abs/2107.02137, 2021. [91] O. Lieber, O. Sharir, B. Lenz, and Y. Shoham, ‚ÄúJurassic1: Technical details and evaluation,‚Äù White Paper. AI21 Labs, vol. 1, 2021. [92] B. Kim, H. Kim, S. Lee, G. Lee, D. Kwak, D. H. Jeon, S. Park, S. Kim, S. Kim, D. Seo, H. Lee, M. Jeong, S. Lee, M. Kim, S. Ko, S. Kim, T. Park, J. Kim, S. Kang, N. Ryu, K. M. Yoo, M. Chang, S. Suh, S. In, J. Park, K. Kim, H. Kim, J. Jeong, Y. G. Yeo, D. Ham, D. Park, M. Y. Lee, J. Kang, I. Kang, J. Ha, W. Park, and N. Sung, ‚ÄúWhat changes can large-scale language models bring? intensive study on hyperclova: Billions-

scale korean generative pretrained transformers,‚Äù in Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 7-11 November, 2021. Association for Computational Linguistics, 2021. [93] S. Wu, X. Zhao, T. Yu, R. Zhang, C. Shen, H. Liu, F. Li, H. Zhu, J. Luo, L. Xu et al., ‚ÄúYuan 1.0: Large-scale pre-trained language model in zero-shot and few-shot learning,‚Äù arXiv preprint arXiv:2110.04725, 2021. [94] A. Askell, Y. Bai, A. Chen, D. Drain, D. Ganguli, T. Henighan, A. Jones, N. Joseph, B. Mann, N. DasSarma, N. Elhage, Z. Hatfield-Dodds, D. Hernandez, J. Kernion, K. Ndousse, C. Olsson, D. Amodei, T. B. Brown, J. Clark, S. McCandlish, C. Olah, and J. Kaplan, ‚ÄúA general language assistant as a laboratory for alignment,‚Äù CoRR, vol. abs/2112.00861, 2021. [95] S. Wang, Y. Sun, Y. Xiang, Z. Wu, S. Ding, W. Gong, S. Feng, J. Shang, Y. Zhao, C. Pang, J. Liu, X. Chen, Y. Lu, W. Liu, X. Wang, Y. Bai, Q. Chen, L. Zhao, S. Li, P. Sun, D. Yu, Y. Ma, H. Tian, H. Wu, T. Wu, W. Zeng, G. Li, W. Gao, and H. Wang, ‚ÄúERNIE 3.0 titan: Exploring larger-scale knowledge enhanced pretraining for language understanding and generation,‚Äù CoRR, vol. abs/2112.12731, 2021. [96] N. Du, Y. Huang, A. M. Dai, S. Tong, D. Lepikhin, Y. Xu, M. Krikun, Y. Zhou, A. W. Yu, O. Firat, B. Zoph, L. Fedus, M. P. Bosma, Z. Zhou, T. Wang, Y. E. Wang, K. Webster, M. Pellat, K. Robinson, K. S. MeierHellstern, T. Duke, L. Dixon, K. Zhang, Q. V. Le, Y. Wu, Z. Chen, and C. Cui, ‚ÄúGlam: Efficient scaling of language models with mixture-of-experts,‚Äù in International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA, 2022, pp. 5547‚Äì5569. [97] S. Smith, M. Patwary, B. Norick, P. LeGresley, S. Rajbhandari, J. Casper, Z. Liu, S. Prabhumoye, G. Zerveas, V. Korthikanti, E. Zheng, R. Child, R. Y. Aminabadi, J. Bernauer, X. Song, M. Shoeybi, Y. He, M. Houston, S. Tiwary, and B. Catanzaro, ‚ÄúUsing deepspeed and megatron to train megatron-turing NLG 530b, A large-scale generative language model,‚Äù CoRR, vol. abs/2201.11990, 2022. [98] Y. Li, D. H. Choi, J. Chung, N. Kushman, J. Schrittwieser, R. Leblond, T. Eccles, J. Keeling, F. Gimeno, A. D. Lago, T. Hubert, P. Choy, C. de Masson d‚ÄôAutume, I. Babuschkin, X. Chen, P. Huang, J. Welbl, S. Gowal, A. Cherepanov, J. Molloy, D. J. Mankowitz, E. S. Robson, P. Kohli, N. de Freitas, K. Kavukcuoglu, and O. Vinyals, ‚ÄúCompetition-level code generation with alphacode,‚Äù Science, 2022. [99] S. Soltan, S. Ananthakrishnan, J. FitzGerald, R. Gupta, W. Hamza, H. Khan, C. Peris, S. Rawls, A. Rosenbaum, A. Rumshisky, C. S. Prakash, M. Sridhar, F. Triefenbach, A. Verma, G. Tu¬® r, and P. Natarajan, ‚ÄúAlexatm 20b: Few-shot learning using a large-scale multilingual seq2seq model,‚Äù CoRR, vol. abs/2208.01448, 2022. [100] A. Glaese, N. McAleese, M. Trebacz, J. Aslanides, V. Firoiu, T. Ewalds, M. Rauh, L. Weidinger, M. Chadwick, P. Thacker, L. Campbell-Gillingham, J. Ue-

64

sato, P. Huang, R. Comanescu, F. Yang, A. See, S. Dathathri, R. Greig, C. Chen, D. Fritz, J. S. Elias, R. Green, S. Mokra¬¥, N. Fernando, B. Wu, R. Foley, S. Young, I. Gabriel, W. Isaac, J. Mellor, D. Hassabis, K. Kavukcuoglu, L. A. Hendricks, and G. Irving, ‚ÄúImproving alignment of dialogue agents via targeted human judgements,‚Äù CoRR, vol. abs/2209.14375, 2022. [101] H. Su, X. Zhou, H. Yu, Y. Chen, Z. Zhu, Y. Yu, and J. Zhou, ‚ÄúWelm: A well-read pre-trained language model for chinese,‚Äù CoRR, vol. abs/2209.10372, 2022. [102] Y. Tay, J. Wei, H. W. Chung, V. Q. Tran, D. R. So, S. Shakeri, X. Garcia, H. S. Zheng, J. Rao, A. Chowdhery, D. Zhou, D. Metzler, S. Petrov, N. Houlsby, Q. V. Le, and M. Dehghani, ‚ÄúTranscending scaling laws with 0.1% extra compute,‚Äù CoRR, vol. abs/2210.11399, 2022. [103] X. Ren, P. Zhou, X. Meng, X. Huang, Y. Wang, W. Wang, P. Li, X. Zhang, A. Podolskiy, G. Arshinov, A. Bout, I. Piontkovskaya, J. Wei, X. Jiang, T. Su, Q. Liu, and J. Yao, ‚ÄúPangu-Œ£: Towards trillion parameter language model with sparse heterogeneous computing,‚Äù CoRR, vol. abs/2303.10845, 2023. [104] A. Radford, R. Jo¬¥ zefowicz, and I. Sutskever, ‚ÄúLearning to generate reviews and discovering sentiment,‚Äù CoRR, vol. abs/1704.01444, 2017. [105] A. Radford, K. Narasimhan, T. Salimans, I. Sutskever et al., ‚ÄúImproving language understanding by generative pre-training,‚Äù 2018. [106] B. McCann, N. S. Keskar, C. Xiong, and R. Socher, ‚ÄúThe natural language decathlon: Multitask learning as question answering,‚Äù CoRR, vol. abs/1806.08730, 2018. [107] Y. Zhang, S. Sun, M. Galley, Y. Chen, C. Brockett, X. Gao, J. Gao, J. Liu, and B. Dolan, ‚ÄúDIALOGPT : Large-scale generative pre-training for conversational response generation,‚Äù in Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations, ACL 2020, Online, July 5-10, 2020, A. Celikyilmaz and T. Wen, Eds. Association for Computational Linguistics, 2020, pp. 270‚Äì278. [108] D. Ham, J. Lee, Y. Jang, and K. Kim, ‚ÄúEnd-to-end neural pipeline for goal-oriented dialogue systems using GPT-2,‚Äù in Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020. Association for Computational Linguistics, 2020, pp. 583‚Äì592. [109] I. Drori, S. Tran, R. Wang, N. Cheng, K. Liu, L. Tang, E. Ke, N. Singh, T. L. Patti, J. Lynch, A. Shporer, N. Verma, E. Wu, and G. Strang, ‚ÄúA neural network solves and generates mathematics problems by program synthesis: Calculus, differential equations, linear algebra, and more,‚Äù CoRR, vol. abs/2112.15594, 2021. [110] A. Neelakantan, T. Xu, R. Puri, A. Radford, J. M. Han, J. Tworek, Q. Yuan, N. Tezak, J. W. Kim, C. Hallacy, J. Heidecke, P. Shyam, B. Power, T. E. Nekoul, G. Sastry, G. Krueger, D. Schnurr, F. P. Such, K. Hsu, M. Thompson, T. Khan, T. Sherbakov, J. Jang, P. Welinder, and L. Weng, ‚ÄúText and code embeddings by contrastive pre-training,‚Äù CoRR, vol. abs/2201.10005, 2022. [111] J. Schulman, F. Wolski, P. Dhariwal, A. Radford,

and O. Klimov, ‚ÄúProximal policy optimization algorithms,‚Äù arXiv preprint arXiv:1707.06347, 2017. [112] N. Stiennon, L. Ouyang, J. Wu, D. M. Ziegler, R. Lowe, C. Voss, A. Radford, D. Amodei, and P. F. Christiano, ‚ÄúLearning to summarize from human feedback,‚Äù CoRR, vol. abs/2009.01325, 2020. [113] OpenAI, ‚ÄúOur approach to alignment research,‚Äù OpenAI Blog, August 2022. [114] ‚Äî‚Äî, ‚ÄúIntroducing chatgpt,‚Äù OpenAI Blog, November 2022. [115] D. Ganguli, L. Lovitt, J. Kernion, A. Askell, Y. Bai, S. Kadavath, B. Mann, E. Perez, N. Schiefer, K. Ndousse, A. Jones, S. Bowman, A. Chen, T. Conerly, N. DasSarma, D. Drain, N. Elhage, S. E. Showk, S. Fort, Z. Hatfield-Dodds, T. Henighan, D. Hernandez, T. Hume, J. Jacobson, S. Johnston, S. Kravec, C. Olsson, S. Ringer, E. Tran-Johnson, D. Amodei, T. Brown, N. Joseph, S. McCandlish, C. Olah, J. Kaplan, and J. Clark, ‚ÄúRed teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned,‚Äù CoRR, vol. abs/2209.07858, 2022. [116] OpenAI, ‚ÄúLessons learned on language model safety and misuse,‚Äù OpenAI Blog, March 2022. [117] E. Almazrouei, H. Alobeidli, A. Alshamsi, A. Cappelli, R. Cojocaru, M. Debbah, E. Goffinet, D. Heslow, J. Launay, Q. Malartic, B. Noune, B. Pannier, and G. Penedo, ‚ÄúFalcon-40B: an open large language model with state-of-the-art performance,‚Äù 2023. [118] L. Huawei Technologies Co., ‚ÄúHuawei mindspore ai development framework,‚Äù in Artificial Intelligence Technology. Springer, 2022, pp. 137‚Äì162. [119] R. Taori, I. Gulrajani, T. Zhang, Y. Dubois, X. Li, C. Guestrin, P. Liang, and T. B. Hashimoto, ‚ÄúStanford alpaca: An instruction-following llama model,‚Äù https://github.com/tatsu-lab/stanford alpaca, 2023. [120] W.-L. Chiang, Z. Li, Z. Lin, Y. Sheng, Z. Wu, H. Zhang, L. Zheng, S. Zhuang, Y. Zhuang, J. E. Gonzalez, I. Stoica, and E. P. Xing, ‚ÄúVicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality,‚Äù 2023. [Online]. Available: https://vicuna.lmsys.org [121] 2023. [Online]. Available: https://github.com/ nebuly-ai/nebullvm/tree/main/apps/accelerate/ chatllama [122] Y. You, ‚ÄúColossalchat: An open-source solution for cloning chatgpt with a complete rlhf pipeline,‚Äù 2023. [Online]. Available: https://medium.com/@yangyou berkeley/ colossalchat-an-open-source-solution-for-cloningchatgpt-with-a-complete-rlhf-pipeline-5edf08fb538b [123] G. Penedo, Q. Malartic, D. Hesslow, R. Cojocaru, A. Cappelli, H. Alobeidli, B. Pannier, E. Almazrouei, and J. Launay, ‚ÄúThe RefinedWeb dataset for Falcon LLM: outperforming curated corpora with web data, and web data only,‚Äù arXiv preprint arXiv:2306.01116, 2023. [124] R. Taori, I. Gulrajani, T. Zhang, Y. Dubois, X. Li, C. Guestrin, P. Liang, and T. B. Hashimoto, ‚ÄúStanford alpaca: An instruction-following llama model,‚Äù https://github.com/tatsu-lab/stanford alpaca, 2023. [125] Y. Wang, Y. Kordi, S. Mishra, A. Liu, N. A. Smith, D. Khashabi, and H. Hajishirzi, ‚ÄúSelf-instruct: Align-

65

ing language model with self generated instructions,‚Äù CoRR, vol. abs/2212.10560, 2022. [126] Alpaca-LoRA, ‚ÄúInstruct-tune llama on consumer hardware,‚Äù https://github.com/tloen/alpaca-lora, 2023. [127] E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, and W. Chen, ‚ÄúLora: Low-rank adaptation of large language models,‚Äù in The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022. [128] X. Geng, A. Gudibande, H. Liu, E. Wallace, P. Abbeel, S. Levine, and D. Song, ‚ÄúKoala: A dialogue model for academic research,‚Äù Blog post, April 2023. [129] Y. Ji, Y. Deng, Y. Gong, Y. Peng, Q. Niu, B. Ma, and X. Li, ‚ÄúBelle: Be everyone‚Äôs large language model engine,‚Äù https://github.com/LianjiaTech/BELLE, 2023. [130] H. Liu, C. Li, Q. Wu, and Y. J. Lee, ‚ÄúVisual instruction tuning,‚Äù CoRR, vol. abs/2304.08485, 2023. [131] D. Zhu, J. Chen, X. Shen, X. Li, and M. Elhoseiny, ‚ÄúMinigpt-4: Enhancing vision-language understanding with advanced large language models,‚Äù CoRR, vol. abs/2304.10592, 2023. [132] W. Dai, J. Li, D. Li, A. M. H. Tiong, J. Zhao, W. Wang, B. Li, P. Fung, and S. C. H. Hoi, ‚ÄúInstructblip: Towards general-purpose vision-language models with instruction tuning,‚Äù CoRR, vol. abs/2305.06500, 2023. [133] Y. Su, T. Lan, H. Li, J. Xu, Y. Wang, and D. Cai, ‚ÄúPandagpt: One model to instruction-follow them all,‚Äù 2023. [134] Y. Zhu, R. Kiros, R. S. Zemel, R. Salakhutdinov, R. Urtasun, A. Torralba, and S. Fidler, ‚ÄúAligning books and movies: Towards story-like visual explanations by watching movies and reading books,‚Äù in 2015 IEEE International Conference on Computer Vision, ICCV 2015, Santiago, Chile, December 7-13, 2015. IEEE Computer Society, 2015, pp. 19‚Äì27. [135] ‚ÄúProject gutenberg.‚Äù [Online]. Available: https:// www.gutenberg.org/ [136] T. H. Trinh and Q. V. Le, ‚ÄúA simple method for commonsense reasoning,‚Äù CoRR, vol. abs/1806.02847, 2018. [137] R. Zellers, A. Holtzman, H. Rashkin, Y. Bisk, A. Farhadi, F. Roesner, and Y. Choi, ‚ÄúDefending against neural fake news,‚Äù in Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, H. M. Wallach, H. Larochelle, A. Beygelzimer, F. d‚ÄôAlche¬¥Buc, E. B. Fox, and R. Garnett, Eds., 2019, pp. 9051‚Äì 9062. [138] A. Gokaslan, V. C. E. Pavlick, and S. Tellex, ‚ÄúOpenwebtext corpus,‚Äù http://Skylion007.github.io/ OpenWebTextCorpus, 2019. [139] J. Baumgartner, S. Zannettou, B. Keegan, M. Squire, and J. Blackburn, ‚ÄúThe pushshift reddit dataset,‚Äù in Proceedings of the Fourteenth International AAAI Conference on Web and Social Media, ICWSM 2020, Held Virtually, Original Venue: Atlanta, Georgia, USA, June 8-11, 2020. AAAI Press, 2020, pp. 830‚Äì839. [140] ‚ÄúWikipedia.‚Äù [Online]. Available: https://en. wikipedia.org/wiki/Main Page

[141] ‚ÄúBigquery dataset.‚Äù [Online]. Available: https:// cloud.google.com/bigquery?hl=zh-cn
[142] L. Gao, S. Biderman, S. Black, L. Golding, T. Hoppe, C. Foster, J. Phang, H. He, A. Thite, N. Nabeshima, S. Presser, and C. Leahy, ‚ÄúThe pile: An 800gb dataset of diverse text for language modeling,‚Äù CoRR, vol. abs/2101.00027, 2021.
[143] H. Laurenc¬∏on, L. Saulnier, T. Wang, C. Akiki, A. V. del Moral, T. Le Scao, L. Von Werra, C. Mou, E. G. Ponferrada, H. Nguyen et al., ‚ÄúThe bigscience roots corpus: A 1.6 tb composite multilingual dataset,‚Äù in Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2022.
[144] ‚ÄúCommon crawl.‚Äù [Online]. Available: https:// commoncrawl.org/
[145] ‚ÄúA reproduction version of cc-stories on hugging face.‚Äù [Online]. Available: https://huggingface.co/ datasets/spacemanidol/cc-stories
[146] B. Wang and A. Komatsuzaki, ‚ÄúGPT-J-6B: A 6 Billion Parameter Autoregressive Language Model,‚Äù https:// github.com/kingoflolz/mesh-transformer-jax, 2021.
[147] T. Wolf, L. Debut, V. Sanh, J. Chaumond, C. Delangue, A. Moi, P. Cistac, T. Rault, R. Louf, M. Funtowicz, J. Davison, S. Shleifer, P. von Platen, C. Ma, Y. Jernite, J. Plu, C. Xu, T. L. Scao, S. Gugger, M. Drame, Q. Lhoest, and A. M. Rush, ‚ÄúTransformers: State-ofthe-art natural language processing,‚Äù in Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, EMNLP 2020 - Demos, Online, November 16-20, 2020. Association for Computational Linguistics, 2020, pp. 38‚Äì45.
[148] J. Bradbury, R. Frostig, P. Hawkins, M. J. Johnson, C. Leary, D. Maclaurin, G. Necula, A. Paszke, J. VanderPlas, S. Wanderman-Milne, and Q. Zhang, ‚ÄúJAX: composable transformations of Python+NumPy programs,‚Äù 2018. [Online]. Available: http://github. com/google/jax
[149] Z. Bian, H. Liu, B. Wang, H. Huang, Y. Li, C. Wang, F. Cui, and Y. You, ‚ÄúColossal-ai: A unified deep learning system for large-scale parallel training,‚Äù CoRR, vol. abs/2110.14883, 2021.
[150] J. Fang, Y. Yu, S. Li, Y. You, and J. Zhou, ‚ÄúPatrickstar: Parallel training of pre-trained models via a chunk-based memory management,‚Äù CoRR, vol. abs/2108.05818, 2021.
[151] ‚ÄúBmtrain: Effient training for big models.‚Äù [Online]. Available: https://github.com/OpenBMB/BMTrain
[152] J. He, J. Qiu, A. Zeng, Z. Yang, J. Zhai, and J. Tang, ‚ÄúFastmoe: A fast mixture-of-expert training system,‚Äù CoRR, vol. abs/2103.13262, 2021.
[153] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga, A. Desmaison, A. Ko¬® pf, E. Z. Yang, Z. DeVito, M. Raison, A. Tejani, S. Chilamkurthy, B. Steiner, L. Fang, J. Bai, and S. Chintala, ‚ÄúPytorch: An imperative style, high-performance deep learning library,‚Äù in Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, H. M. Wallach, H. Larochelle, A. Beygelzimer, F. d‚ÄôAlche¬¥-Buc, E. B. Fox, and R. Gar-

66

nett, Eds., 2019, pp. 8024‚Äì8035. [154] M. Abadi, P. Barham, J. Chen, Z. Chen, A. Davis,
J. Dean, M. Devin, S. Ghemawat, G. Irving, M. Isard, M. Kudlur, J. Levenberg, R. Monga, S. Moore, D. G. Murray, B. Steiner, P. A. Tucker, V. Vasudevan, P. Warden, M. Wicke, Y. Yu, and X. Zheng, ‚ÄúTensorflow: A system for large-scale machine learning,‚Äù in 12th USENIX Symposium on Operating Systems Design and Implementation, OSDI 2016, Savannah, GA, USA, November 2-4, 2016, K. Keeton and T. Roscoe, Eds. USENIX Association, 2016, pp. 265‚Äì283. [155] T. Chen, M. Li, Y. Li, M. Lin, N. Wang, M. Wang, T. Xiao, B. Xu, C. Zhang, and Z. Zhang, ‚ÄúMxnet: A flexible and efficient machine learning library for heterogeneous distributed systems,‚Äù CoRR, vol. abs/1512.01274, 2015. [156] Y. Ma, D. Yu, T. Wu, and H. Wang, ‚ÄúPaddlepaddle: An open-source deep learning platform from industrial practice,‚Äù Frontiers of Data and Domputing, vol. 1, no. 1, p. 105, 2019. [157] J. Yuan, X. Li, C. Cheng, J. Liu, R. Guo, S. Cai, C. Yao, F. Yang, X. Yi, C. Wu, H. Zhang, and J. Zhao, ‚ÄúOneflow: Redesign the distributed deep learning framework from scratch,‚Äù CoRR, vol. abs/2110.15032, 2021. [158] S. Roller, E. Dinan, N. Goyal, D. Ju, M. Williamson, Y. Liu, J. Xu, M. Ott, E. M. Smith, Y. Boureau, and J. Weston, ‚ÄúRecipes for building an open-domain chatbot,‚Äù in Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, EACL 2021, Online, April 19 - 23, 2021, 2021, pp. 300‚Äì325. [159] A. Lewkowycz, A. Andreassen, D. Dohan, E. Dyer, H. Michalewski, V. V. Ramasesh, A. Slone, C. Anil, I. Schlag, T. Gutman-Solo, Y. Wu, B. Neyshabur, G. Gur-Ari, and V. Misra, ‚ÄúSolving quantitative reasoning problems with language models,‚Äù CoRR, vol. abs/2206.14858, 2022. [160] T. Saier, J. Krause, and M. Fa¬®rber, ‚Äúunarxive 2022: All arxiv publications pre-processed for nlp, including structured full-text and citation network,‚Äù arXiv preprint arXiv:2303.14957, 2023. [161] H. A. Simon, ‚ÄúExperiments with a heuristic compiler,‚Äù J. ACM, vol. 10, no. 4, pp. 493‚Äì506, 1963. [162] Z. Manna and R. J. Waldinger, ‚ÄúToward automatic program synthesis,‚Äù Commun. ACM, vol. 14, no. 3, pp. 151‚Äì165, 1971. [163] Z. Feng, D. Guo, D. Tang, N. Duan, X. Feng, M. Gong, L. Shou, B. Qin, T. Liu, D. Jiang, and M. Zhou, ‚ÄúCodebert: A pre-trained model for programming and natural languages,‚Äù in Findings of EMNLP, 2020. [164] J. Austin, A. Odena, M. I. Nye, M. Bosma, H. Michalewski, D. Dohan, E. Jiang, C. J. Cai, M. Terry, Q. V. Le, and C. Sutton, ‚ÄúProgram synthesis with large language models,‚Äù CoRR, vol. abs/2108.07732, 2021. [165] S. Black, L. Gao, P. Wang, C. Leahy, and S. Biderman, ‚ÄúGPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow,‚Äù 2021. [166] F. F. Xu, U. Alon, G. Neubig, and V. J. Hellendoorn, ‚ÄúA systematic evaluation of large language models of code,‚Äù in MAPS@PLDI, 2022. [167] A. Madaan, S. Zhou, U. Alon, Y. Yang, and G. Neubig,

‚ÄúLanguage models of code are few-shot commonsense learners,‚Äù in Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022, Y. Goldberg, Z. Kozareva, and Y. Zhang, Eds. Association for Computational Linguistics, 2022, pp. 1384‚Äì1403. [168] S. Longpre, G. Yauney, E. Reif, K. Lee, A. Roberts, B. Zoph, D. Zhou, J. Wei, K. Robinson, D. Mimno et al., ‚ÄúA pretrainer‚Äôs guide to training data: Measuring the effects of data age, domain coverage, quality, & toxicity,‚Äù arXiv preprint arXiv:2305.13169, 2023. [169] D. Hernandez, T. B. Brown, T. Conerly, N. DasSarma, D. Drain, S. E. Showk, N. Elhage, Z. Hatfield-Dodds, T. Henighan, T. Hume, S. Johnston, B. Mann, C. Olah, C. Olsson, D. Amodei, N. Joseph, J. Kaplan, and S. McCandlish, ‚ÄúScaling laws and interpretability of learning from repeated data,‚Äù CoRR, vol. abs/2205.10487, 2022. [170] A. Holtzman, J. Buys, L. Du, M. Forbes, and Y. Choi, ‚ÄúThe curious case of neural text degeneration,‚Äù in 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020. [171] K. Lee, D. Ippolito, A. Nystrom, C. Zhang, D. Eck, C. Callison-Burch, and N. Carlini, ‚ÄúDeduplicating training data makes language models better,‚Äù in Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022, 2022, pp. 8424‚Äì 8445. [172] N. Carlini, D. Ippolito, M. Jagielski, K. Lee, F. Trame`r, and C. Zhang, ‚ÄúQuantifying memorization across neural language models,‚Äù CoRR, 2022. [173] N. Carlini, F. Trame`r, E. Wallace, M. Jagielski, A. Herbert-Voss, K. Lee, A. Roberts, T. B. Brown, D. Song, U¬¥ . Erlingsson, A. Oprea, and C. Raffel, ‚ÄúExtracting training data from large language models,‚Äù in 30th USENIX Security Symposium, USENIX Security 2021, August 11-13, 2021, 2021, pp. 2633‚Äì2650. [174] N. Kandpal, E. Wallace, and C. Raffel, ‚ÄúDeduplicating training data mitigates privacy risks in language models,‚Äù in International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA. PMLR, 2022, pp. 10 697‚Äì10 707. [175] J. D. Lafferty, A. McCallum, and F. C. N. Pereira, ‚ÄúConditional random fields: Probabilistic models for segmenting and labeling sequence data,‚Äù in Proceedings of the Eighteenth International Conference on Machine Learning (ICML 2001), Williams College, Williamstown, MA, USA, June 28 - July 1, 2001, C. E. Brodley and A. P. Danyluk, Eds. Morgan Kaufmann, 2001, pp. 282‚Äì289. [176] P. Gage, ‚ÄúA new algorithm for data compression,‚Äù C Users Journal, vol. 12, no. 2, pp. 23‚Äì38, 1994. [177] R. Sennrich, B. Haddow, and A. Birch, ‚ÄúNeural machine translation of rare words with subword units,‚Äù in Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12, 2016, Berlin, Germany, Volume 1: Long Papers. The Association for Computer Linguistics, 2016.

67

[178] M. Schuster and K. Nakajima, ‚ÄúJapanese and korean voice search,‚Äù in 2012 IEEE international conference on acoustics, speech and signal processing (ICASSP). IEEE, 2012, pp. 5149‚Äì5152.
[179] Y. Wu, M. Schuster, Z. Chen, Q. V. Le, M. Norouzi, W. Macherey, M. Krikun, Y. Cao, Q. Gao, K. Macherey, J. Klingner, A. Shah, M. Johnson, X. Liu, L. Kaiser, S. Gouws, Y. Kato, T. Kudo, H. Kazawa, K. Stevens, G. Kurian, N. Patil, W. Wang, C. Young, J. Smith, J. Riesa, A. Rudnick, O. Vinyals, G. Corrado, M. Hughes, and J. Dean, ‚ÄúGoogle‚Äôs neural machine translation system: Bridging the gap between human and machine translation,‚Äù CoRR, vol. abs/1609.08144, 2016.
[180] T. Kudo, ‚ÄúSubword regularization: Improving neural network translation models with multiple subword candidates,‚Äù in Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, ACL 2018, Melbourne, Australia, July 15-20, 2018, Volume 1: Long Papers, I. Gurevych and Y. Miyao, Eds. Association for Computational Linguistics, 2018, pp. 66‚Äì75.
[181] T. Kudo and J. Richardson, ‚ÄúSentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing,‚Äù in Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, EMNLP 2018: System Demonstrations, Brussels, Belgium, October 31 - November 4, 2018, E. Blanco and W. Lu, Eds. Association for Computational Linguistics, 2018.
[182] M. Davis and M. Du¬® rst, ‚ÄúUnicode normalization forms,‚Äù 2001.
[183] D. Paperno, G. Kruszewski, A. Lazaridou, Q. N. Pham, R. Bernardi, S. Pezzelle, M. Baroni, G. Boleda, and R. Ferna¬¥ndez, ‚ÄúThe LAMBADA dataset: Word prediction requiring a broad discourse context,‚Äù in ACL (1). The Association for Computer Linguistics, 2016.
[184] P. Nakkiran, G. Kaplun, Y. Bansal, T. Yang, B. Barak, and I. Sutskever, ‚ÄúDeep double descent: Where bigger models and more data hurt,‚Äù in 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020.
[185] B. Zhang, B. Ghorbani, A. Bapna, Y. Cheng, X. Garcia, J. Shen, and O. Firat, ‚ÄúExamining scaling and transfer of language model architectures for machine translation,‚Äù in International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA, 2022, pp. 26 176‚Äì26 192.
[186] L. Dong, N. Yang, W. Wang, F. Wei, X. Liu, Y. Wang, J. Gao, M. Zhou, and H. Hon, ‚ÄúUnified language model pre-training for natural language understanding and generation,‚Äù in Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, 2019, pp. 13 042‚Äì13 054.
[187] A. Clark, D. de Las Casas, A. Guy, A. Mensch, M. Paganini, J. Hoffmann, B. Damoc, B. A. Hechtman, T. Cai, S. Borgeaud, G. van den Driessche, E. Rutherford, T. Hennigan, M. J. Johnson, A. Cassirer,

C. Jones, E. Buchatskaya, D. Budden, L. Sifre, S. Osindero, O. Vinyals, M. Ranzato, J. W. Rae, E. Elsen, K. Kavukcuoglu, and K. Simonyan, ‚ÄúUnified scaling laws for routed language models,‚Äù in International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA, 2022, pp. 4057‚Äì4086. [188] M. Ding, Z. Yang, W. Hong, W. Zheng, C. Zhou, D. Yin, J. Lin, X. Zou, Z. Shao, H. Yang, and J. Tang, ‚ÄúCogview: Mastering text-to-image generation via transformers,‚Äù in Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual, 2021, pp. 19 822‚Äì19 835. [189] L. J. Ba, J. R. Kiros, and G. E. Hinton, ‚ÄúLayer normalization,‚Äù vol. abs/1607.06450, 2016. [190] B. Zhang and R. Sennrich, ‚ÄúRoot mean square layer normalization,‚Äù in Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, 2019, pp. 12 360‚Äì 12 371. [191] H. Wang, S. Ma, L. Dong, S. Huang, D. Zhang, and F. Wei, ‚ÄúDeepnet: Scaling transformers to 1, 000 layers,‚Äù vol. abs/2203.00555, 2022. [192] V. Nair and G. E. Hinton, ‚ÄúRectified linear units improve restricted boltzmann machines,‚Äù in Proceedings of the 27th international conference on machine learning (ICML-10), 2010, pp. 807‚Äì814. [193] A. Wang, A. Singh, J. Michael, F. Hill, O. Levy, and S. R. Bowman, ‚ÄúGLUE: A multi-task benchmark and analysis platform for natural language understanding,‚Äù in Proceedings of the Workshop: Analyzing and Interpreting Neural Networks for NLP, BlackboxNLP@EMNLP 2018, Brussels, Belgium, November 1, 2018, T. Linzen, G. Chrupala, and A. Alishahi, Eds. Association for Computational Linguistics, 2018, pp. 353‚Äì355. [194] P. Ramachandran, B. Zoph, and Q. V. Le, ‚ÄúSearching for activation functions,‚Äù arXiv preprint arXiv:1710.05941, 2017. [195] N. Shazeer, ‚ÄúGLU variants improve transformer,‚Äù vol. abs/2002.05202, 2020. [196] J. Su, Y. Lu, S. Pan, B. Wen, and Y. Liu, ‚ÄúRoformer: Enhanced transformer with rotary position embedding,‚Äù vol. abs/2104.09864, 2021. [197] O. Press, N. A. Smith, and M. Lewis, ‚ÄúTrain short, test long: Attention with linear biases enables input length extrapolation,‚Äù in The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022, 2022. [198] S. Ioffe and C. Szegedy, ‚ÄúBatch normalization: Accelerating deep network training by reducing internal covariate shift,‚Äù in Proceedings of the 32nd International Conference on Machine Learning, ICML 2015, Lille, France, 6-11 July 2015, ser. JMLR Workshop and Conference Proceedings, F. R. Bach and D. M. Blei, Eds., vol. 37. JMLR.org, 2015, pp. 448‚Äì456. [Online]. Available: http://proceedings.mlr.press/ v37/ioffe15.html [199] S. Narang, H. W. Chung, Y. Tay, L. Fedus, T. Fe¬¥vry, M. Matena, K. Malkan, N. Fiedel, N. Shazeer, Z. Lan,

68

Y. Zhou, W. Li, N. Ding, J. Marcus, A. Roberts, and C. Raffel, ‚ÄúDo transformer modifications transfer across implementations and applications?‚Äù in Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 7-11 November, 2021, 2021, pp. 5758‚Äì5773. [200] R. Xiong, Y. Yang, D. He, K. Zheng, S. Zheng, C. Xing, H. Zhang, Y. Lan, L. Wang, and T. Liu, ‚ÄúOn layer normalization in the transformer architecture,‚Äù in ICML, 2020. [201] A. Baevski and M. Auli, ‚ÄúAdaptive input representations for neural language modeling,‚Äù in 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019. [202] L. Liu, X. Liu, J. Gao, W. Chen, and J. Han, ‚ÄúUnderstanding the difficulty of training transformers,‚Äù in Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online, November 16-20, 2020. Association for Computational Linguistics, 2020, pp. 5747‚Äì5763. [203] D. Hendrycks and K. Gimpel, ‚ÄúGaussian error linear units (gelus),‚Äù arXiv preprint arXiv:1606.08415, 2016. [204] Y. N. Dauphin, A. Fan, M. Auli, and D. Grangier, ‚ÄúLanguage modeling with gated convolutional networks,‚Äù in Proceedings of the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017, 2017, pp. 933‚Äì941. [205] T. L. Scao, T. Wang, D. Hesslow, S. Bekman, M. S. Bari, S. Biderman, H. Elsahar, N. Muennighoff, J. Phang, O. Press, C. Raffel, V. Sanh, S. Shen, L. Sutawika, J. Tae, Z. X. Yong, J. Launay, and I. Beltagy, ‚ÄúWhat language model to train if you have one million GPU hours?‚Äù in Findings of the Association for Computational Linguistics: EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022, 2022, pp. 765‚Äì782. [206] P. Shaw, J. Uszkoreit, and A. Vaswani, ‚ÄúSelfattention with relative position representations,‚Äù in Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACLHLT, New Orleans, Louisiana, USA, June 1-6, 2018, Volume 2 (Short Papers), M. A. Walker, H. Ji, and A. Stent, Eds. Association for Computational Linguistics, 2018, pp. 464‚Äì468. [Online]. Available: https://doi.org/10.18653/v1/n18-2074 [207] Z. Dai, Z. Yang, Y. Yang, J. G. Carbonell, Q. V. Le, and R. Salakhutdinov, ‚ÄúTransformer-xl: Attentive language models beyond a fixed-length context,‚Äù in Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28- August 2, 2019, Volume 1: Long Papers, A. Korhonen, D. R. Traum, and L. Ma`rquez, Eds. Association for Computational Linguistics, 2019, pp. 2978‚Äì2988. [Online]. Available: https://doi.org/10.18653/v1/p19-1285 [208] Z. Yang, Z. Dai, Y. Yang, J. Carbonell, R. R. Salakhutdinov, and Q. V. Le, ‚ÄúXlnet: Generalized autoregressive pretraining for language understanding,‚Äù Advances in neural information processing systems, vol. 32, 2019.

[209] Y. Sun, L. Dong, B. Patra, S. Ma, S. Huang, A. Benhaim, V. Chaudhary, X. Song, and F. Wei, ‚ÄúA length-extrapolatable transformer,‚Äù CoRR, vol. abs/2212.10554, 2022. [Online]. Available: https: //doi.org/10.48550/arXiv.2212.10554
[210] H. Peng, N. Pappas, D. Yogatama, R. Schwartz, N. A. Smith, and L. Kong, ‚ÄúRandom feature attention,‚Äù in 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021.
[211] M. Zaheer, G. Guruganesh, K. A. Dubey, J. Ainslie, C. Alberti, S. OntanÀú o¬¥ n, P. Pham, A. Ravula, Q. Wang, L. Yang, and A. Ahmed, ‚ÄúBig bird: Transformers for longer sequences,‚Äù in Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020.
[212] R. Child, S. Gray, A. Radford, and I. Sutskever, ‚ÄúGenerating long sequences with sparse transformers,‚Äù CoRR, vol. abs/1904.10509, 2019.
[213] N. Shazeer, ‚ÄúFast transformer decoding: One writehead is all you need,‚Äù CoRR, vol. abs/1911.02150, 2019. [Online]. Available: http://arxiv.org/abs/1911. 02150
[214] R. Li, L. B. Allal, Y. Zi, N. Muennighoff, D. Kocetkov, C. Mou, M. Marone, C. Akiki, J. Li, J. Chim, Q. Liu, E. Zheltonozhskii, T. Y. Zhuo, T. Wang, O. Dehaene, M. Davaadorj, J. Lamy-Poirier, J. Monteiro, O. Shliazhko, N. Gontier, N. Meade, A. Zebaze, M. Yee, L. K. Umapathi, J. Zhu, B. Lipkin, M. Oblokulov, Z. Wang, R. M. V, J. Stillerman, S. S. Patel, D. Abulkhanov, M. Zocca, M. Dey, Z. Zhang, N. Fahmy, U. Bhattacharyya, W. Yu, S. Singh, S. Luccioni, P. Villegas, M. Kunakov, F. Zhdanov, M. Romero, T. Lee, N. Timor, J. Ding, C. Schlesinger, H. Schoelkopf, J. Ebert, T. Dao, M. Mishra, A. Gu, J. Robinson, C. J. Anderson, B. Dolan-Gavitt, D. Contractor, S. Reddy, D. Fried, D. Bahdanau, Y. Jernite, C. M. Ferrandis, S. Hughes, T. Wolf, A. Guha, L. von Werra, and H. de Vries, ‚ÄúStarcoder: may the source be with you!‚Äù CoRR, vol. abs/2305.06161, 2023. [Online]. Available: https://doi.org/10.48550/arXiv.2305.06161
[215] T. Dao, D. Y. Fu, S. Ermon, A. Rudra, and C. Re, ‚ÄúFlashattention: Fast and memory-efficient exact attention with IO-awareness,‚Äù in NeurIPS, 2022.
[216] R. Anil, A. M. Dai, O. Firat, M. Johnson, D. Lepikhin, A. Passos, S. Shakeri, E. Taropa, P. Bailey, Z. Chen et al., ‚ÄúPalm 2 technical report,‚Äù arXiv preprint arXiv:2305.10403, 2023.
[217] A. Yuan, A. Coenen, E. Reif, and D. Ippolito, ‚ÄúWordcraft: story writing with large language models,‚Äù in 27th International Conference on Intelligent User Interfaces, 2022, pp. 841‚Äì852.
[218] A. Katharopoulos, A. Vyas, N. Pappas, and F. Fleuret, ‚ÄúTransformers are rnns: Fast autoregressive transformers with linear attention,‚Äù in Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, ser. Proceedings of Machine Learning Research, vol. 119. PMLR, 2020, pp. 5156‚Äì5165. [Online]. Available: http://proceedings.mlr.press/

69

v119/katharopoulos20a.html [219] C. Zhu, W. Ping, C. Xiao, M. Shoeybi, T. Goldstein,
A. Anandkumar, and B. Catanzaro, ‚ÄúLong-short transformer: Efficient transformers for language and vision,‚Äù in Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual, M. Ranzato, A. Beygelzimer, Y. N. Dauphin, P. Liang, and J. W. Vaughan, Eds., 2021, pp. 17 723‚Äì17 736. [Online]. Available: https://proceedings.neurips.cc/paper/2021/hash/ 9425be43ba92c2b4454ca7bf602efad8-Abstract.html [220] K. M. Choromanski, V. Likhosherstov, D. Dohan, X. Song, A. Gane, T. Sarlo¬¥ s, P. Hawkins, J. Q. Davis, A. Mohiuddin, L. Kaiser, D. B. Belanger, L. J. Colwell, and A. Weller, ‚ÄúRethinking attention with performers,‚Äù in 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021. [Online]. Available: https://openreview.net/forum?id=Ua6zuk0WRH [221] N. Kitaev, L. Kaiser, and A. Levskaya, ‚ÄúReformer: The efficient transformer,‚Äù in 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020. [Online]. Available: https://openreview.net/forum? id=rkgNKkHtvB [222] A. Gu, K. Goel, and C. Re¬¥, ‚ÄúEfficiently modeling long sequences with structured state spaces,‚Äù in The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022. [Online]. Available: https://openreview.net/forum?id=uYLFoz1vlAC [223] H. Mehta, A. Gupta, A. Cutkosky, and B. Neyshabur, ‚ÄúLong range language modeling via gated state spaces,‚Äù CoRR, vol. abs/2206.13947, 2022. [Online]. Available: https://doi.org/10.48550/arXiv.2206.13947 [224] T. Dao, D. Y. Fu, K. K. Saab, A. W. Thomas, A. Rudra, and C. Re¬¥, ‚ÄúHungry hungry hippos: Towards language modeling with state space models,‚Äù CoRR, vol. abs/2212.14052, 2022. [Online]. Available: https://doi.org/10.48550/arXiv.2212.14052 [225] B. Peng, E. Alcaide, Q. Anthony, A. Albalak, S. Arcadinho, H. Cao, X. Cheng, M. Chung, M. Grella, K. K. G. V., X. He, H. Hou, P. Kazienko, J. Kocon, J. Kong, B. Koptyra, H. Lau, K. S. I. Mantri, F. Mom, A. Saito, X. Tang, B. Wang, J. S. Wind, S. Wozniak, R. Zhang, Z. Zhang, Q. Zhao, P. Zhou, J. Zhu, and R. Zhu, ‚ÄúRWKV: reinventing rnns for the transformer era,‚Äù CoRR, vol. abs/2305.13048, 2023. [Online]. Available: https://doi.org/10.48550/arXiv.2305.13048 [226] D. P. Kingma and J. Ba, ‚ÄúAdam: A method for stochastic optimization,‚Äù in 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, Y. Bengio and Y. LeCun, Eds., 2015. [227] I. Loshchilov and F. Hutter, ‚ÄúFixing weight decay regularization in adam,‚Äù CoRR, vol. abs/1711.05101, 2017. [228] N. Shazeer and M. Stern, ‚ÄúAdafactor: Adaptive learning rates with sublinear memory cost,‚Äù in Proceedings of the 35th International Conference on Machine Learning,

ICML 2018, Stockholmsma¬®ssan, Stockholm, Sweden, July 10-15, 2018, ser. Proceedings of Machine Learning Research, J. G. Dy and A. Krause, Eds., vol. 80. PMLR, 2018, pp. 4603‚Äì4611. [229] Y. Huang, Y. Cheng, A. Bapna, O. Firat, D. Chen, M. X. Chen, H. Lee, J. Ngiam, Q. V. Le, Y. Wu, and Z. Chen, ‚ÄúGpipe: Efficient training of giant neural networks using pipeline parallelism,‚Äù in Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, H. M. Wallach, H. Larochelle, A. Beygelzimer, F. d‚ÄôAlche¬¥-Buc, E. B. Fox, and R. Garnett, Eds., 2019, pp. 103‚Äì112. [230] A. Harlap, D. Narayanan, A. Phanishayee, V. Seshadri, N. R. Devanur, G. R. Ganger, and P. B. Gibbons, ‚ÄúPipedream: Fast and efficient pipeline parallel DNN training,‚Äù CoRR, vol. abs/1806.03377, 2018. [231] S. Rajbhandari, J. Rasley, O. Ruwase, and Y. He, ‚ÄúZero: memory optimizations toward training trillion parameter models,‚Äù in Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis, SC 2020, Virtual Event / Atlanta, Georgia, USA, November 9-19, 2020, C. Cuicchi, I. Qualters, and W. T. Kramer, Eds. IEEE/ACM, 2020, p. 20. [232] P. Micikevicius, S. Narang, J. Alben, G. F. Diamos, E. Elsen, D. Garc¬¥ƒ±a, B. Ginsburg, M. Houston, O. Kuchaiev, G. Venkatesh, and H. Wu, ‚ÄúMixed precision training,‚Äù CoRR, vol. abs/1710.03740, 2017. [233] Q. Xu, S. Li, C. Gong, and Y. You, ‚ÄúAn efficient 2d method for training super-large deep learning models,‚Äù CoRR, vol. abs/2104.05343, 2021. [234] B. Wang, Q. Xu, Z. Bian, and Y. You, ‚ÄúTesseract: Parallelize the tensor parallelism efficiently,‚Äù in Proceedings of the 51st International Conference on Parallel Processing, ICPP 2022, Bordeaux, France, 29 August 2022 - 1 September 2022. ACM, 2022. [235] Z. Bian, Q. Xu, B. Wang, and Y. You, ‚ÄúMaximizing parallelism in distributed training for huge neural networks,‚Äù CoRR, vol. abs/2105.14450, 2021. [236] S. Li, F. Xue, C. Baranwal, Y. Li, and Y. You, ‚ÄúSequence parallelism: Long sequence training from system perspective,‚Äù arXiv e-prints, pp. arXiv‚Äì2105, 2021. [237] FairScale authors, ‚ÄúFairscale: A general purpose modular pytorch library for high performance and large scale training,‚Äù https://github.com/ facebookresearch/fairscale, 2021. [238] L. Zheng, Z. Li, H. Zhang, Y. Zhuang, Z. Chen, Y. Huang, Y. Wang, Y. Xu, D. Zhuo, E. P. Xing et al., ‚ÄúAlpa: Automating inter-and {Intra-Operator} parallelism for distributed deep learning,‚Äù in OSDI, 2022, pp. 559‚Äì578. [239] T. Chen, B. Xu, C. Zhang, and C. Guestrin, ‚ÄúTraining deep nets with sublinear memory cost,‚Äù CoRR, vol. abs/1604.06174, 2016. [240] S. Mishra, D. Khashabi, C. Baral, and H. Hajishirzi, ‚ÄúCross-task generalization via natural language crowdsourcing instructions,‚Äù in Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022, S. Muresan, P. Nakov,

70

and A. Villavicencio, Eds., 2022, pp. 3470‚Äì3487. [241] S. H. Bach, V. Sanh, Z. X. Yong, A. Webson, C. Raffel,
N. V. Nayak, A. Sharma, T. Kim, M. S. Bari, T. Fe¬¥vry, Z. Alyafeai, M. Dey, A. Santilli, Z. Sun, S. Ben-David, C. Xu, G. Chhablani, H. Wang, J. A. Fries, M. S. AlShaibani, S. Sharma, U. Thakker, K. Almubarak, X. Tang, D. R. Radev, M. T. Jiang, and A. M. Rush, ‚ÄúPromptsource: An integrated development environment and repository for natural language prompts,‚Äù in ACL (demo). Association for Computational Linguistics, 2022, pp. 93‚Äì104. [242] T. Tang, J. Li, W. X. Zhao, and J. Wen, ‚ÄúMVP: multitask supervised pre-training for natural language generation,‚Äù CoRR, vol. abs/2206.12131, 2022. [243] Y. Bai, A. Jones, K. Ndousse, A. Askell, A. Chen, N. DasSarma, D. Drain, S. Fort, D. Ganguli, T. Henighan, N. Joseph, S. Kadavath, J. Kernion, T. Conerly, S. E. Showk, N. Elhage, Z. Hatfield-Dodds, D. Hernandez, T. Hume, S. Johnston, S. Kravec, L. Lovitt, N. Nanda, C. Olsson, D. Amodei, T. B. Brown, J. Clark, S. McCandlish, C. Olah, B. Mann, and J. Kaplan, ‚ÄúTraining a helpful and harmless assistant with reinforcement learning from human feedback,‚Äù CoRR, vol. abs/2204.05862, 2022. [Online]. Available: https://doi.org/10.48550/arXiv.2204.05862 [244] B. Guo, X. Zhang, Z. Wang, M. Jiang, J. Nie, Y. Ding, J. Yue, and Y. Wu, ‚ÄúHow close is chatgpt to human experts? comparison corpus, evaluation, and detection,‚Äù arXiv preprint arXiv:2301.07597, 2023. [245] A. Ko¬® pf, Y. Kilcher, D. von Ru¬® tte, S. Anagnostidis, Z.-R. Tam, K. Stevens, A. Barhoum, N. M. Duc, O. Stanley, R. Nagyfi et al., ‚ÄúOpenassistant conversations‚Äìdemocratizing large language model alignment,‚Äù arXiv preprint arXiv:2304.07327, 2023. [246] C. Xu, D. Guo, N. Duan, and J. McAuley, ‚ÄúBaize: An open-source chat model with parameter-efficient tuning on self-chat data,‚Äù arXiv preprint arXiv:2304.01196, 2023. [247] Y. Ji, Y. Gong, Y. Deng, Y. Peng, Q. Niu, B. Ma, and X. Li, ‚ÄúTowards better instruction following language models for chinese: Investigating the impact of training data and evaluation,‚Äù arXiv preprint arXiv:2304.07854, 2023. [248] R. Lou, K. Zhang, and W. Yin, ‚ÄúIs prompt all you need? no. A comprehensive and broader view of instruction learning,‚Äù CoRR, vol. abs/2303.10475, 2023. [249] X. Liu, P. He, W. Chen, and J. Gao, ‚ÄúMulti-task deep neural networks for natural language understanding,‚Äù in ACL (1). Association for Computational Linguistics, 2019, pp. 4487‚Äì4496. [250] A. Aghajanyan, A. Gupta, A. Shrivastava, X. Chen, L. Zettlemoyer, and S. Gupta, ‚ÄúMuppet: Massive multi-task representations with pre-finetuning,‚Äù in EMNLP (1). Association for Computational Linguistics, 2021, pp. 5799‚Äì5811. [251] S. Longpre, L. Hou, T. Vu, A. Webson, H. W. Chung, Y. Tay, D. Zhou, Q. V. Le, B. Zoph, J. Wei, and A. Roberts, ‚ÄúThe flan collection: Designing data and methods for effective instruction tuning,‚Äù CoRR, vol. abs/2301.13688, 2023. [252] H. Chen, Y. Zhang, Q. Zhang, H. Yang, X. Hu,

X. Ma, Y. Yanggong, and J. Zhao, ‚ÄúMaybe only 0.5% data is needed: A preliminary exploration of low training data instruction tuning,‚Äù arXiv preprint arXiv:2305.09246, 2023. [253] C. Zhou, P. Liu, P. Xu, S. Iyer, J. Sun, Y. Mao, X. Ma, A. Efrat, P. Yu, L. Yu et al., ‚ÄúLima: Less is more for alignment,‚Äù arXiv preprint arXiv:2305.11206, 2023. [254] K. Singhal, S. Azizi, T. Tu, S. S. Mahdavi, J. Wei, H. W. Chung, N. Scales, A. Tanwani, H. Cole-Lewis, S. Pfohl et al., ‚ÄúLarge language models encode clinical knowledge,‚Äù arXiv preprint arXiv:2212.13138, 2022. [255] J. Zhang, R. Xie, Y. Hou, W. X. Zhao, L. Lin, and J. Wen, ‚ÄúRecommendation as instruction following: A large language model empowered recommendation approach,‚Äù CoRR, vol. abs/2305.07001, 2023. [256] H. Wang, C. Liu, N. Xi, Z. Qiang, S. Zhao, B. Qin, and T. Liu, ‚ÄúHuatuo: Tuning llama model with chinese medical knowledge,‚Äù arXiv preprint arXiv:2304.06975, 2023. [257] Q. Huang, M. Tao, Z. An, C. Zhang, C. Jiang, Z. Chen, Z. Wu, and Y. Feng, ‚ÄúLawyer llama technical report,‚Äù arXiv preprint arXiv:2305.15062, 2023. [258] S. Wu, O. Irsoy, S. Lu, V. Dabravolski, M. Dredze, S. Gehrmann, P. Kambadur, D. Rosenberg, and G. Mann, ‚ÄúBloomberggpt: A large language model for finance,‚Äù arXiv preprint arXiv:2303.17564, 2023. [259] T. Liu and B. K. H. Low, ‚ÄúGoat: Fine-tuned llama outperforms gpt-4 on arithmetic tasks,‚Äù arXiv preprint arXiv:2305.14201, 2023. [260] C. Xu, Q. Sun, K. Zheng, X. Geng, P. Zhao, J. Feng, C. Tao, and D. Jiang, ‚ÄúWizardlm: Empowering large language models to follow complex instructions,‚Äù CoRR, vol. abs/2304.12244, 2023. [Online]. Available: https://doi.org/10.48550/arXiv.2304.12244 [261] Z. Sun, Y. Shen, Q. Zhou, H. Zhang, Z. Chen, D. Cox, Y. Yang, and C. Gan, ‚ÄúPrinciple-driven self-alignment of language models from scratch with minimal human supervision,‚Äù arXiv preprint arXiv:2305.03047, 2023. [262] YuLan-Chat-Team, ‚ÄúYulan-chat: An open-source bilingual chatbot,‚Äù https://github.com/RUC-GSAI/ YuLan-Chat, 2023. [263] Y. Dubois, X. Li, R. Taori, T. Zhang, I. Gulrajani, J. Ba, C. Guestrin, P. Liang, and T. B. Hashimoto, ‚ÄúAlpacafarm: A simulation framework for methods that learn from human feedback,‚Äù CoRR, vol. abs/2305.14387, 2023. [Online]. Available: https: //doi.org/10.48550/arXiv.2305.14387 [264] D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and J. Steinhardt, ‚ÄúMeasuring massive multitask language understanding,‚Äù in ICLR. OpenReview.net, 2021. [265] A. Srivastava, A. Rastogi, A. Rao, A. A. M. Shoeb, A. Abid, A. Fisch, A. R. Brown, A. Santoro, A. Gupta, A. Garriga-Alonso, A. Kluska, A. Lewkowycz, A. Agarwal, A. Power, A. Ray, A. Warstadt, A. W. Kocurek, A. Safaya, A. Tazarv, A. Xiang, A. Parrish, A. Nie, A. Hussain, A. Askell, A. Dsouza, A. Rahane, A. S. Iyer, A. Andreassen, A. Santilli, A. Stuhlmu¬® ller, A. M. Dai, A. La, A. K. Lampinen, A. Zou, A. Jiang, A. Chen, A. Vuong, A. Gupta, A. Gottardi, A. Norelli, A. Venkatesh, A. Gholamidavoodi, A. Tabassum,

71

A. Menezes, A. Kirubarajan, A. Mullokandov, A. Sabharwal, A. Herrick, A. Efrat, A. Erdem, A. Karakas, and et al., ‚ÄúBeyond the imitation game: Quantifying and extrapolating the capabilities of language models,‚Äù CoRR, vol. abs/2206.04615, 2022. [266] Z. Kenton, T. Everitt, L. Weidinger, I. Gabriel, V. Mikulik, and G. Irving, ‚ÄúAlignment of language agents,‚Äù CoRR, vol. abs/2103.14659, 2021. [267] D. M. Ziegler, N. Stiennon, J. Wu, T. B. Brown, A. Radford, D. Amodei, P. F. Christiano, and G. Irving, ‚ÄúFinetuning language models from human preferences,‚Äù CoRR, vol. abs/1909.08593, 2019. [268] A. Askell, Y. Bai, A. Chen, D. Drain, D. Ganguli, T. Henighan, A. Jones, N. Joseph, B. Mann, N. DasSarma, N. Elhage, Z. Hatfield-Dodds, D. Hernandez, J. Kernion, K. Ndousse, C. Olsson, D. Amodei, T. B. Brown, J. Clark, S. McCandlish, C. Olah, and J. Kaplan, ‚ÄúA general language assistant as a laboratory for alignment,‚Äù CoRR, vol. abs/2112.00861, 2021. [269] E. Perez, S. Huang, H. F. Song, T. Cai, R. Ring, J. Aslanides, A. Glaese, N. McAleese, and G. Irving, ‚ÄúRed teaming language models with language models,‚Äù in Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022, Y. Goldberg, Z. Kozareva, and Y. Zhang, Eds. Association for Computational Linguistics, 2022, pp. 3419‚Äì 3448. [270] J. Menick, M. Trebacz, V. Mikulik, J. Aslanides, H. F. Song, M. Chadwick, M. Glaese, S. Young, L. Campbell-Gillingham, G. Irving, and N. McAleese, ‚ÄúTeaching language models to support answers with verified quotes,‚Äù CoRR, vol. abs/2203.11147, 2022. [271] H. Dong, W. Xiong, D. Goyal, R. Pan, S. Diao, J. Zhang, K. Shum, and T. Zhang, ‚ÄúRAFT: reward ranked finetuning for generative foundation model alignment,‚Äù CoRR, vol. abs/2304.06767, 2023. [Online]. Available: https://doi.org/10.48550/arXiv.2304.06767 [272] X. L. Li and P. Liang, ‚ÄúPrefix-tuning: Optimizing continuous prompts for generation,‚Äù in Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual Event, August 16, 2021, C. Zong, F. Xia, W. Li, and R. Navigli, Eds. Association for Computational Linguistics, 2021, pp. 4582‚Äì4597. [273] B. Lester, R. Al-Rfou, and N. Constant, ‚ÄúThe power of scale for parameter-efficient prompt tuning,‚Äù in Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 7-11 November, 2021, M. Moens, X. Huang, L. Specia, and S. W. Yih, Eds. Association for Computational Linguistics, 2021, pp. 3045‚Äì3059. [274] N. Houlsby, A. Giurgiu, S. Jastrzebski, B. Morrone, Q. de Laroussilhe, A. Gesmundo, M. Attariyan, and S. Gelly, ‚ÄúParameter-efficient transfer learning for NLP,‚Äù in Proceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA, 2019, pp. 2790‚Äì2799.

[275] Z. Hu, Y. Lan, L. Wang, W. Xu, E. Lim, R. K. Lee, L. Bing, and S. Poria, ‚ÄúLlm-adapters: An adapter family for parameter-efficient fine-tuning of large language models,‚Äù CoRR, vol. abs/2304.01933, 2023.
[276] J. He, C. Zhou, X. Ma, T. Berg-Kirkpatrick, and G. Neubig, ‚ÄúTowards a unified view of parameterefficient transfer learning,‚Äù in The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022.
[277] X. Liu, K. Ji, Y. Fu, Z. Du, Z. Yang, and J. Tang, ‚ÄúPtuning v2: Prompt tuning can be comparable to finetuning universally across scales and tasks,‚Äù CoRR, vol. abs/2110.07602, 2021.
[278] X. Liu, Y. Zheng, Z. Du, M. Ding, Y. Qian, Z. Yang, and J. Tang, ‚ÄúGPT understands, too,‚Äù CoRR, vol. abs/2103.10385, 2021.
[279] Y. Gu, X. Han, Z. Liu, and M. Huang, ‚ÄúPpt: Pre-trained prompt tuning for few-shot learning,‚Äù in Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 2022, pp. 8410‚Äì8423.
[280] Z. Jiang, F. F. Xu, J. Araki, and G. Neubig, ‚ÄúHow can we know what language models know?‚Äù Transactions of the Association for Computational Linguistics, vol. 8, pp. 423‚Äì438, 2020.
[281] T. Shin, Y. Razeghi, R. L. Logan IV, E. Wallace, and S. Singh, ‚ÄúAutoprompt: Eliciting knowledge from language models with automatically generated prompts,‚Äù in Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), 2020, pp. 4222‚Äì4235.
[282] Q. Zhang, M. Chen, A. Bukharin, P. He, Y. Cheng, W. Chen, and T. Zhao, ‚ÄúAdaptive budget allocation for parameter-efficient fine-tuning,‚Äù CoRR, vol. abs/2303.10512, 2023. [Online]. Available: https: //doi.org/10.48550/arXiv.2303.10512
[283] M. Valipour, M. Rezagholizadeh, I. Kobyzev, and A. Ghodsi, ‚ÄúDylora: Parameter efficient tuning of pre-trained models using dynamic search-free lowrank adaptation,‚Äù CoRR, vol. abs/2210.07558, 2022. [Online]. Available: https://doi.org/10.48550/arXiv. 2210.07558
[284] N. Ding, Y. Qin, G. Yang, F. Wei, Y. Zonghan, Y. Su, S. Hu, Y. Chen, C.-M. Chan, W. Chen, J. Yi, W. Zhao, X. Wang, Z. Liu, H.-T. Zheng, J. Chen, Y. Liu, J. Tang, J. Li, and M. Sun, ‚ÄúParameter-efficient fine-tuning of large-scale pre-trained language models,‚Äù Nature Machine Intelligence, vol. 5, pp. 1‚Äì16, 03 2023.
[285] R. Zhang, J. Han, A. Zhou, X. Hu, S. Yan, P. Lu, H. Li, P. Gao, and Y. Qiao, ‚ÄúLlama-adapter: Efficient finetuning of language models with zero-init attention,‚Äù CoRR, vol. abs/2303.16199, 2023.
[286] J. Pfeiffer, I. Vulic, I. Gurevych, and S. Ruder, ‚ÄúMADX: an adapter-based framework for multi-task crosslingual transfer,‚Äù in Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online, November 16-20, 2020, B. Webber, T. Cohn, Y. He, and Y. Liu, Eds. Association for Computational Linguistics, 2020, pp. 7654‚Äì7673.
[287] S. Mangrulkar, S. Gugger, L. Debut, Y. Belkada, and S. Paul, ‚ÄúPeft: State-of-the-art parameter-efficient fine-

72

tuning methods,‚Äù https://github.com/huggingface/ peft, 2022. [288] A. Gholami, S. Kim, Z. Dong, Z. Yao, M. W. Mahoney, and K. Keutzer, ‚ÄúA survey of quantization methods for efficient neural network inference,‚Äù CoRR, vol. abs/2103.13630, 2021. [Online]. Available: https://arxiv.org/abs/2103.13630 [289] T. Dettmers, M. Lewis, Y. Belkada, and L. Zettlemoyer, ‚ÄúLlm.int8(): 8-bit matrix multiplication for transformers at scale,‚Äù CoRR, vol. abs/2208.07339, 2022. [290] G. Xiao, J. Lin, M. Seznec, J. Demouth, and S. Han, ‚ÄúSmoothquant: Accurate and efficient posttraining quantization for large language models,‚Äù CoRR, vol. abs/2211.10438, 2022. [Online]. Available: https://doi.org/10.48550/arXiv.2211.10438 [291] Z. Yao, R. Y. Aminabadi, M. Zhang, X. Wu, C. Li, and Y. He, ‚ÄúZeroquant: Efficient and affordable posttraining quantization for large-scale transformers,‚Äù in NeurIPS, 2022. [292] J. Lin, J. Tang, H. Tang, S. Yang, X. Dang, and S. Han, ‚ÄúAwq: Activation-aware weight quantization for llm compression and acceleration,‚Äù 2023. [293] E. Frantar, S. Ashkboos, T. Hoefler, and D. Alistarh, ‚ÄúGptq: Accurate post-training quantization for generative pre-trained transformers,‚Äù arXiv preprint arXiv:2210.17323, 2022. [294] E. Frantar and D. Alistarh, ‚ÄúOptimal brain compression: A framework for accurate post-training quantization and pruning,‚Äù in NeurIPS, 2022. [295] T. Dettmers, A. Pagnoni, A. Holtzman, and L. Zettlemoyer, ‚ÄúQlora: Efficient finetuning of quantized llms,‚Äù arXiv preprint arXiv:2305.14314, 2023. [296] Z. Liu, B. Oguz, C. Zhao, E. Chang, P. Stock, Y. Mehdad, Y. Shi, R. Krishnamoorthi, and V. Chandra, ‚ÄúLlm-qat: Data-free quantization aware training for large language models,‚Äù 2023. [297] Z. Yao, X. Wu, C. Li, S. Youn, and Y. He, ‚ÄúZeroquantv2: Exploring post-training quantization in llms from comprehensive study to low rank compensation,‚Äù 2023. [298] T. Dettmers and L. Zettlemoyer, ‚ÄúThe case for 4-bit precision: k-bit inference scaling laws,‚Äù CoRR, vol. abs/2212.09720, 2022. [299] T. Dettmers, M. Lewis, Y. Belkada, and L. Zettlemoyer, ‚ÄúLlm.int8(): 8-bit matrix multiplication for transformers at scale,‚Äù CoRR, vol. abs/2208.07339, 2022. [Online]. Available: https://doi.org/10.48550/arXiv.2208.07339 [300] X. Wei, X. Cui, N. Cheng, X. Wang, X. Zhang, S. Huang, P. Xie, J. Xu, Y. Chen, M. Zhang et al., ‚ÄúZero-shot information extraction via chatting with chatgpt,‚Äù arXiv preprint arXiv:2302.10205, 2023. [301] T. Dettmers, M. Lewis, S. Shleifer, and L. Zettlemoyer, ‚Äú8-bit optimizers via block-wise quantization,‚Äù 9th International Conference on Learning Representations, ICLR, 2022. [302] C. Tao, L. Hou, W. Zhang, L. Shang, X. Jiang, Q. Liu, P. Luo, and N. Wong, ‚ÄúCompression of generative pre-trained language models via quantization,‚Äù in Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),

ACL 2022, Dublin, Ireland, May 22-27, 2022, S. Muresan, P. Nakov, and A. Villavicencio, Eds. Association for Computational Linguistics, 2022, pp. 4821‚Äì4836. [303] D. Zhou, N. Scha¬®rli, L. Hou, J. Wei, N. Scales, X. Wang, D. Schuurmans, O. Bousquet, Q. Le, and E. H. Chi, ‚ÄúLeast-to-most prompting enables complex reasoning in large language models,‚Äù CoRR, vol. abs/2205.10625, 2022. [304] S. Min, X. Lyu, A. Holtzman, M. Artetxe, M. Lewis, H. Hajishirzi, and L. Zettlemoyer, ‚ÄúRethinking the role of demonstrations: What makes in-context learning work?‚Äù in Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December 711, 2022. Association for Computational Linguistics, 2022, pp. 11 048‚Äì11 064. [305] Y. Lu, M. Bartolo, A. Moore, S. Riedel, and P. Stenetorp, ‚ÄúFantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity,‚Äù in Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022, S. Muresan, P. Nakov, and A. Villavicencio, Eds., 2022, pp. 8086‚Äì8098. [306] Z. Zhao, E. Wallace, S. Feng, D. Klein, and S. Singh, ‚ÄúCalibrate before use: Improving few-shot performance of language models,‚Äù in Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, M. Meila and T. Zhang, Eds., 2021, pp. 12 697‚Äì12 706. [307] J. Liu, D. Shen, Y. Zhang, B. Dolan, L. Carin, and W. Chen, ‚ÄúWhat makes good in-context examples for gpt-3?‚Äù in Proceedings of Deep Learning Inside Out: The 3rd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures, DeeLIO@ACL 2022, Dublin, Ireland and Online, May 27, 2022, 2022, pp. 100‚Äì114. [308] Y. Lee, C. Lim, and H. Choi, ‚ÄúDoes GPT-3 generate empathetic dialogues? A novel in-context example selection method and automatic evaluation metric for empathetic dialogue generation,‚Äù in Proceedings of the 29th International Conference on Computational Linguistics, COLING 2022, Gyeongju, Republic of Korea, October 12-17, 2022, N. Calzolari, C. Huang, H. Kim, J. Pustejovsky, L. Wanner, K. Choi, P. Ryu, H. Chen, L. Donatelli, H. Ji, S. Kurohashi, P. Paggio, N. Xue, S. Kim, Y. Hahm, Z. He, T. K. Lee, E. Santus, F. Bond, and S. Na, Eds. International Committee on Computational Linguistics, 2022, pp. 669‚Äì683. [309] I. Levy, B. Bogin, and J. Berant, ‚ÄúDiverse demonstrations improve in-context compositional generalization,‚Äù CoRR, vol. abs/2212.06800, 2022. [310] H. Su, J. Kasai, C. H. Wu, W. Shi, T. Wang, J. Xin, R. Zhang, M. Ostendorf, L. Zettlemoyer, N. A. Smith, and T. Yu, ‚ÄúSelective annotation makes language models better few-shot learners,‚Äù CoRR, 2022. [311] X. Ye, S. Iyer, A. Celikyilmaz, V. Stoyanov, G. Durrett, and R. Pasunuru, ‚ÄúComplementary explanations for effective in-context learning,‚Äù CoRR, 2022. [312] X. Li and X. Qiu, ‚ÄúFinding supporting examples for in-context learning,‚Äù CoRR, 2023. [313] O. Rubin, J. Herzig, and J. Berant, ‚ÄúLearning to re-

73

trieve prompts for in-context learning,‚Äù in Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL 2022, Seattle, WA, United States, July 10-15, 2022, 2022, pp. 2655‚Äì2671. [314] Y. Zhang, S. Feng, and C. Tan, ‚ÄúActive example selection for in-context learning,‚Äù in Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022, 2022, pp. 9134‚Äì9148. [315] F. Gilardi, M. Alizadeh, and M. Kubli, ‚ÄúChatgpt outperforms crowd-workers for text-annotation tasks,‚Äù 2023. [316] H. J. Kim, H. Cho, J. Kim, T. Kim, K. M. Yoo, and S. Lee, ‚ÄúSelf-generated in-context learning: Leveraging auto-regressive language models as a demonstration generator,‚Äù CoRR, vol. abs/2206.08082, 2022. [317] S. M. Xie, A. Raghunathan, P. Liang, and T. Ma, ‚ÄúAn explanation of in-context learning as implicit bayesian inference,‚Äù in The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022, 2022. [318] Z. Zhang, A. Zhang, M. Li, and A. Smola, ‚ÄúAutomatic chain of thought prompting in large language models,‚Äù CoRR, vol. abs/2210.03493, 2022. [319] Z. Wu, Y. Wang, J. Ye, and L. Kong, ‚ÄúSelf-adaptive incontext learning,‚Äù CoRR, vol. abs/2212.10375, 2022. [320] Y. Gu, L. Dong, F. Wei, and M. Huang, ‚ÄúPre-training to learn in context,‚Äù CoRR, vol. abs/2305.09137, 2023. [321] S. Min, M. Lewis, L. Zettlemoyer, and H. Hajishirzi, ‚ÄúMetaicl: Learning to learn in context,‚Äù in Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL 2022, Seattle, WA, United States, July 10-15, 2022, M. Carpuat, M. de Marneffe, and I. V. M. Ru¬¥ƒ±z, Eds., 2022, pp. 2791‚Äì2809. [322] M. Hahn and N. Goyal, ‚ÄúA theory of emergent in-context learning as implicit structure induction,‚Äù CoRR, vol. abs/2303.07971, 2023. [323] J. Pan, T. Gao, H. Chen, and D. Chen, ‚ÄúWhat in-context learning ‚Äùlearns‚Äù in-context: Disentangling task recognition and task learning,‚Äù CoRR, vol. abs/2305.09731, 2023. [324] N. Wies, Y. Levine, and A. Shashua, ‚ÄúThe learnability of in-context learning,‚Äù CoRR, vol. abs/2303.07895, 2023. [325] A. Webson and E. Pavlick, ‚ÄúDo prompt-based models really understand the meaning of their prompts?‚Äù in Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL 2022, Seattle, WA, United States, July 10-15, 2022, 2022, pp. 2300‚Äì 2344. [326] J. von Oswald, E. Niklasson, E. Randazzo, J. Sacramento, A. Mordvintsev, A. Zhmoginov, and M. Vladymyrov, ‚ÄúTransformers learn in-context by gradient descent,‚Äù CoRR, vol. abs/2212.07677, 2022. [327] C. Olsson, N. Elhage, N. Nanda, N. Joseph, N. DasSarma, T. Henighan, B. Mann, A. Askell, Y. Bai, A. Chen, T. Conerly, D. Drain, D. Ganguli, Z. Hatfield-Dodds, D. Hernandez, S. Johnston,

A. Jones, J. Kernion, L. Lovitt, K. Ndousse, D. Amodei, T. Brown, J. Clark, J. Kaplan, S. McCandlish, and C. Olah, ‚ÄúIn-context learning and induction heads,‚Äù CoRR, vol. abs/2209.11895, 2022. [328] E. Akyu¬® rek, D. Schuurmans, J. Andreas, T. Ma, and D. Zhou, ‚ÄúWhat learning algorithm is in-context learning? investigations with linear models,‚Äù CoRR, vol. abs/2211.15661, 2022. [329] J. Wei, J. Wei, Y. Tay, D. Tran, A. Webson, Y. Lu, X. Chen, H. Liu, D. Huang, D. Zhou et al., ‚ÄúLarger language models do in-context learning differently,‚Äù arXiv preprint arXiv:2303.03846, 2023. [330] J. Coda-Forno, M. Binz, Z. Akata, M. M. Botvinick, J. X. Wang, and E. Schulz, ‚ÄúMeta-in-context learning in large language models,‚Äù CoRR, vol. abs/2305.12907, 2023. [331] J. W. Wei, L. Hou, A. K. Lampinen, X. Chen, D. Huang, Y. Tay, X. Chen, Y. Lu, D. Zhou, T. Ma, and Q. V. Le, ‚ÄúSymbol tuning improves in-context learning in language models,‚Äù CoRR, vol. abs/2305.08298, 2023. [332] S. Miao, C. Liang, and K. Su, ‚ÄúA diverse corpus for evaluating and developing english math word problem solvers,‚Äù in Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020, D. Jurafsky, J. Chai, N. Schluter, and J. R. Tetreault, Eds. Association for Computational Linguistics, 2020, pp. 975‚Äì984. [333] A. Talmor, J. Herzig, N. Lourie, and J. Berant, ‚ÄúCommonsenseqa: A question answering challenge targeting commonsense knowledge,‚Äù in Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers), J. Burstein, C. Doran, and T. Solorio, Eds. Association for Computational Linguistics, 2019, pp. 4149‚Äì4158. [334] Y. Li, Z. Lin, S. Zhang, Q. Fu, B. Chen, J. Lou, and W. Chen, ‚ÄúOn the advance of making language models better reasoners,‚Äù CoRR, vol. abs/2206.02336, 2022. [335] Y. Fu, H. Peng, A. Sabharwal, P. Clark, and T. Khot, ‚ÄúComplexity-based prompting for multi-step reasoning,‚Äù CoRR, vol. abs/2210.00720, 2022. [336] T. Kojima, S. S. Gu, M. Reid, Y. Matsuo, and Y. Iwasawa, ‚ÄúLarge language models are zero-shot reasoners,‚Äù CoRR, vol. abs/2205.11916, 2022. [337] X. Wang, J. Wei, D. Schuurmans, Q. V. Le, E. H. Chi, and D. Zhou, ‚ÄúSelf-consistency improves chain of thought reasoning in language models,‚Äù CoRR, vol. abs/2203.11171, 2022. [338] ‚Äî‚Äî, ‚ÄúRationale-augmented ensembles in language models,‚Äù CoRR, 2022. [339] E. Zelikman, J. Mu, N. D. Goodman, and Y. T. Wu, ‚ÄúStar: Self-taught reasoner bootstrapping reasoning with reasoning,‚Äù 2022. [340] P. Liang, R. Bommasani, T. Lee, D. Tsipras, D. Soylu, M. Yasunaga, Y. Zhang, D. Narayanan, Y. Wu, A. Kumar, B. Newman, B. Yuan, B. Yan, C. Zhang, C. Cosgrove, C. D. Manning, C. Re¬¥, D. Acosta-Navas, D. A. Hudson, E. Zelikman, E. Durmus, F. Ladhak, F. Rong, H. Ren, H. Yao, J. Wang, K. Santhanam, L. J. Orr,

74

L. Zheng, M. Yu¬® ksekgo¬® nu¬® l, M. Suzgun, N. Kim, N. Guha, N. S. Chatterji, O. Khattab, P. Henderson, Q. Huang, R. Chi, S. M. Xie, S. Santurkar, S. Ganguli, T. Hashimoto, T. Icard, T. Zhang, V. Chaudhary, W. Wang, X. Li, Y. Mai, Y. Zhang, and Y. Koreeda, ‚ÄúHolistic evaluation of language models,‚Äù CoRR, vol. abs/2211.09110, 2022. [341] A. Madaan and A. Yazdanbakhsh, ‚ÄúText and patterns: For effective chain of thought, it takes two to tango,‚Äù CoRR, vol. abs/2209.07686, 2022. [342] Z. Zhang, A. Zhang, M. Li, H. Zhao, G. Karypis, and A. Smola, ‚ÄúMultimodal chain-of-thought reasoning in language models,‚Äù CoRR, vol. abs/2302.00923, 2023. [343] F. Shi, M. Suzgun, M. Freitag, X. Wang, S. Srivats, S. Vosoughi, H. W. Chung, Y. Tay, S. Ruder, D. Zhou, D. Das, and J. Wei, ‚ÄúLanguage models are multilingual chain-of-thought reasoners,‚Äù CoRR, vol. abs/2210.03057, 2022. [344] J. Qian, H. Wang, Z. Li, S. Li, and X. Yan, ‚ÄúLimitations of language models in arithmetic and symbolic induction,‚Äù CoRR, vol. abs/2208.05051, 2022. [345] N. Bian, X. Han, L. Sun, H. Lin, Y. Lu, and B. He, ‚ÄúChatGPT is a Knowledgeable but Inexperienced Solver: An Investigation of Commonsense Problem in Large Language Models,‚Äù CoRR, 2023. [346] L. Gao, A. Madaan, S. Zhou, U. Alon, P. Liu, Y. Yang, J. Callan, and G. Neubig, ‚ÄúPAL: program-aided language models,‚Äù CoRR, vol. abs/2211.10435, 2022. [347] L. Wang, W. Xu, Y. Lan, Z. Hu, Y. Lan, R. K. Lee, and E. Lim, ‚ÄúPlan-and-solve prompting: Improving zeroshot chain-of-thought reasoning by large language models,‚Äù CoRR, vol. abs/2305.04091, 2023. [Online]. Available: https://doi.org/10.48550/arXiv.2305.04091 [348] I. Singh, V. Blukis, A. Mousavian, A. Goyal, D. Xu, J. Tremblay, D. Fox, J. Thomason, and A. Garg, ‚ÄúProgprompt: Generating situated robot task plans using large language models,‚Äù CoRR, vol. abs/2209.11302, 2022. [349] S. Yao, D. Yu, J. Zhao, I. Shafran, T. L. Griffiths, Y. Cao, and K. Narasimhan, ‚ÄúTree of thoughts: Deliberate problem solving with large language models,‚Äù CoRR, vol. abs/2305.10601, 2023. [350] G. Wang, Y. Xie, Y. Jiang, A. Mandlekar, C. Xiao, Y. Zhu, L. Fan, and A. Anandkumar, ‚ÄúVoyager: An open-ended embodied agent with large language models,‚Äù arXiv preprint arXiv:2305.16291, 2023. [351] N. Shinn, F. Cassano, B. Labash, A. Gopinath, K. Narasimhan, and S. Yao, ‚ÄúReflexion: Language agents with verbal reinforcement learning,‚Äù 2023. [352] Y. Lu, P. Lu, Z. Chen, W. Zhu, X. E. Wang, and W. Y. Wang, ‚ÄúMultimodal procedural planning via dual text-image prompting,‚Äù CoRR, vol. abs/2305.01795, 2023. [353] X. Jiang, Y. Dong, L. Wang, Q. Shang, and G. Li, ‚ÄúSelf-planning code generation with large language model,‚Äù CoRR, vol. abs/2303.06689, 2023. [Online]. Available: https://doi.org/10.48550/arXiv.2303.06689 [354] T. Khot, H. Trivedi, M. Finlayson, Y. Fu, K. Richardson, P. Clark, and A. Sabharwal, ‚ÄúDecomposed prompting: A modular approach for solving complex tasks,‚Äù CoRR, vol. abs/2210.02406, 2022. [Online]. Available:

https://doi.org/10.48550/arXiv.2210.02406 [355] Y. Shen, K. Song, X. Tan, D. Li, W. Lu, and
Y. Zhuang, ‚ÄúHugginggpt: Solving ai tasks with chatgpt and its friends in huggingface,‚Äù arXiv preprint arXiv:2303.17580, 2023. [356] Q. Lyu, S. Havaldar, A. Stein, L. Zhang, D. Rao, E. Wong, M. Apidianaki, and C. Callison-Burch, ‚ÄúFaithful chain-of-thought reasoning,‚Äù CoRR, vol. abs/2301.13379, 2023. [357] B. Liu, Y. Jiang, X. Zhang, Q. Liu, S. Zhang, J. Biswas, and P. Stone, ‚ÄúLLM+P: empowering large language models with optimal planning proficiency,‚Äù CoRR, vol. abs/2304.11477, 2023. [Online]. Available: https://doi.org/10.48550/arXiv.2304.11477 [358] S. Hao, Y. Gu, H. Ma, J. J. Hong, Z. Wang, D. Z. Wang, and Z. Hu, ‚ÄúReasoning with language model is planning with world model,‚Äù CoRR, vol. abs/2305.14992, 2023. [359] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer, ‚ÄúHigh-resolution image synthesis with latent diffusion models,‚Äù in IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2022, New Orleans, LA, USA, June 18-24, 2022, 2022, pp. 10 674‚Äì10 685. [360] J. S. Park, J. C. O‚ÄôBrien, C. J. Cai, M. R. Morris, P. Liang, and M. S. Bernstein, ‚ÄúGenerative agents: Interactive simulacra of human behavior,‚Äù CoRR, vol. abs/2304.03442, 2023. [361] S. Yao, J. Zhao, D. Yu, N. Du, I. Shafran, K. Narasimhan, and Y. Cao, ‚ÄúReact: Synergizing reasoning and acting in language models,‚Äù CoRR, vol. abs/2210.03629, 2022. [362] Z. Chen, K. Zhou, B. Zhang, Z. Gong, W. X. Zhao, and J. Wen, ‚ÄúChatcot: Tool-augmented chain-of-thought reasoning on chat-based large language models,‚Äù CoRR, vol. abs/2305.14323, 2023. [363] Z. Wang, S. Cai, A. Liu, X. Ma, and Y. Liang, ‚ÄúDescribe, explain, plan and select: Interactive planning with large language models enables open-world multi-task agents,‚Äù CoRR, vol. abs/2302.01560, 2023. [364] H. Sun, Y. Zhuang, L. Kong, B. Dai, and C. Zhang, ‚ÄúAdaplanner: Adaptive planning from feedback with language models,‚Äù arXiv preprint arXiv:2305.16653, 2023. [365] J. Wang, X. Yi, R. Guo, H. Jin, P. Xu, S. Li, X. Wang, X. Guo, C. Li, X. Xu et al., ‚ÄúMilvus: A purpose-built vector data management system,‚Äù in Proceedings of the 2021 International Conference on Management of Data, 2021, pp. 2614‚Äì2627. [366] M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz, ‚ÄúBuilding a large annotated corpus of english: The penn treebank,‚Äù Comput. Linguistics, vol. 19, no. 2, pp. 313‚Äì330, 1993. [367] S. Merity, C. Xiong, J. Bradbury, and R. Socher, ‚ÄúPointer sentinel mixture models,‚Äù in ICLR (Poster). OpenReview.net, 2017. [368] O. Bojar, C. Buck, C. Federmann, B. Haddow, P. Koehn, J. Leveling, C. Monz, P. Pecina, M. Post, H. Saint-Amand, R. Soricut, L. Specia, and A. Tamchyna, ‚ÄúFindings of the 2014 workshop on statistical machine translation,‚Äù in WMT@ACL. The Association

75

for Computer Linguistics, 2014, pp. 12‚Äì58. [369] O. Bojar, R. Chatterjee, C. Federmann, Y. Graham,
B. Haddow, M. Huck, A. Jimeno-Yepes, P. Koehn, V. Logacheva, C. Monz, M. Negri, A. Ne¬¥ve¬¥ol, M. L. Neves, M. Popel, M. Post, R. Rubino, C. Scarton, L. Specia, M. Turchi, K. Verspoor, and M. Zampieri, ‚ÄúFindings of the 2016 conference on machine translation,‚Äù in WMT. The Association for Computer Linguistics, 2016, pp. 131‚Äì198. [370] L. Barrault, O. Bojar, M. R. Costa-jussa`, C. Federmann, M. Fishel, Y. Graham, B. Haddow, M. Huck, P. Koehn, S. Malmasi, C. Monz, M. Mu¬® ller, S. Pal, M. Post, and M. Zampieri, ‚ÄúFindings of the 2019 conference on machine translation (WMT19),‚Äù in Proceedings of the Fourth Conference on Machine Translation, WMT 2019, Florence, Italy, August 1-2, 2019 - Volume 2: Shared Task Papers, Day 1, O. Bojar, R. Chatterjee, C. Federmann, M. Fishel, Y. Graham, B. Haddow, M. Huck, A. Jimeno-Yepes, P. Koehn, A. Martins, C. Monz, M. Negri, A. Ne¬¥ve¬¥ol, M. L. Neves, M. Post, M. Turchi, and K. Verspoor, Eds. Association for Computational Linguistics, 2019, pp. 1‚Äì61. [371] L. Barrault, M. Biesialska, O. Bojar, M. R. Costajussa`, C. Federmann, Y. Graham, R. Grundkiewicz, B. Haddow, M. Huck, E. Joanis, T. Kocmi, P. Koehn, C. Lo, N. Ljubesic, C. Monz, M. Morishita, M. Nagata, T. Nakazawa, S. Pal, M. Post, and M. Zampieri, ‚ÄúFindings of the 2020 conference on machine translation (WMT20),‚Äù in Proceedings of the Fifth Conference on Machine Translation, WMT@EMNLP 2020, Online, November 19-20, 2020, L. Barrault, O. Bojar, F. Bougares, R. Chatterjee, M. R. Costa-jussa`, C. Federmann, M. Fishel, A. Fraser, Y. Graham, P. Guzman, B. Haddow, M. Huck, A. Jimeno-Yepes, P. Koehn, A. Martins, M. Morishita, C. Monz, M. Nagata, T. Nakazawa, and M. Negri, Eds. Association for Computational Linguistics, 2020, pp. 1‚Äì55. [372] F. Akhbardeh, A. Arkhangorodsky, M. Biesialska, O. Bojar, R. Chatterjee, V. Chaudhary, M. R. Costajussa`, C. EspanÀú a-Bonet, A. Fan, C. Federmann, M. Freitag, Y. Graham, R. Grundkiewicz, B. Haddow, L. Harter, K. Heafield, C. Homan, M. Huck, K. AmponsahKaakyire, J. Kasai, D. Khashabi, K. Knight, T. Kocmi, P. Koehn, N. Lourie, C. Monz, M. Morishita, M. Nagata, A. Nagesh, T. Nakazawa, M. Negri, S. Pal, A. A. Tapo, M. Turchi, V. Vydrin, and M. Zampieri, ‚ÄúFindings of the 2021 conference on machine translation (WMT21),‚Äù in Proceedings of the Sixth Conference on Machine Translation, WMT@EMNLP 2021, Online Event, November 10-11, 2021, L. Barrault, O. Bojar, F. Bougares, R. Chatterjee, M. R. Costa-jussa`, C. Federmann, M. Fishel, A. Fraser, M. Freitag, Y. Graham, R. Grundkiewicz, P. Guzman, B. Haddow, M. Huck, A. Jimeno-Yepes, P. Koehn, T. Kocmi, A. Martins, M. Morishita, and C. Monz, Eds. Association for Computational Linguistics, 2021, pp. 1‚Äì88. [373] T. Kocmi, R. Bawden, O. Bojar, A. Dvorkovich, C. Federmann, M. Fishel, T. Gowda, Y. Graham, R. Grundkiewicz, B. Haddow, R. Knowles, P. Koehn, C. Monz, M. Morishita, M. Nagata, T. Nakazawa, M. Nova¬¥k, M. Popel, and M. Popovic, ‚ÄúFindings of the 2022

conference on machine translation (WMT22),‚Äù in Proceedings of the Seventh Conference on Machine Translation, WMT 2022, Abu Dhabi, United Arab Emirates (Hybrid), December 7-8, 2022, P. Koehn, L. Barrault, O. Bojar, F. Bougares, R. Chatterjee, M. R. Costajussa`, C. Federmann, M. Fishel, A. Fraser, M. Freitag, Y. Graham, R. Grundkiewicz, P. Guzman, B. Haddow, M. Huck, A. Jimeno-Yepes, T. Kocmi, A. Martins, M. Morishita, C. Monz, M. Nagata, T. Nakazawa, M. Negri, A. Ne¬¥ve¬¥ol, M. Neves, M. Popel, M. Turchi, and M. Zampieri, Eds. Association for Computational Linguistics, 2022, pp. 1‚Äì45. [374] N. Goyal, C. Gao, V. Chaudhary, P. Chen, G. Wenzek, D. Ju, S. Krishnan, M. Ranzato, F. Guzma¬¥n, and A. Fan, ‚ÄúThe flores-101 evaluation benchmark for lowresource and multilingual machine translation,‚Äù Trans. Assoc. Comput. Linguistics, vol. 10, pp. 522‚Äì538, 2022. [375] R. Bawden, E. Bilinski, T. Lavergne, and S. Rosset, ‚ÄúDiabla: a corpus of bilingual spontaneous written dialogues for machine translation,‚Äù Lang. Resour. Evaluation, vol. 55, no. 3, pp. 635‚Äì660, 2021. [376] R. Nallapati, B. Zhou, C. N. dos Santos, C¬∏ . Gu¬® lc¬∏ehre, and B. Xiang, ‚ÄúAbstractive text summarization using sequence-to-sequence rnns and beyond,‚Äù in Proceedings of the 20th SIGNLL Conference on Computational Natural Language Learning, CoNLL 2016, Berlin, Germany, August 11-12, 2016, Y. Goldberg and S. Riezler, Eds. ACL, 2016, pp. 280‚Äì290. [377] S. Narayan, S. B. Cohen, and M. Lapata, ‚ÄúDon‚Äôt give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization,‚Äù in EMNLP. Association for Computational Linguistics, 2018, pp. 1797‚Äì1807. [378] F. Ladhak, E. Durmus, C. Cardie, and K. Mckeown, ‚ÄúWikilingua: A new benchmark dataset for crosslingual abstractive summarization,‚Äù in Findings of the Association for Computational Linguistics: EMNLP 2020, 2020, pp. 4034‚Äì4048. [379] S. Moon, P. Shah, A. Kumar, and R. Subba, ‚ÄúOpendialkg: Explainable conversational reasoning with attention-based walks over knowledge graphs,‚Äù in ACL (1). Association for Computational Linguistics, 2019, pp. 845‚Äì854. [380] D. Hendrycks, S. Basart, S. Kadavath, M. Mazeika, A. Arora, E. Guo, C. Burns, S. Puranik, H. He, D. Song, and J. Steinhardt, ‚ÄúMeasuring coding challenge competence with APPS,‚Äù in NeurIPS Datasets and Benchmarks, 2021. [381] Y. Lai, C. Li, Y. Wang, T. Zhang, R. Zhong, L. Zettlemoyer, S. W. Yih, D. Fried, S. I. Wang, and T. Yu, ‚ÄúDS-1000: A natural and reliable benchmark for data science code generation,‚Äù CoRR, vol. abs/2211.11501, 2022. [382] Z. Wang, S. Zhou, D. Fried, and G. Neubig, ‚ÄúExecution-based evaluation for open-domain code generation,‚Äù CoRR, vol. abs/2212.10481, 2022. [383] T. Kwiatkowski, J. Palomaki, O. Redfield, M. Collins, A. P. Parikh, C. Alberti, D. Epstein, I. Polosukhin, J. Devlin, K. Lee, K. Toutanova, L. Jones, M. Kelcey, M. Chang, A. M. Dai, J. Uszkoreit, Q. Le, and S. Petrov, ‚ÄúNatural questions: a benchmark for question answer-

76

ing research,‚Äù Trans. Assoc. Comput. Linguistics, pp. 452‚Äì466, 2019. [384] P. Clark, I. Cowhey, O. Etzioni, T. Khot, A. Sabharwal, C. Schoenick, and O. Tafjord, ‚ÄúThink you have solved question answering? try arc, the AI2 reasoning challenge,‚Äù CoRR, vol. abs/1803.05457, 2018. [385] S. Lin, J. Hilton, and O. Evans, ‚ÄúTruthfulqa: Measuring how models mimic human falsehoods,‚Äù in Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022, 2022, pp. 3214‚Äì3252. [386] J. Berant, A. Chou, R. Frostig, and P. Liang, ‚ÄúSemantic parsing on freebase from question-answer pairs,‚Äù in Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, EMNLP 2013, 18-21 October 2013, Grand Hyatt Seattle, Seattle, Washington, USA, A meeting of SIGDAT, a Special Interest Group of the ACL, 2013, pp. 1533‚Äì1544. [387] M. Joshi, E. Choi, D. S. Weld, and L. Zettlemoyer, ‚ÄúTriviaqa: A large scale distantly supervised challenge dataset for reading comprehension,‚Äù in Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, ACL 2017, Vancouver, Canada, July 30 - August 4, Volume 1: Long Papers, 2017, pp. 1601‚Äì1611. [388] Y. Bisk, R. Zellers, R. L. Bras, J. Gao, and Y. Choi, ‚ÄúPIQA: reasoning about physical commonsense in natural language,‚Äù in The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The ThirtySecond Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020, 2020, pp. 7432‚Äì7439. [389] M. Dubey, D. Banerjee, A. Abdelkawi, and J. Lehmann, ‚ÄúLc-quad 2.0: A large dataset for complex question answering over wikidata and dbpedia,‚Äù in The Semantic Web - ISWC 2019 - 18th International Semantic Web Conference, Auckland, New Zealand, October 26-30, 2019, Proceedings, Part II, 2019, pp. 69‚Äì78. [390] Y. Gu, S. Kase, M. Vanni, B. M. Sadler, P. Liang, X. Yan, and Y. Su, ‚ÄúBeyond I.I.D.: three levels of generalization for question answering on knowledge bases,‚Äù in WWW ‚Äô21: The Web Conference 2021, Virtual Event / Ljubljana, Slovenia, April 19-23, 2021, 2021, pp. 3477‚Äì 3488. [391] S. Cao, J. Shi, L. Pan, L. Nie, Y. Xiang, L. Hou, J. Li, B. He, and H. Zhang, ‚ÄúKQA pro: A dataset with explicit compositional programs for complex question answering over knowledge base,‚Äù in Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022, 2022, pp. 6101‚Äì6119. [392] X. Hu, X. Wu, Y. Shu, and Y. Qu, ‚ÄúLogical form generation via multi-task learning for complex question answering over knowledge bases,‚Äù in Proceedings of the 29th International Conference on Computational Linguistics, COLING 2022, Gyeongju, Republic of Korea, October 12-17, 2022, 2022, pp. 1687‚Äì1696. [393] S. Longpre, Y. Lu, and J. Daiber, ‚ÄúMKQA: A linguistically diverse benchmark for multilingual open

domain question answering,‚Äù Trans. Assoc. Comput. Linguistics, vol. 9, pp. 1389‚Äì1406, 2021. [394] T. Saikh, T. Ghosal, A. Mittal, A. Ekbal, and P. Bhattacharyya, ‚ÄúScienceqa: a novel resource for question answering on scholarly articles,‚Äù Int. J. Digit. Libr., vol. 23, no. 3, pp. 289‚Äì301, 2022. [395] T. Mihaylov, P. Clark, T. Khot, and A. Sabharwal, ‚ÄúCan a suit of armor conduct electricity? A new dataset for open book question answering,‚Äù in Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Brussels, Belgium, October 31 November 4, 2018, 2018, pp. 2381‚Äì2391. [396] T. Nguyen, M. Rosenberg, X. Song, J. Gao, S. Tiwary, R. Majumder, and L. Deng, ‚ÄúMS MARCO: A human generated machine reading comprehension dataset,‚Äù in Proceedings of the Workshop on Cognitive Computation: Integrating neural and symbolic approaches 2016 co-located with the 30th Annual Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain, December 9, 2016, 2016. [397] T. Khot, P. Clark, M. Guerquin, P. Jansen, and A. Sabharwal, ‚ÄúQASC: A dataset for question answering via sentence composition,‚Äù in The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020, 2020, pp. 8082‚Äì8090. [398] P. Rajpurkar, J. Zhang, K. Lopyrev, and P. Liang, ‚ÄúSquad: 100, 000+ questions for machine comprehension of text,‚Äù in Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, EMNLP 2016, Austin, Texas, USA, November 1-4, 2016, 2016, pp. 2383‚Äì2392. [399] A. H. Miller, A. Fisch, J. Dodge, A. Karimi, A. Bordes, and J. Weston, ‚ÄúKey-value memory networks for directly reading documents,‚Äù in Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, EMNLP 2016, Austin, Texas, USA, November 1-4, 2016, 2016, pp. 1400‚Äì1409. [400] B. Goodrich, V. Rao, P. J. Liu, and M. Saleh, ‚ÄúAssessing the factual accuracy of generated text,‚Äù in Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, KDD 2019, Anchorage, AK, USA, August 4-8, 2019, 2019, pp. 166‚Äì175. [401] K. Toutanova and D. Chen, ‚ÄúObserved versus latent features for knowledge base and text inference,‚Äù in Proceedings of the 3rd Workshop on Continuous Vector Space Models and their Compositionality, CVSC 2015, Beijing, China, July 26-31, 2015, 2015, pp. 57‚Äì66. [402] K. D. Bollacker, C. Evans, P. K. Paritosh, T. Sturge, and J. Taylor, ‚ÄúFreebase: a collaboratively created graph database for structuring human knowledge,‚Äù in Proceedings of the ACM SIGMOD International Conference on Management of Data, SIGMOD 2008, Vancouver, BC, Canada, June 10-12, 2008, 2008, pp. 1247‚Äì1250. [403] T. Dettmers, P. Minervini, P. Stenetorp, and S. Riedel, ‚ÄúConvolutional 2d knowledge graph embeddings,‚Äù in Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Ap-

77

plications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18), New Orleans, Louisiana, USA, February 2-7, 2018, 2018, pp. 1811‚Äì1818. [404] G. A. Miller, ‚ÄúWordnet: A lexical database for english,‚Äù Commun. ACM, pp. 39‚Äì41, 1995. [405] F. Petroni, T. Rockta¬®schel, S. Riedel, P. S. H. Lewis, A. Bakhtin, Y. Wu, and A. H. Miller, ‚ÄúLanguage models as knowledge bases?‚Äù in Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019, Hong Kong, China, November 3-7, 2019, 2019, pp. 2463‚Äì 2473. [406] F. Mahdisoltani, J. Biega, and F. M. Suchanek, ‚ÄúYAGO3: A knowledge base from multilingual wikipedias,‚Äù in Seventh Biennial Conference on Innovative Data Systems Research, CIDR 2015, Asilomar, CA, USA, January 4-7, 2015, Online Proceedings, 2015. [407] F. M. Suchanek, G. Kasneci, and G. Weikum, ‚ÄúYago: a core of semantic knowledge,‚Äù in Proceedings of the 16th International Conference on World Wide Web, WWW 2007, Banff, Alberta, Canada, May 8-12, 2007, 2007, pp. 697‚Äì706. [408] M. Geva, D. Khashabi, E. Segal, T. Khot, D. Roth, and J. Berant, ‚ÄúDid aristotle use a laptop? A question answering benchmark with implicit reasoning strategies,‚Äù Trans. Assoc. Comput. Linguistics, vol. 9, pp. 346‚Äì 361, 2021. [409] Z. Yang, P. Qi, S. Zhang, Y. Bengio, W. W. Cohen, R. Salakhutdinov, and C. D. Manning, ‚ÄúHotpotqa: A dataset for diverse, explainable multi-hop question answering,‚Äù in Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Brussels, Belgium, October 31 - November 4, 2018. Association for Computational Linguistics, 2018, pp. 2369‚Äì 2380. [410] C. Clark, K. Lee, M. Chang, T. Kwiatkowski, M. Collins, and K. Toutanova, ‚ÄúBoolq: Exploring the surprising difficulty of natural yes/no questions,‚Äù in Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers), J. Burstein, C. Doran, and T. Solorio, Eds. Association for Computational Linguistics, 2019, pp. 2924‚Äì2936. [411] M. Sap, H. Rashkin, D. Chen, R. L. Bras, and Y. Choi, ‚ÄúSocialiqa: Commonsense reasoning about social interactions,‚Äù CoRR, vol. abs/1904.09728, 2019. [412] R. Zellers, A. Holtzman, Y. Bisk, A. Farhadi, and Y. Choi, ‚ÄúHellaswag: Can a machine really finish your sentence?‚Äù in Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28- August 2, 2019, Volume 1: Long Papers, A. Korhonen, D. R. Traum, and L. Ma`rquez, Eds. Association for Computational Linguistics, 2019, pp. 4791‚Äì4800. [413] K. Sakaguchi, R. L. Bras, C. Bhagavatula, and Y. Choi, ‚ÄúWinogrande: An adversarial winograd schema challenge at scale,‚Äù in AAAI. AAAI Press, 2020, pp. 8732‚Äì

8740. [414] M. Roemmele, C. A. Bejan, and A. S. Gordon, ‚ÄúChoice
of plausible alternatives: An evaluation of commonsense causal reasoning,‚Äù in Logical Formalizations of Commonsense Reasoning, Papers from the 2011 AAAI Spring Symposium, Technical Report SS-11-06, Stanford, California, USA, March 21-23, 2011. AAAI, 2011. [415] K. Sakaguchi, C. Bhagavatula, R. L. Bras, N. Tandon, P. Clark, and Y. Choi, ‚Äúproscript: Partially ordered scripts generation,‚Äù in Findings of the Association for Computational Linguistics: EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 16-20 November, 2021, M. Moens, X. Huang, L. Specia, and S. W. Yih, Eds. Association for Computational Linguistics, 2021, pp. 2138‚Äì2149. [416] B. Dalvi, L. Huang, N. Tandon, W. Yih, and P. Clark, ‚ÄúTracking state changes in procedural text: a challenge dataset and models for process paragraph comprehension,‚Äù in Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2018, New Orleans, Louisiana, USA, June 1-6, 2018, Volume 1 (Long Papers), M. A. Walker, H. Ji, and A. Stent, Eds. Association for Computational Linguistics, 2018, pp. 1595‚Äì1604. [417] S. Saha, P. Yadav, L. Bauer, and M. Bansal, ‚ÄúExplagraphs: An explanation graph generation task for structured commonsense reasoning,‚Äù in Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 7-11 November, 2021, M. Moens, X. Huang, L. Specia, and S. W. Yih, Eds. Association for Computational Linguistics, 2021, pp. 7716‚Äì7740. [418] O. Tafjord, B. Dalvi, and P. Clark, ‚ÄúProofwriter: Generating implications, proofs, and abductive statements over natural language,‚Äù in Findings of the Association for Computational Linguistics: ACL/IJCNLP 2021, Online Event, August 1-6, 2021, ser. Findings of ACL, C. Zong, F. Xia, W. Li, and R. Navigli, Eds., vol. ACL/IJCNLP 2021. Association for Computational Linguistics, 2021, pp. 3621‚Äì3634. [419] B. Dalvi, P. Jansen, O. Tafjord, Z. Xie, H. Smith, L. Pipatanangkura, and P. Clark, ‚ÄúExplaining answers with entailment trees,‚Äù in Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 7-11 November, 2021, M. Moens, X. Huang, L. Specia, and S. W. Yih, Eds. Association for Computational Linguistics, 2021, pp. 7358‚Äì7370. [420] A. Saparov and H. He, ‚ÄúLanguage models are greedy reasoners: A systematic formal analysis of chain-ofthought,‚Äù CoRR, vol. abs/2210.01240, 2022. [421] C. Anil, Y. Wu, A. Andreassen, A. Lewkowycz, V. Misra, V. V. Ramasesh, A. Slone, G. Gur-Ari, E. Dyer, and B. Neyshabur, ‚ÄúExploring length generalization in large language models,‚Äù CoRR, vol. abs/2207.04901, 2022. [422] K. Cobbe, V. Kosaraju, M. Bavarian, J. Hilton, R. Nakano, C. Hesse, and J. Schulman, ‚ÄúTraining verifiers to solve math word problems,‚Äù CoRR, vol.

78

abs/2110.14168, 2021. [423] A. Patel, S. Bhattamishra, and N. Goyal, ‚ÄúAre NLP
models really able to solve simple math word problems?‚Äù in NAACL-HLT. Association for Computational Linguistics, 2021, pp. 2080‚Äì2094. [424] S. Roy and D. Roth, ‚ÄúSolving general arithmetic word problems,‚Äù in Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, EMNLP 2015, Lisbon, Portugal, September 17-21, 2015, L. Ma`rquez, C. Callison-Burch, J. Su, D. Pighin, and Y. Marton, Eds. The Association for Computational Linguistics, 2015, pp. 1743‚Äì1752. [425] A. Amini, S. Gabriel, S. Lin, R. Koncel-Kedziorski, Y. Choi, and H. Hajishirzi, ‚ÄúMathqa: Towards interpretable math word problem solving with operationbased formalisms,‚Äù in Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers), J. Burstein, C. Doran, and T. Solorio, Eds. Association for Computational Linguistics, 2019, pp. 2357‚Äì2367. [426] W. Ling, D. Yogatama, C. Dyer, and P. Blunsom, ‚ÄúProgram induction by rationale generation: Learning to solve and explain algebraic word problems,‚Äù in Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, ACL 2017, Vancouver, Canada, July 30 - August 4, Volume 1: Long Papers, R. Barzilay and M. Kan, Eds. Association for Computational Linguistics, 2017, pp. 158‚Äì167. [427] R. Koncel-Kedziorski, S. Roy, A. Amini, N. Kushman, and H. Hajishirzi, ‚ÄúMawps: A math word problem repository,‚Äù in Proceedings of the 2016 conference of the north american chapter of the association for computational linguistics: human language technologies, 2016, pp. 1152‚Äì 1157. [428] D. Dua, Y. Wang, P. Dasigi, G. Stanovsky, S. Singh, and M. Gardner, ‚ÄúDROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs,‚Äù in Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers), 2019, pp. 2368‚Äì 2378. [429] S. Welleck, J. Liu, R. L. Bras, H. Hajishirzi, Y. Choi, and K. Cho, ‚ÄúNaturalproofs: Mathematical theorem proving in natural language,‚Äù in Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021, December 2021, virtual, J. Vanschoren and S. Yeung, Eds., 2021. [430] A. Q. Jiang, W. Li, J. M. Han, and Y. Wu, ‚ÄúLisa: Language models of isabelle proofs,‚Äù in 6th Conference on Artificial Intelligence and Theorem Proving, 2021, pp. 378‚Äì392. [431] K. Zheng, J. M. Han, and S. Polu, ‚Äúminif2f: a crosssystem benchmark for formal olympiad-level mathematics,‚Äù in The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 2529, 2022. OpenReview.net, 2022.

[432] Z. Azerbayev, B. Piotrowski, H. Schoelkopf, E. W. Ayers, D. Radev, and J. Avigad, ‚ÄúProofnet: Autoformalizing and formally proving undergraduate-level mathematics,‚Äù CoRR, vol. abs/2302.12433, 2023.
[433] D. Bahdanau, K. Cho, and Y. Bengio, ‚ÄúNeural machine translation by jointly learning to align and translate,‚Äù in ICLR, 2015.
[434] K. Papineni, S. Roukos, T. Ward, and W. Zhu, ‚ÄúBleu: a method for automatic evaluation of machine translation,‚Äù in Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, July 6-12, 2002, Philadelphia, PA, USA. ACL, 2002, pp. 311‚Äì318.
[435] C.-Y. Lin, ‚ÄúROUGE: A package for automatic evaluation of summaries,‚Äù in Text Summarization Branches Out. Association for Computational Linguistics, Jul. 2004, pp. 74‚Äì81.
[436] W. Jiao, W. Wang, J.-t. Huang, X. Wang, and Z. Tu, ‚ÄúIs chatgpt a good translator? a preliminary study,‚Äù arXiv preprint arXiv:2301.08745, 2023.
[437] T. Zhang, F. Ladhak, E. Durmus, P. Liang, K. R. McKeown, and T. B. Hashimoto, ‚ÄúBenchmarking large language models for news summarization,‚Äù CoRR, vol. abs/2301.13848, 2023.
[438] T. Goyal, J. J. Li, and G. Durrett, ‚ÄúNews summarization and evaluation in the era of GPT-3,‚Äù CoRR, vol. abs/2209.12356, 2022.
[439] S. Gehrmann, E. Clark, and T. Sellam, ‚ÄúRepairing the cracked foundation: A survey of obstacles in evaluation practices for generated text,‚Äù CoRR, vol. abs/2202.06935, 2022.
[440] J. Wang, Y. Liang, F. Meng, H. Shi, Z. Li, J. Xu, J. Qu, and J. Zhou, ‚ÄúIs chatgpt a good NLG evaluator? A preliminary study,‚Äù CoRR, vol. abs/2303.04048, 2023.
[441] Y. Liu, D. Iter, Y. Xu, S. Wang, R. Xu, and C. Zhu, ‚ÄúGeval: NLG evaluation using GPT-4 with better human alignment,‚Äù CoRR, vol. abs/2303.16634, 2023.
[442] J. Jiang, K. Zhou, Z. Dong, K. Ye, W. X. Zhao, and J. Wen, ‚ÄúStructgpt: A general framework for large language model to reason over structured data,‚Äù CoRR, vol. abs/2305.09645, 2023.
[443] K. Yang, Y. Tian, N. Peng, and D. Klein, ‚ÄúRe3: Generating longer stories with recursive reprompting and revision,‚Äù in Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022, Y. Goldberg, Z. Kozareva, and Y. Zhang, Eds. Association for Computational Linguistics, 2022, pp. 4393‚Äì4479.
[444] W. Zhou, Y. E. Jiang, P. Cui, T. Wang, Z. Xiao, Y. Hou, R. Cotterell, and M. Sachan, ‚ÄúRecurrentgpt: Interactive generation of (arbitrarily) long text,‚Äù CoRR, vol. abs/2305.13304, 2023.
[445] S. Gulwani, O. Polozov, and R. Singh, ‚ÄúProgram synthesis,‚Äù Found. Trends Program. Lang., vol. 4, no. 1-2, pp. 1‚Äì119, 2017.
[446] S. Zhang, Z. Chen, Y. Shen, M. Ding, J. B. Tenenbaum, and C. Gan, ‚ÄúPlanning with large language models for code generation,‚Äù 2023.
[447] M. Welsh, ‚ÄúThe end of programming,‚Äù Commun. ACM, vol. 66, no. 1, pp. 34‚Äì35, 2023.
[448] Y. Bang, S. Cahyawijaya, N. Lee, W. Dai, D. Su,

79

B. Wilie, H. Lovenia, Z. Ji, T. Yu, W. Chung, Q. V. Do, Y. Xu, and P. Fung, ‚ÄúA multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity,‚Äù CoRR, vol. abs/2302.04023, 2023. [449] Y. Liu, A. R. Fabbri, P. Liu, Y. Zhao, L. Nan, R. Han, S. Han, S. R. Joty, C. Wu, C. Xiong, and D. Radev, ‚ÄúRevisiting the gold standard: Grounding summarization evaluation with robust human evaluation,‚Äù CoRR, vol. abs/2212.07981, 2022. [450] A. R. Fabbri, W. Kryscinski, B. McCann, C. Xiong, R. Socher, and D. R. Radev, ‚ÄúSummeval: Re-evaluating summarization evaluation,‚Äù Trans. Assoc. Comput. Linguistics, vol. 9, pp. 391‚Äì409, 2021. [451] T. Tang, H. Lu, Y. E. Jiang, H. Huang, D. Zhang, W. X. Zhao, and F. Wei, ‚ÄúNot all metrics are guilty: Improving NLG evaluation with LLM paraphrasing,‚Äù CoRR, vol. abs/2305.15067, 2023. [452] X. Wang, X. Tang, W. X. Zhao, J. Wang, and J. Wen, ‚ÄúRethinking the evaluation for conversational recommendation in the era of large language models,‚Äù CoRR, vol. abs/2305.13112, 2023. [453] M. Gao, J. Ruan, R. Sun, X. Yin, S. Yang, and X. Wan, ‚ÄúHuman-like summarization evaluation with chatgpt,‚Äù CoRR, vol. abs/2304.02554, 2023. [454] Y. Ji, Y. Gong, Y. Peng, C. Ni, P. Sun, D. Pan, B. Ma, and X. Li, ‚ÄúExploring chatgpt‚Äôs ability to rank content: A preliminary study on consistency with human preferences,‚Äù CoRR, vol. abs/2303.07610, 2023. [455] Y. Bai, J. Ying, Y. Cao, X. Lv, Y. He, X. Wang, J. Yu, K. Zeng, Y. Xiao, H. Lyu, J. Zhang, J. Li, and L. Hou, ‚ÄúBenchmarking foundation models with languagemodel-as-an-examiner,‚Äù CoRR, vol. abs/2306.04181, 2023. [456] Y. Liu, S. Feng, D. Wang, Y. Zhang, and H. Schu¬® tze, ‚ÄúEvaluate what you can‚Äôt evaluate: Unassessable generated responses quality,‚Äù CoRR, vol. abs/2305.14658, 2023. [457] P. Wang, L. Li, L. Chen, D. Zhu, B. Lin, Y. Cao, Q. Liu, T. Liu, and Z. Sui, ‚ÄúLarge language models are not fair evaluators,‚Äù CoRR, vol. abs/2305.17926, 2023. [458] J. Ye, X. Chen, N. Xu, C. Zu, Z. Shao, S. Liu, Y. Cui, Z. Zhou, C. Gong, Y. Shen, J. Zhou, S. Chen, T. Gui, Q. Zhang, and X. Huang, ‚ÄúA comprehensive capability analysis of gpt-3 and gpt-3.5 series models,‚Äù arXiv preprint arXiv:2303.10420, 2023. [459] M. McCloskey and N. J. Cohen, ‚ÄúCatastrophic interference in connectionist networks: The sequential learning problem,‚Äù in Psychology of learning and motivation, 1989, pp. 109‚Äì165. [460] R. Kemker, M. McClure, A. Abitino, T. L. Hayes, and C. Kanan, ‚ÄúMeasuring catastrophic forgetting in neural networks,‚Äù in Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18), New Orleans, Louisiana, USA, February 2-7, 2018, 2018, pp. 3390‚Äì3398. [461] T. Xie, C. H. Wu, P. Shi, R. Zhong, T. Scholak, M. Yasunaga, C. Wu, M. Zhong, P. Yin, S. I. Wang, V. Zhong,

B. Wang, C. Li, C. Boyle, A. Ni, Z. Yao, D. Radev, C. Xiong, L. Kong, R. Zhang, N. A. Smith, L. Zettlemoyer, and T. Yu, ‚ÄúUnifiedskg: Unifying and multitasking structured knowledge grounding with text-totext language models,‚Äù in EMNLP. Association for Computational Linguistics, 2022, pp. 602‚Äì631. [462] A. Roberts, C. Raffel, and N. Shazeer, ‚ÄúHow much knowledge can you pack into the parameters of a language model?‚Äù in Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online, November 16-20, 2020, 2020, pp. 5418‚Äì5426. [463] G. Izacard, P. S. H. Lewis, M. Lomeli, L. Hosseini, F. Petroni, T. Schick, J. Dwivedi-Yu, A. Joulin, S. Riedel, and E. Grave, ‚ÄúFew-shot learning with retrieval augmented language models,‚Äù CoRR, vol. abs/2208.03299, 2022. [464] K. Guu, K. Lee, Z. Tung, P. Pasupat, and M. Chang, ‚ÄúRetrieval augmented language model pre-training,‚Äù in Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, 2020, pp. 3929‚Äì3938. [465] P. S. H. Lewis, E. Perez, A. Piktus, F. Petroni, V. Karpukhin, N. Goyal, H. Ku¬® ttler, M. Lewis, W. Yih, T. Rockta¬®schel, S. Riedel, and D. Kiela, ‚ÄúRetrievalaugmented generation for knowledge-intensive NLP tasks,‚Äù in Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. [466] Y. Lan, G. He, J. Jiang, J. Jiang, W. X. Zhao, and J. Wen, ‚ÄúComplex knowledge base question answering: A survey,‚Äù CoRR, vol. abs/2108.06688, 2021. [467] S. Borgeaud, A. Mensch, J. Hoffmann, T. Cai, E. Rutherford, K. Millican, G. van den Driessche, J. Lespiau, B. Damoc, A. Clark, D. de Las Casas, A. Guy, J. Menick, R. Ring, T. Hennigan, S. Huang, L. Maggiore, C. Jones, A. Cassirer, A. Brock, M. Paganini, G. Irving, O. Vinyals, S. Osindero, K. Simonyan, J. W. Rae, E. Elsen, and L. Sifre, ‚ÄúImproving language models by retrieving from trillions of tokens,‚Äù in International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA, ser. Proceedings of Machine Learning Research, K. Chaudhuri, S. Jegelka, L. Song, C. Szepesva¬¥ri, G. Niu, and S. Sabato, Eds., vol. 162. PMLR, 2022, pp. 2206‚Äì2240. [468] S. Xu, L. Pang, H. Shen, X. Cheng, and T.-S. Chua, ‚ÄúSearch-in-the-chain: Towards accurate, credible and traceable large language models for knowledgeintensive tasks,‚Äù CoRR, vol. abs/2304.14732, 2023. [469] B. Peng, M. Galley, P. He, H. Cheng, Y. Xie, Y. Hu, Q. Huang, L. Liden, Z. Yu, W. Chen, and J. Gao, ‚ÄúCheck your facts and try again: Improving large language models with external knowledge and automated feedback,‚Äù CoRR, vol. abs/2302.12813, 2023. [470] Z. Jiang, F. F. Xu, L. Gao, Z. Sun, Q. Liu, J. DwivediYu, Y. Yang, J. Callan, and G. Neubig, ‚ÄúActive retrieval augmented generation,‚Äù CoRR, vol. abs/2305.06983, 2023. [471] J. Li, X. Cheng, W. X. Zhao, J. Nie, and J. Wen,

80

‚ÄúHalueval: A large-scale hallucination evaluation benchmark for large language models,‚Äù CoRR, vol. abs/2305.11747, 2023. [472] Y. Li, Y. Du, K. Zhou, J. Wang, W. X. Zhao, and J. Wen, ‚ÄúEvaluating object hallucination in large vision-language models,‚Äù CoRR, vol. abs/2305.10355, 2023. [473] S. Kadavath, T. Conerly, A. Askell, T. J. Henighan, D. Drain, E. Perez, N. Schiefer, Z. Dodds, N. DasSarma, E. Tran-Johnson, S. Johnston, S. El-Showk, A. Jones, N. Elhage, T. Hume, A. Chen, Y. Bai, S. Bowman, S. Fort, D. Ganguli, D. Hernandez, J. Jacobson, J. Kernion, S. Kravec, L. Lovitt, K. Ndousse, C. Olsson, S. Ringer, D. Amodei, T. B. Brown, J. Clark, N. Joseph, B. Mann, S. McCandlish, C. Olah, and J. Kaplan, ‚ÄúLanguage models (mostly) know what they know,‚Äù CoRR, vol. abs/2207.05221, 2022. [474] P. Manakul, A. Liusie, and M. J. F. Gales, ‚ÄúSelfcheckgpt: Zero-resource black-box hallucination detection for generative large language models,‚Äù ArXiv, vol. abs/2305.06983, 2023. [475] S. Agarwal, I. Akkaya, V. Balcom, M. Bavarian, G. Bernadett-Shapiro, G. Brockman, M. Brundage, J. Chan, F. Chantzis, N. Deutsch, B. Eastman, A. Eleti, N. Felix, S. P. Fishman, I. Fulford, C. Gibson, J. Gross, M. Heaton, J. Hilton, X. Hu, S. Jain, H. Jin, L. Kilpatrick, C. Kim, M. Kolhede, A. Mayne, P. McMillan, D. Medina, J. Menick, A. Mishchenko, A. Nair, R. Nayak, A. Neelakantan, R. Nuttall, J. Parish, A. T. Passos, A. Perelman, F. de Avila Belbute Peres, V. Pong, J. Schulman, E. Sigler, N. Staudacher, N. Turley, J. Tworek, R. Greene, A. Vijayvergiya, C. Voss, J. Weng, M. Wiethoff, S. Yoo, K. Yu, W. Zaremba, S. Zhao, W. Zhuk, and B. Zoph, ‚ÄúChatgpt plugins,‚Äù OpenAI Blog, March 2023. [476] A. Lazaridou, E. Gribovskaya, W. Stokowiec, and N. Grigorev, ‚ÄúInternet-augmented language models through few-shot prompting for open-domain question answering,‚Äù CoRR, vol. abs/2203.05115, 2022. [477] H. Qian, Y. Zhu, Z. Dou, H. Gu, X. Zhang, Z. Liu, R. Lai, Z. Cao, J. Nie, and J. Wen, ‚ÄúWebbrain: Learning to generate factually correct articles for queries by grounding on large web corpus,‚Äù CoRR, vol. abs/2304.04358, 2023. [478] J. Liu, J. Jin, Z. Wang, J. Cheng, Z. Dou, and J. Wen, ‚ÄúRETA-LLM: A retrieval-augmented large language model toolkit,‚Äù CoRR, vol. abs/2306.05212, 2023. [479] D. Dai, L. Dong, Y. Hao, Z. Sui, B. Chang, and F. Wei, ‚ÄúKnowledge neurons in pretrained transformers,‚Äù in Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022, S. Muresan, P. Nakov, and A. Villavicencio, Eds. Association for Computational Linguistics, 2022, pp. 8493‚Äì8502. [480] K. Meng, D. Bau, A. J. Andonian, and Y. Belinkov, ‚ÄúLocating and editing factual associations in gpt,‚Äù in Advances in Neural Information Processing Systems, 2022. [481] M. Geva, R. Schuster, J. Berant, and O. Levy, ‚ÄúTransformer feed-forward layers are key-value memories,‚Äù in Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021,

Virtual Event / Punta Cana, Dominican Republic, 7-11 November, 2021, M. Moens, X. Huang, L. Specia, and S. W. Yih, Eds. Association for Computational Linguistics, 2021, pp. 5484‚Äì5495. [482] Y. Yao, P. Wang, B. Tian, S. Cheng, Z. Li, S. Deng, H. Chen, and N. Zhang, ‚ÄúEditing large language models: Problems, methods, and opportunities,‚Äù CoRR, vol. abs/2305.13172, 2023. [483] Z. Shao, Y. Gong, Y. Shen, M. Huang, N. Duan, and W. Chen, ‚ÄúSynthetic prompting: Generating chain-ofthought demonstrations for large language models,‚Äù CoRR, vol. abs/2302.00618, 2023. [484] Sifatkaur, M. Singh, V. S. B, and N. Malviya, ‚ÄúMind meets machine: Unravelling gpt-4‚Äôs cognitive psychology,‚Äù CoRR, vol. abs/2303.11436, 2023. [485] M. I. Nye, A. J. Andreassen, G. Gur-Ari, H. Michalewski, J. Austin, D. Bieber, D. Dohan, A. Lewkowycz, M. Bosma, D. Luan, C. Sutton, and A. Odena, ‚ÄúShow your work: Scratchpads for intermediate computation with language models,‚Äù CoRR, vol. abs/2112.00114, 2021. [486] J. Qian, H. Wang, Z. Li, S. Li, and X. Yan, ‚ÄúLimitations of language models in arithmetic and symbolic induction,‚Äù CoRR, vol. abs/2208.05051, 2022. [487] W. X. Zhao, K. Zhou, Z. Gong, B. Zhang, Y. Zhou, J. Sha, Z. Chen, S. Wang, C. Liu, and J. Wen, ‚ÄúJiuzhang: A chinese pre-trained language model for mathematical problem understanding,‚Äù in KDD ‚Äô22: The 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, Washington, DC, USA, August 14 - 18, 2022, A. Zhang and H. Rangwala, Eds. ACM, 2022, pp. 4571‚Äì4581. [488] Q. Wang, C. Kaliszyk, and J. Urban, ‚ÄúFirst experiments with neural translation of informal to formal mathematics,‚Äù in Intelligent Computer Mathematics 11th International Conference, CICM 2018, Hagenberg, Austria, August 13-17, 2018, Proceedings, ser. Lecture Notes in Computer Science, F. Rabe, W. M. Farmer, G. O. Passmore, and A. Youssef, Eds., vol. 11006. Springer, 2018, pp. 255‚Äì270. [489] S. Polu and I. Sutskever, ‚ÄúGenerative language modeling for automated theorem proving,‚Äù CoRR, vol. abs/2009.03393, 2020. [490] A. Q. Jiang, W. Li, S. Tworkowski, K. Czechowski, T. Odrzygo¬¥ zdz, P. Milos, Y. Wu, and M. Jamnik, ‚ÄúThor: Wielding hammers to integrate language models and automated theorem provers,‚Äù CoRR, vol. abs/2205.10893, 2022. [491] S. Polu, J. M. Han, K. Zheng, M. Baksys, I. Babuschkin, and I. Sutskever, ‚ÄúFormal mathematics statement curriculum learning,‚Äù CoRR, vol. abs/2202.01344, 2022. [492] Y. Wu, A. Q. Jiang, W. Li, M. N. Rabe, C. Staats, M. Jamnik, and C. Szegedy, ‚ÄúAutoformalization with large language models,‚Äù CoRR, vol. abs/2205.12615, 2022. [493] A. Q. Jiang, S. Welleck, J. P. Zhou, W. Li, J. Liu, M. Jamnik, T. Lacroix, Y. Wu, and G. Lample, ‚ÄúDraft, sketch, and prove: Guiding formal theorem provers with informal proofs,‚Äù CoRR, vol. abs/2210.12283, 2022. [494] S. Yao, D. Yu, J. Zhao, I. Shafran, T. L. Griffiths, Y. Cao,

81

and K. Narasimhan, ‚ÄúTree of thoughts: Deliberate problem solving with large language models,‚Äù CoRR, vol. abs/2305.10601, 2023. [495] A. Madaan, N. Tandon, P. Gupta, S. Hallinan, L. Gao, S. Wiegreffe, U. Alon, N. Dziri, S. Prabhumoye, Y. Yang, S. Welleck, B. P. Majumder, S. Gupta, A. Yazdanbakhsh, and P. Clark, ‚ÄúSelf-refine: Iterative refinement with self-feedback,‚Äù CoRR, vol. abs/2303.17651, 2023. [496] N. Shinn, B. Labash, and A. Gopinath, ‚ÄúReflexion: an autonomous agent with dynamic memory and selfreflection,‚Äù CoRR, vol. abs/2303.11366, 2023. [497] Z. Gou, Z. Shao, Y. Gong, Y. Shen, Y. Yang, N. Duan, and W. Chen, ‚ÄúCRITIC: large language models can self-correct with tool-interactive critiquing,‚Äù CoRR, vol. abs/2305.11738, 2023. [498] J. Uesato, N. Kushman, R. Kumar, H. F. Song, N. Y. Siegel, L. Wang, A. Creswell, G. Irving, and I. Higgins, ‚ÄúSolving math word problems with process- and outcome-based feedback,‚Äù CoRR, vol. abs/2211.14275, 2022. [499] H. Lightman, V. Kosaraju, Y. Burda, H. Edwards, B. Baker, T. Lee, J. Leike, J. Schulman, I. Sutskever, and K. Cobbe, ‚ÄúLet‚Äôs verify step by step,‚Äù CoRR, vol. abs/2305.20050, 2023. [500] Z. Yuan, H. Yuan, C. Tan, W. Wang, and S. Huang, ‚ÄúHow well do large language models perform in arithmetic tasks?‚Äù CoRR, vol. abs/2304.02015, 2023. [501] X. Pi, Q. Liu, B. Chen, M. Ziyadi, Z. Lin, Q. Fu, Y. Gao, J. Lou, and W. Chen, ‚ÄúReasoning like program executors,‚Äù in Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022, 2022, pp. 761‚Äì779. [502] H. Zhou, A. Nova, H. Larochelle, A. C. Courville, B. Neyshabur, and H. Sedghi, ‚ÄúTeaching algorithmic reasoning via in-context learning,‚Äù CoRR, vol. abs/2211.09066, 2022. [503] A. Parisi, Y. Zhao, and N. Fiedel, ‚ÄúTALM: tool augmented language models,‚Äù CoRR, vol. abs/2205.12255, 2022. [504] N. Nangia, C. Vania, R. Bhalerao, and S. R. Bowman, ‚ÄúCrows-pairs: A challenge dataset for measuring social biases in masked language models,‚Äù in Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online, November 16-20, 2020, 2020, pp. 1953‚Äì1967. [505] R. Rudinger, J. Naradowsky, B. Leonard, and B. V. Durme, ‚ÄúGender bias in coreference resolution,‚Äù in Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT, New Orleans, Louisiana, USA, June 1-6, 2018, Volume 2 (Short Papers), 2018, pp. 8‚Äì14. [506] S. Gehman, S. Gururangan, M. Sap, Y. Choi, and N. A. Smith, ‚ÄúRealtoxicityprompts: Evaluating neural toxic degeneration in language models,‚Äù in Findings of the Association for Computational Linguistics: EMNLP 2020, Online Event, 16-20 November 2020, ser. Findings of ACL, T. Cohn, Y. He, and Y. Liu, Eds., vol. EMNLP 2020. Association for Computational Linguistics,

2020, pp. 3356‚Äì3369. [507] X. Puig, K. Ra, M. Boben, J. Li, T. Wang, S. Fidler,
and A. Torralba, ‚ÄúVirtualhome: Simulating household activities via programs,‚Äù in CVPR. Computer Vision Foundation / IEEE Computer Society, 2018, pp. 8494‚Äì 8502. [508] S. Srivastava, C. Li, M. Lingelbach, R. Mart¬¥ƒ±n-Mart¬¥ƒ±n, F. Xia, K. E. Vainio, Z. Lian, C. Gokmen, S. Buch, C. K. Liu, S. Savarese, H. Gweon, J. Wu, and L. FeiFei, ‚ÄúBEHAVIOR: benchmark for everyday household activities in virtual, interactive, and ecological environments,‚Äù in CoRL, ser. Proceedings of Machine Learning Research, vol. 164. PMLR, 2021, pp. 477‚Äì 490. [509] M. Shridhar, J. Thomason, D. Gordon, Y. Bisk, W. Han, R. Mottaghi, L. Zettlemoyer, and D. Fox, ‚ÄúALFRED: A benchmark for interpreting grounded instructions for everyday tasks,‚Äù in CVPR. Computer Vision Foundation / IEEE, 2020, pp. 10 737‚Äì10 746. [510] M. Shridhar, X. Yuan, M. CoÀÜ te¬¥, Y. Bisk, A. Trischler, and M. J. Hausknecht, ‚ÄúAlfworld: Aligning text and embodied environments for interactive learning,‚Äù in 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021. [511] S. Yao, H. Chen, J. Yang, and K. Narasimhan, ‚ÄúWebshop: Towards scalable real-world web interaction with grounded language agents,‚Äù in NeurIPS, 2022. [512] X. Deng, Y. Gu, B. Zheng, S. Chen, S. Stevens, B. Wang, H. Sun, and Y. Su, ‚ÄúMind2web: Towards a generalist agent for the web,‚Äù CoRR, vol. abs/2306.06070, 2023. [513] W. H. Guss, B. Houghton, N. Topin, P. Wang, C. Codel, M. Veloso, and R. Salakhutdinov, ‚ÄúMinerl: A largescale dataset of minecraft demonstrations,‚Äù in Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI 2019, Macao, China, August 10-16, 2019, S. Kraus, Ed. ijcai.org, 2019, pp. 2442‚Äì2448. [514] L. Fan, G. Wang, Y. Jiang, A. Mandlekar, Y. Yang, H. Zhu, A. Tang, D. Huang, Y. Zhu, and A. Anandkumar, ‚ÄúMinedojo: Building open-ended embodied agents with internet-scale knowledge,‚Äù in NeurIPS, 2022. [515] P. Lu, L. Qiu, K. Chang, Y. N. Wu, S. Zhu, T. Rajpurohit, P. Clark, and A. Kalyan, ‚ÄúDynamic prompt learning via policy gradient for semi-structured mathematical reasoning,‚Äù CoRR, vol. abs/2209.14610, 2022. [516] B. Zhang, K. Zhou, X. Wei, W. X. Zhao, J. Sha, S. Wang, and J. rong Wen, ‚ÄúEvaluating and improving toolaugmented computation-intensive math reasoning,‚Äù CoRR, vol. abs/2306.02408, 2023. [517] R. Yang, L. Song, Y. Li, S. Zhao, Y. Ge, X. Li, and Y. Shan, ‚ÄúGpt4tools: Teaching large language model to use tools via self-instruction,‚Äù CoRR, vol. abs/2305.18752, 2023. [518] S. G. Patil, T. Zhang, X. Wang, and J. E. Gonzalez, ‚ÄúGorilla: Large language model connected with massive apis,‚Äù CoRR, vol. abs/2305.15334, 2023. [519] W. Yih, M. Richardson, C. Meek, M. Chang, and J. Suh, ‚ÄúThe value of semantic parse labeling for knowledge base question answering,‚Äù in Proceedings of the 54th

82

Annual Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12, 2016, Berlin, Germany, Volume 2: Short Papers. The Association for Computer Linguistics, 2016. [520] H. Puerto, G. G. Sahin, and I. Gurevych, ‚ÄúMetaqa: Combining expert agents for multi-skill question answering,‚Äù in Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, EACL 2023, Dubrovnik, Croatia, May 2-6, 2023, A. Vlachos and I. Augenstein, Eds. Association for Computational Linguistics, 2023, pp. 3548‚Äì3562. [521] P. Pasupat and P. Liang, ‚ÄúCompositional semantic parsing on semi-structured tables,‚Äù in Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing, ACL 2015, July 26-31, 2015, Beijing, China, Volume 1: Long Papers. The Association for Computer Linguistics, 2015, pp. 1470‚Äì 1480. [522] V. Zhong, C. Xiong, and R. Socher, ‚ÄúSeq2sql: Generating structured queries from natural language using reinforcement learning,‚Äù CoRR, vol. abs/1709.00103, 2017. [523] W. Chen, H. Wang, J. Chen, Y. Zhang, H. Wang, S. Li, X. Zhou, and W. Y. Wang, ‚ÄúTabfact: A large-scale dataset for table-based fact verification,‚Äù in 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020. [524] T. Yu, R. Zhang, K. Yang, M. Yasunaga, D. Wang, Z. Li, J. Ma, I. Li, Q. Yao, S. Roman, Z. Zhang, and D. R. Radev, ‚ÄúSpider: A large-scale human-labeled dataset for complex and cross-domain semantic parsing and text-to-sql task,‚Äù in Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Brussels, Belgium, October 31 - November 4, 2018, E. Riloff, D. Chiang, J. Hockenmaier, and J. Tsujii, Eds. Association for Computational Linguistics, 2018, pp. 3911‚Äì3921. [525] W. Huang, P. Abbeel, D. Pathak, and I. Mordatch, ‚ÄúLanguage models as zero-shot planners: Extracting actionable knowledge for embodied agents,‚Äù in ICML, ser. Proceedings of Machine Learning Research, vol. 162. PMLR, 2022, pp. 9118‚Äì9147. [526] T. Carta, C. Romac, T. Wolf, S. Lamprier, O. Sigaud, and P. Oudeyer, ‚ÄúGrounding large language models in interactive environments with online reinforcement learning,‚Äù CoRR, vol. abs/2302.02662, 2023. [527] X. Zhu, Y. Chen, H. Tian, C. Tao, W. Su, C. Yang, G. Huang, B. Li, L. Lu, X. Wang, Y. Qiao, Z. Zhang, and J. Dai, ‚ÄúGhost in the minecraft: Generally capable agents for open-world environments via large language models with text-based knowledge and memory,‚Äù CoRR, vol. abs/2305.17144, 2023. [528] G. Wang, Y. Xie, Y. Jiang, A. Mandlekar, C. Xiao, Y. Zhu, L. Fan, and A. Anandkumar, ‚ÄúVoyager: An open-ended embodied agent with large language models,‚Äù CoRR, vol. abs/2305.16291, 2023. [529] M. Ahn, A. Brohan, N. Brown, Y. Chebotar, O. Cortes, B. David, C. Finn, K. Gopalakrishnan, K. Hausman,

A. Herzog, D. Ho, J. Hsu, J. Ibarz, B. Ichter, A. Irpan, E. Jang, R. J. Ruano, K. Jeffrey, S. Jesmonth, N. J. Joshi, R. Julian, D. Kalashnikov, Y. Kuang, K. Lee, S. Levine, Y. Lu, L. Luu, C. Parada, P. Pastor, J. Quiambao, K. Rao, J. Rettinghouse, D. Reyes, P. Sermanet, N. Sievers, C. Tan, A. Toshev, V. Vanhoucke, F. Xia, T. Xiao, P. Xu, S. Xu, and M. Yan, ‚ÄúDo as I can, not as I say: Grounding language in robotic affordances,‚Äù CoRR, vol. abs/2204.01691, 2022. [530] J. Liang, W. Huang, F. Xia, P. Xu, K. Hausman, B. Ichter, P. Florence, and A. Zeng, ‚ÄúCode as policies: Language model programs for embodied control,‚Äù CoRR, vol. abs/2209.07753, 2022. [531] Y. Fu, H. Peng, T. Khot, and M. Lapata, ‚ÄúImproving language model negotiation with self-play and in-context learning from AI feedback,‚Äù CoRR, vol. abs/2305.10142, 2023. [532] N. Mehta, M. Teruel, P. F. Sanz, X. Deng, A. H. Awadallah, and J. Kiseleva, ‚ÄúImproving grounded language understanding in a collaborative environment by interacting with agents through help feedback,‚Äù CoRR, vol. abs/2304.10750, 2023. [533] S. G. Patil, T. Zhang, X. Wang, and J. E. Gonzalez, ‚ÄúGorilla: Large language model connected with massive apis,‚Äù CoRR, vol. abs/2305.15334, 2023. [534] S. Hao, T. Liu, Z. Wang, and Z. Hu, ‚ÄúToolkengpt: Augmenting frozen language models with massive tools via tool embeddings,‚Äù CoRR, vol. abs/2305.11554, 2023. [535] Y. Liang, C. Wu, T. Song, W. Wu, Y. Xia, Y. Liu, Y. Ou, S. Lu, L. Ji, S. Mao, Y. Wang, L. Shou, M. Gong, and N. Duan, ‚ÄúTaskmatrix.ai: Completing tasks by connecting foundation models with millions of apis,‚Äù CoRR, vol. abs/2303.16434, 2023. [536] T. Cai, X. Wang, T. Ma, X. Chen, and D. Zhou, ‚ÄúLarge language models as tool makers,‚Äù CoRR, vol. abs/2305.17126, 2023. [537] J. Huang, S. S. Gu, L. Hou, Y. Wu, X. Wang, H. Yu, and J. Han, ‚ÄúLarge language models can self-improve,‚Äù CoRR, vol. abs/2210.11610, 2022. [538] M. Suzgun, N. Scales, N. Scha¬®rli, S. Gehrmann, Y. Tay, H. W. Chung, A. Chowdhery, Q. V. Le, E. H. Chi, D. Zhou, and J. Wei, ‚ÄúChallenging big-bench tasks and whether chain-of-thought can solve them,‚Äù CoRR, vol. abs/2210.09261, 2022. [539] W. Zhong, R. Cui, Y. Guo, Y. Liang, S. Lu, Y. Wang, A. Saied, W. Chen, and N. Duan, ‚ÄúAgieval: A humancentric benchmark for evaluating foundation models,‚Äù CoRR, vol. abs/2304.06364, 2023. [540] H. Zeng, ‚ÄúMeasuring massive multitask chinese understanding,‚Äù CoRR, vol. abs/2304.12986, 2023. [541] C. Liu, R. Jin, Y. Ren, L. Yu, T. Dong, X. Peng, S. Zhang, J. Peng, P. Zhang, Q. Lyu, X. Su, Q. Liu, and D. Xiong, ‚ÄúM3KE: A massive multi-level multisubject knowledge evaluation benchmark for chinese large language models,‚Äù CoRR, vol. abs/2305.10263, 2023. [542] Y. Huang, Y. Bai, Z. Zhu, J. Zhang, J. Zhang, T. Su, J. Liu, C. Lv, Y. Zhang, J. Lei, Y. Fu, M. Sun, and J. He, ‚ÄúC-eval: A multi-level multi-discipline chinese evaluation suite for foundation models,‚Äù CoRR, vol.

83

abs/2305.08322, 2023. [543] Z. Gu, X. Zhu, H. Ye, L. Zhang, J. Wang, S. Jiang,
Z. Xiong, Z. Li, Q. He, R. Xu, W. Huang, W. Zheng, H. Feng, and Y. Xiao, ‚ÄúXiezhi: An ever-updating benchmark for holistic domain knowledge evaluation,‚Äù CoRR, vol. abs/2306.05783, 2023. [544] J. H. Clark, J. Palomaki, V. Nikolaev, E. Choi, D. Garrette, M. Collins, and T. Kwiatkowski, ‚ÄúTydi QA: A benchmark for information-seeking question answering in typologically diverse languages,‚Äù Trans. Assoc. Comput. Linguistics, vol. 8, pp. 454‚Äì470, 2020. [545] L. Gao, J. Tow, S. Biderman, S. Black, A. DiPofi, C. Foster, L. Golding, J. Hsu, K. McDonell, N. Muennighoff, J. Phang, L. Reynolds, E. Tang, A. Thite, B. Wang, K. Wang, and A. Zou, ‚ÄúA framework for few-shot language model evaluation,‚Äù Sep. 2021. [546] C. Zan, K. Peng, L. Ding, B. Qiu, B. Liu, S. He, Q. Lu, Z. Zhang, C. Liu, W. Liu, Y. Zhan, and D. Tao, ‚ÄúVegamt: The JD explore academy machine translation system for WMT22,‚Äù in Proceedings of the Seventh Conference on Machine Translation, WMT 2022, Abu Dhabi, United Arab Emirates (Hybrid), December 7-8, 2022, P. Koehn, L. Barrault, O. Bojar, F. Bougares, R. Chatterjee, M. R. Costa-jussa`, C. Federmann, M. Fishel, A. Fraser, M. Freitag, Y. Graham, R. Grundkiewicz, P. Guzman, B. Haddow, M. Huck, A. Jimeno-Yepes, T. Kocmi, A. Martins, M. Morishita, C. Monz, M. Nagata, T. Nakazawa, M. Negri, A. Ne¬¥ve¬¥ol, M. Neves, M. Popel, M. Turchi, and M. Zampieri, Eds. Association for Computational Linguistics, 2022, pp. 411‚Äì422. [547] Y. Zhao, M. Khalman, R. Joshi, S. Narayan, M. Saleh, and P. J. Liu, ‚ÄúCalibrating sequence likelihood improves conditional language generation,‚Äù CoRR, vol. abs/2210.00045, 2022. [Online]. Available: https: //doi.org/10.48550/arXiv.2210.00045 [548] D. Khashabi, S. Min, T. Khot, A. Sabharwal, O. Tafjord, P. Clark, and H. Hajishirzi, ‚ÄúUnifiedqa: Crossing format boundaries with a single QA system,‚Äù in EMNLP (Findings), ser. Findings of ACL, vol. EMNLP 2020. Association for Computational Linguistics, 2020, pp. 1896‚Äì1907. [549] X. Zhu, J. Wang, L. Zhang, Y. Zhang, R. Gan, J. Zhang, and Y. Yang, ‚ÄúSolving math word problem via cooperative reasoning induced language models,‚Äù arXiv preprint arXiv:2210.16257, 2022. [550] A. Nguyen, N. Karampatziakis, and W. Chen, ‚ÄúMeet in the middle: A new pre-training paradigm,‚Äù CoRR, vol. abs/2303.07295, 2023. [Online]. Available: https://doi.org/10.48550/arXiv.2303.07295 [551] H. Li, J. Zhang, C. Li, and H. Chen, ‚ÄúRESDSQL: decoupling schema linking and skeleton parsing for text-to-sql,‚Äù CoRR, vol. abs/2302.05965, 2023. [Online]. Available: https://doi.org/10.48550/arXiv. 2302.05965 [552] W. Kang and J. J. McAuley, ‚ÄúSelf-attentive sequential recommendation,‚Äù in IEEE International Conference on Data Mining, ICDM 2018, Singapore, November 17-20, 2018. IEEE Computer Society, 2018, pp. 197‚Äì206. [553] B. Yang, C. Han, Y. Li, L. Zuo, and Z. Yu, ‚ÄúImproving conversational recommendation systems‚Äô quality with context-aware item meta-information,‚Äù in Find-

ings of the Association for Computational Linguistics: NAACL 2022, Seattle, WA, United States, July 10-15, 2022, M. Carpuat, M. de Marneffe, and I. V. M. Ru¬¥ƒ±z, Eds. Association for Computational Linguistics, 2022, pp. 38‚Äì48. [554] E. Almazrouei, H. Alobeidli, A. Alshamsi, A. Cappelli, R. Cojocaru, M. Debbah, E. Goffinet, D. Heslow, J. Launay, Q. Malartic, B. Noune, B. Pannier, and G. Penedo, ‚ÄúFalcon-40B: an open large language model with state-of-the-art performance,‚Äù 2023. [555] S. K. K. Santu and D. Feng, ‚ÄúTeler: A general taxonomy of LLM prompts for benchmarking complex tasks,‚Äù CoRR, vol. abs/2305.11430, 2023. [Online]. Available: https://doi.org/10.48550/arXiv. 2305.11430 [556] J. White, Q. Fu, S. Hays, M. Sandborn, C. Olea, H. Gilbert, A. Elnashar, J. Spencer-Smith, and D. C. Schmidt, ‚ÄúA prompt pattern catalog to enhance prompt engineering with chatgpt,‚Äù arXiv preprint arXiv:2302.11382, 2023. [557] OpenAI, ‚ÄúGpt best practices,‚Äù OpenAI, 2023. [Online]. Available: https://platform.openai.com/ docs/guides/gpt-best-practices [558] Contributors, ‚ÄúAi short,‚Äù 2023. [Online]. Available: https://www.aishort.top/ [559] ‚Äî‚Äî, ‚ÄúAwesome chatgpt prompts,‚Äù Github, 2023. [Online]. Available: https://github.com/f/awesomechatgpt-prompts/ [560] V. Liu and L. B. Chilton, ‚ÄúDesign guidelines for prompt engineering text-to-image generative models,‚Äù in Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems, 2022, pp. 1‚Äì23. [561] L. Beurer-Kellner, M. Fischer, and M. Vechev, ‚ÄúPrompting is programming: A query language for large language models,‚Äù Proceedings of the ACM on Programming Languages, vol. 7, no. PLDI, pp. 1946‚Äì 1969, 2023. [562] P. Lu, B. Peng, H. Cheng, M. Galley, K.-W. Chang, Y. N. Wu, S.-C. Zhu, and J. Gao, ‚ÄúChameleon: Plug-andplay compositional reasoning with large language models,‚Äù arXiv preprint arXiv:2304.09842, 2023. [563] Y. Hou, J. Zhang, Z. Lin, H. Lu, R. Xie, J. J. McAuley, and W. X. Zhao, ‚ÄúLarge language models are zeroshot rankers for recommender systems,‚Äù CoRR, vol. abs/2305.08845, 2023. [564] S. Chang and E. Fosler-Lussier, ‚ÄúHow to prompt llms for text-to-sql: A study in zero-shot, singledomain, and cross-domain settings,‚Äù CoRR, vol. abs/2305.11853, 2023. [Online]. Available: https: //doi.org/10.48550/arXiv.2305.11853 [565] R. Tang, X. Han, X. Jiang, and X. Hu, ‚ÄúDoes synthetic data generation of llms help clinical text mining?‚Äù arXiv preprint arXiv:2303.04360, 2023. [566] O. Nov, N. Singh, and D. M. Mann, ‚ÄúPutting chatgpt‚Äôs medical advice to the (turing) test,‚Äù CoRR, vol. abs/2301.10035, 2023. [567] K. Yang, S. Ji, T. Zhang, Q. Xie, and S. Ananiadou, ‚ÄúOn the evaluations of chatgpt and emotion-enhanced prompting for mental health analysis,‚Äù CoRR, vol. abs/2304.03347, 2023. [568] K. Jeblick, B. Schachtner, J. Dexl, A. Mittermeier, A. T.

84

Stu¬® ber, J. Topalis, T. Weber, P. Wesp, B. O. Sabel, J. Ricke, and M. Ingrisch, ‚ÄúChatgpt makes medicine easy to swallow: An exploratory case study on simplified radiology reports,‚Äù CoRR, vol. abs/2212.14882, 2022. [569] K. Singhal, T. Tu, J. Gottweis, R. Sayres, E. Wulczyn, L. Hou, K. Clark, S. Pfohl, H. Cole-Lewis, D. Neal, M. Schaekermann, A. Wang, M. Amin, S. Lachgar, P. A. Mansfield, S. Prakash, B. Green, E. Dominowska, B. A. y Arcas, N. Tomasev, Y. Liu, R. Wong, C. Semturs, S. S. Mahdavi, J. K. Barral, D. R. Webster, G. S. Corrado, Y. Matias, S. Azizi, A. Karthikesalingam, and V. Natarajan, ‚ÄúTowards expert-level medical question answering with large language models,‚Äù CoRR, vol. abs/2305.09617, 2023. [570] S. Chen, B. H. Kann, M. B. Foote, H. J. Aerts, G. K. Savova, R. H. Mak, and D. S. Bitterman, ‚ÄúThe utility of chatgpt for cancer treatment information,‚Äù medRxiv, 2023. [571] K. Malinka, M. Peres¬¥ƒ±ni, A. Firc, O. Hujnak, and F. Janus, ‚ÄúOn the educational impact of chatgpt: Is artificial intelligence ready to obtain a university degree?‚Äù CoRR, vol. abs/2303.11146, 2023. [572] T. Susnjak, ‚ÄúChatgpt: The end of online exam integrity?‚Äù CoRR, vol. abs/2212.09292, 2022. [573] K. Tan, T. Pang, and C. Fan, ‚ÄúTowards applying powerful large ai models in classroom teaching: Opportunities, challenges and prospects,‚Äù 2023. [574] F. Kamalov and I. Gurrib, ‚ÄúA new era of artificial intelligence in education: A multifaceted revolution,‚Äù CoRR, vol. abs/2305.18303, 2023. [575] E. Kasneci, K. Se√üler, S. Ku¬® chemann, M. Bannert, D. Dementieva, F. Fischer, U. Gasser, G. Groh, S. Gu¬® nnemann, E. Hu¬® llermeier et al., ‚ÄúChatgpt for good? on opportunities and challenges of large language models for education,‚Äù Learning and Individual Differences, vol. 103, p. 102274, 2023. [576] A. Blair-Stanek, N. Holzenberger, and B. V. Durme, ‚ÄúCan GPT-3 perform statutory reasoning?‚Äù CoRR, vol. abs/2302.06100, 2023. [577] D. Trautmann, A. Petrova, and F. Schilder, ‚ÄúLegal prompt engineering for multilingual legal judgement prediction,‚Äù CoRR, vol. abs/2212.02199, 2022. [578] J. H. Choi, K. E. Hickman, A. Monahan, and D. Schwarcz, ‚ÄúChatgpt goes to law school,‚Äù Available at SSRN, 2023. [579] J. J. Nay, ‚ÄúLaw informs code: A legal informatics approach to aligning artificial intelligence with humans,‚Äù CoRR, vol. abs/2209.13020, 2022. [580] F. Yu, L. Quartey, and F. Schilder, ‚ÄúLegal prompting: Teaching a language model to think like a lawyer,‚Äù CoRR, vol. abs/2212.01326, 2022. [581] D. Trautmann, A. Petrova, and F. Schilder, ‚ÄúLegal prompt engineering for multilingual legal judgement prediction,‚Äù CoRR, vol. abs/2212.02199, 2022. [582] A. Tamkin, M. Brundage, J. Clark, and D. Ganguli, ‚ÄúUnderstanding the capabilities, limitations, and societal impact of large language models,‚Äù CoRR, vol. abs/2102.02503, 2021. [583] Z. Sun, ‚ÄúA short survey of viewing large language models in legal aspect,‚Äù CoRR, vol. abs/2303.09136,

2023. [584] A. Abid, M. Farooqi, and J. Zou, ‚ÄúPersistent anti-
muslim bias in large language models,‚Äù in AIES ‚Äô21: AAAI/ACM Conference on AI, Ethics, and Society, Virtual Event, USA, May 19-21, 2021, M. Fourcade, B. Kuipers, S. Lazar, and D. K. Mulligan, Eds. ACM, 2021, pp. 298‚Äì306. [585] A. Shah and S. Chava, ‚ÄúZero is not hero yet: Benchmarking zero-shot performance of llms for financial tasks,‚Äù CoRR, vol. abs/2305.16633, 2023. [586] D. Araci, ‚ÄúFinbert: Financial sentiment analysis with pre-trained language models,‚Äù CoRR, vol. abs/1908.10063, 2019. [587] J. C. S. Alvarado, K. Verspoor, and T. Baldwin, ‚ÄúDomain adaption of named entity recognition to support credit risk assessment,‚Äù in Proceedings of the Australasian Language Technology Association Workshop, ALTA 2015, Parramatta, Australia, December 8 - 9, 2015, B. Hachey and K. Webster, Eds. ACL, 2015, pp. 84‚Äì90. [588] G. Son, H. Jung, M. Hahm, K. Na, and S. Jin, ‚ÄúBeyond classification: Financial reasoning in state-of-the-art language models,‚Äù CoRR, vol. abs/2305.01505, 2023. [589] X. Zhang, Q. Yang, and D. Xu, ‚ÄúXuanyuan 2.0: A large chinese financial chat model with hundreds of billions parameters,‚Äù arXiv preprint arXiv:2305.12002, 2023. [590] H. Yang, X.-Y. Liu, and C. D. Wang, ‚ÄúFingpt: Opensource financial large language models,‚Äù CoRR, vol. abs/2306.06031, 2023. [591] Q. Jin, B. Dhingra, Z. Liu, W. W. Cohen, and X. Lu, ‚ÄúPubmedqa: A dataset for biomedical research question answering,‚Äù in Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019, Hong Kong, China, November 3-7, 2019, 2019, pp. 2567‚Äì2577. [592] A. Krithara, A. Nentidis, K. Bougiatiotis, and G. Paliouras, ‚ÄúBioasq-qa: A manually curated corpus for biomedical question answering,‚Äù 2022. [593] C. Zhang, C. Zhang, C. Li, Y. Qiao, S. Zheng, S. K. Dam, M. Zhang, J. U. Kim, S. T. Kim, J. Choi, G. Park, S. Bae, L. Lee, P. Hui, I. S. Kweon, and C. S. Hong, ‚ÄúOne small step for generative ai, one giant leap for AGI: A complete survey on chatgpt in AIGC era,‚Äù CoRR, vol. abs/2304.06488, 2023. [594] M. Haman and M. Skolnik, ‚ÄúUsing chatgpt to conduct a literature review.‚Äù Accountability in research, 2023. [595] O¬® . Aydƒ±n and E. Karaarslan, ‚ÄúOpenai chatgpt generated literature review: Digital twin in healthcare,‚Äù SSRN Electronic Journal, 2022. [596] Y. J. Park, D. Kaplan, Z. Ren, C. Hsu, C. Li, H. Xu, S. Li, and J. Li, ‚ÄúCan chatgpt be used to generate scientific hypotheses?‚Äù CoRR, vol. abs/2304.12208, 2023. [597] M. M. Hassan, R. A. Knipper, and S. K. K. Santu, ‚ÄúChatgpt as your personal data scientist,‚Äù CoRR, vol. abs/2305.13657, 2023. [598] L. Cheng, X. Li, and L. Bing, ‚ÄúIs GPT-4 a good data analyst?‚Äù CoRR, vol. abs/2305.15038, 2023. [599] S. I. M. Hussam Alkaissi, ‚ÄúArtificial hallucinations in chatgpt: Implications in scientific writing,‚Äù PubMed, 2023. [600] A. Azaria, R. Azoulay, and S. Reches, ‚ÄúChatgpt

85
is a remarkable tool ‚Äì for experts,‚Äù CoRR, vol. abs/2306.03102, 2023. [601] O. O. Buruk, ‚ÄúAcademic writing with GPT-3.5: reflections on practices, efficacy and transparency,‚Äù CoRR, vol. abs/2304.11079, 2023. [602] R. Liu and N. B. Shah, ‚ÄúReviewergpt? an exploratory study on using large language models for paper reviewing,‚Äù CoRR, vol. abs/2306.00622, 2023. [603] M. Kosinski, ‚ÄúTheory of mind may have spontaneously emerged in large language models,‚Äù CoRR, vol. abs/2302.02083, 2023. [604] M. M. Amin, E. Cambria, and B. W. Schuller, ‚ÄúWill affective computing emerge from foundation models and general ai? A first evaluation on chatgpt,‚Äù CoRR, vol. abs/2303.03186, 2023. [605] G. Sridhara, R. H. G., and S. Mazumdar, ‚ÄúChatgpt: A study on its utility for ubiquitous software engineering tasks,‚Äù CoRR, vol. abs/2305.16837, 2023. [606] W. Sun, C. Fang, Y. You, Y. Miao, Y. Liu, Y. Li, G. Deng, S. Huang, Y. Chen, Q. Zhang, H. Qian, Y. Liu, and Z. Chen, ‚ÄúAutomatic code summarization via chatgpt: How far are we?‚Äù CoRR, vol. abs/2305.12865, 2023. [607] C. S. Xia and L. Zhang, ‚ÄúConversational automated program repair,‚Äù CoRR, vol. abs/2301.13246, 2023. [608] H. Cho, H. J. Kim, J. Kim, S. Lee, S. Lee, K. M. Yoo, and T. Kim, ‚ÄúPrompt-augmented linear probing: Scaling beyond the limit of few-shot in-context learners,‚Äù CoRR, vol. abs/2212.10873, 2022. [609] S. Shin, S. Lee, H. Ahn, S. Kim, H. Kim, B. Kim, K. Cho, G. Lee, W. Park, J. Ha, and N. Sung, ‚ÄúOn the effect of pretraining corpora on in-context learning by a largescale language model,‚Äù in NAACL-HLT. Association for Computational Linguistics, 2022, pp. 5168‚Äì5186. [610] Y. Tay, M. Dehghani, D. Bahri, and D. Metzler, ‚ÄúEfficient transformers: A survey,‚Äù ACM Comput. Surv., vol. 55, no. 6, pp. 109:1‚Äì109:28, 2023.

