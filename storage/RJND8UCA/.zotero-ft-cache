arXiv:2306.08543v1 [cs.CL] 14 Jun 2023
Average GPT4 Score

Knowledge Distillation of Large Language Models

Yuxian Gu1,2∗, Li Dong2, Furu Wei2, Minlie Huang1 1The CoAI Group, Tsinghua University 2Microsoft Research
guyx21@mails.tsinghua.edu.cn {lidong1,fuwei}@microsoft.com
aihuang@tsinghua.edu.cn

Abstract
Knowledge Distillation (KD) is a promising technique for reducing the high computational demand of large language models (LLMs). However, previous KD methods are primarily applied to white-box classification models or training small models to imitate black-box model APIs like ChatGPT. How to effectively distill the knowledge from white-box generative LLMs is still under-explored, which becomes more and more important with the prosperity of LLMs. In this work, we propose MINILLM that distills smaller language models from generative larger language models. We first replace the forward Kullback-Leibler divergence (KLD) objective in the standard KD approaches with reverse KLD, which is more suitable for KD on generative language models, to prevent the student model from overestimating the low-probability regions of the teacher distribution. Then, we derive an effective optimization approach to learn this objective. Extensive experiments in the instruction-following setting show that the MINILLM models generate more precise responses with the higher overall quality, lower exposure bias, better calibration, and higher long-text generation performance. Our method is also scalable for different model families with 120M to 13B parameters. We will release our code and model checkpoints at https://aka.ms/MiniLLM.

50

Teacher: GPT-2-1.5B 60

45

55

40

MiniLLM 50

35

SeqKD 45

100M 200M 400M 700M # of student parameters

Teacher: GPT-J 6B 60

Teacher: OPT 13B

55

50 MiniLLM SeqKD 45

MiniLLM SeqKD

1B 1.5B 2B 2.5B # of student parameters

1.5B

3B

6B

# of student parameters

Figure 1: The comparison of MINILLM with the sequence-level KD (SeqKD) in terms of the average GPT-4 feedback score on our evaluation sets. Left: GPT-2-1.5B as the teacher and GPT-2 125M, 340M, 760M as the students. Middle: GPT-J 6B as the teacher and GPT-2 760M, 1.5B, GPT-Neo 2.7B as the students. Right: OPT 13B as the teacher and OPT 1.3B, 2.7B, 6.7B as the students.

∗Contribution during an internship at Microsoft Research.

1 Introduction
With the rapid development of large language models (LLMs; HZD+21, BHA+21, BMR+20, Ope23, CND+22), a common technique to reduce high computational resource demand is knowledge distillation (KD; HVD15), where we train a small student model with supervision from a large teacher model. Two categories of KD are commonly applied: black-box KD, where only the teacher predictions are accessible, and white-box KD, where the teacher parameters are available to use [JBMD21]. Recently, black-box KD has shown promising results in fine-tuning small models on the prompt-response pairs generated by LLM APIs [TGZ+23, CLL+23, WWZ+23, PLH+23]. With the emergence of more open-source LLMs [ZRG+22, TLI+23], white-box KD becomes more valuable for both research communities and industry sectors because student models receive better signals from white-box teacher models, thereby potentially resulting in higher performance. However, white-box KD approaches are mostly studied for small (< 1B parameters) language understanding models [SDCW19, WWD+20], while white-box KD for generative LLMs is yet to be explored.
In this work, we investigate white-box KD of LLMs. We argue that the standard KD objectives [KR16, SST+20] are sub-optimal for LLMs that perform tasks in a generative manner. Given the teacher distribution p(y|x) and the student distribution qθ(y|x) parameterized by θ, standard KD objectives (including several variants for sequence-level models) essentially minimize the approximated forward Kullback-Leibler divergence (KLD) between the teacher and the student distribution, termed as KL[p||qθ], which forces p to cover all the modes of qθ. For text classification tasks, KL[p||qθ] works well because the output space usually consists of finite-number classes such that both p(y|x) and qθ(y|x) have few modes. However, for open text generation tasks, where the output spaces are much more complex and p(y|x) can contain much more modes than what qθ(y|x) can express due to the limited model capacity. Minimizing forward KLD can cause qθ to assign unreasonably high probabilities to the void regions of p [MG19] and produces samples very unlikely under p during free-run generation [Hus15].
To alleviate this problem, we propose to minimize the reverse KLD, KL[qθ||p], which is widely used in computer vision [KW13] and reinforcement learning [CPO+19]. Compared to KL[p||qθ], minimizing KL[qθ||p] causes qθ to seek the major modes of p, and assign low probabilities to the p’s void regions [M+05], which is illustrated by a pilot experiment in Section 2.1. In language generation of LLMs, this means the student model avoids learning too many long-tail variants [HBD+20] of the teacher distribution and focuses on the correctness of the generated response, which is critical in practical scenarios that require truthfulness and reliability [JLF+23]. To optimize minθ KL[qθ||p] as shown in Section 2.2, we derive the gradient of the objective with Policy Gradient [SMSM99]. Although recent works have shown success in fine-tuning PLMs with policy optimization [OWJ+22, RAB+23], we found that training the model still suffers from high variance, reward hacking, and generation length bias. Therefore, we introduce (1) single-step regularization to reduce variance, (2) teacher-mixed sampling to alleviate reward hacking, and (3) length normalization to eliminate the length bias. Finally, we introduce the algorithm of MINILLM in Section 2.3.
We apply MINILLM to various generative language models [RWC+19, ZRG+22, TLI+23] with parameter sizes ranging from 120M to 13B in the instruction-following setting [SWR+22, WBZ+22] that covers a large range of NLP tasks. We use five instruction-following datasets with the GPT-4 feedback and Rouge-L [Lin04] for evaluation. Our experiments show that MINILM consistently outperforms the standard KD baselines on all the datasets and scales up well from 120M to 13B models (see Figure 1). More analysis shows that MINILLM has lower exposure bias, better calibration, and performs better at generating long responses with better diversity.

2 Method

We consider conditional language generation tasks where the model produces a response y = {yt}Tt=1 conditioning on a prompt x sampled from the distribution px. We formulate KD as an opti-

mization problem to minimize the difference between a fixed teacher model distribution p(y|x)

and a student model distribution qθ(y|x) parameterized by θ. The standard KD for genera-

tive

models

approximately2

minimizes

the

forward

KLD:

KL[p||qθ ]

=

Ex∼px ,y ∼p′

log

p(y|x) qθ (y|x)

,

2We say “approximately” because for word-level KD, y is sampled from the real distribution, not the teacher distribution. For a strong enough teacher model, we can consider the two distributions approximately the same.

2

𝒚∼𝑝 Teacher

Prompt 𝒙

𝑞! Student

Forward KLD ℒ 𝜃 = KL[𝑝| 𝑞!

∇ℒ(𝜃)

a

Sequence-Level KD

𝑝 Teacher

Prompt 𝒙

Student 𝒚 ∼ 𝑞!

Reverse KLD 𝒥 𝜃 = KL[𝑞!| 𝑝

∇𝒥(𝜃) (Section 2.2)
MiniLLM (Ours)

Figure 3: Comparison between sequence-level KD (left) and our MINILLM (right). Sequence-level KD forces the student to memorize all teacher-generated samples, while MINILLM allows the student model to improve its own generation with the teacher’s feedback.

where p′ can be real data distribution (word-level KD) or teacher distribution p (sequence-level KD). Though widely used, KL[p||qθ] has been shown to overestimate the void regions of p in language generation tasks when qθ is insufficiently expressive to cover all the modes of p′ [JKH+23]. KD for LLMs fits the case because LLMs perform various tasks in a generative manner, such that the low-capacity student models cannot perfectly imitate the complex language generation distribution of the teacher models or humans.

0.4

Target Distribution Forward KLD

Reverse KLD (ours)

0.2

0.0 0.0 2.5 5.0 7.5 10.0 12.5

Figure 2: The toy experiment. We fit a Gaussian mixture with a Gaussian distribution using forward KLD and reverse KLD.

2.1 MiniLLM: Knowledge Distillation with Reverse KLD

In this work, we consider minimizing the reverse KLD between the student and teacher distributions as the learning objective of MINILLM:

θ = arg min J (θ) = arg min KL[qθ||p]

θ

θ

=

arg

min
θ

E
x∼px ,y ∼qθ

log

qθ(y|x) . p(y|x)

(1)

[Hus15] has shown that minimizing KL[qθ||p] leads to a mode-seeking behavior, where qθ assigns high probabilities to p’s large modes, ignoring the small ones. Figure 2 illustrates the difference
between the two KLDs when a Gaussian distribution tries to fit a Gaussian mixture. We can see that minimizing forward KLD causes qθ to place large probability mass on the zero-probability places of p, which corresponds to the generation of low-quality texts in practice, while reverse KLD focuses on p’s major modes, which is crucial to ensure the correctness and faithfulness of language generation.

We term our KD method for LLMs by minimizing the reverse KLD as MINILLM, which is illustrated in Figure 3. Unlike sequence-level KD, MINILLM does not force qθ to fit all y sampled from the teacher distribution p. Instead, it encourages the student to generate samples preferred by the teacher
within its own capacities, which is more possible to achieve.

2.2 Optimization with Policy Gradient

Gradient Derivation We notice that the gradient of the objective function J (θ) described in Equation (1) can be derived using the Policy Gradient Theorem [Wil92, HTAL17]:

T

∇J (θ) = − E

(Rt − 1)∇ log qθ(yt|y<t, x),

(2)

y∼qθ (·|x) t=1

where T = |y| and Rt =

log T
t′ =t

p(yt′ |y<t′ ,x) qθ (yt′ |y<t′ ,x)

is the accumulation of rt′

=

log p(yt′ |y<t′ ,x)
qθ (yt′ |y<t′ ,x)

that measures the quality of each step generation. Intuitively, we want the generation to have

high probabilities under the teacher distribution by increasing p(yt′ |y<t′ , x), but simultaneously stay diverse by lowering qθ(yt′ |y<t′ , x). The expectation is computed by Monte-Carlo sampling.
However, policy gradient suffers from high variance and reward hacking [SHKK22]. Although subsequent works proposed better solutions like PPO [SWD+17], we find that these issues still

remain. Besides, we notice that Rt favors short sentences, which causes our model to output empty responses. Therefore, we propose three strategies to mitigate these problems.

3

Single-Step Regularization [CPO+19] has found that the single-step generation quality rt is critical to the training variance because the error in the front tokens accumulates along the whole sentence. To pay more attention to rt, we re-write ∇J (θ) to split rt from Rt and directly compute the gradient of Eyt∼qθ(t)[rt] as a regularization (see Appendix A.2 for the full derivation):

T

T

∇J (θ) = E − Rt+1∇ log qθ(yt|y<t, x) + E − ∇ E [rt]

y∼qθ (·|x)

t=1

y∼qθ (·|x)

t=1 yt∼qθ (t)

(3)

= (∇J )Main + (∇J )Reg,

where qθ(t) = qθ(·|y<t, x). Note that Eyt∼qθ(t)[rt] can be computed directly by summing over the vocabulary instead of using Monte-Carlo sampling and is derivable with respect to θ. This
regularization gives a more precise and efficient estimation of the single-step generation quality,
which reduces the variance during training and accelerates convergence.

Teacher-Mixed Sampling We observe reward hacking [SHKK22] when training with Equation 2 because qθ sometimes produces degenerated sentences y that receive high scores from the teacher (e.g., repeated phrases) during sampling, especially for small student models. To create a better sampling distribution, we mix the teacher and the student distribution at each time step:

p(yt|y<t, x) = α · p(yt|y<t, x) + (1 − α) · qθ(yt|y<t, x),

(4)

where α controls the strength of the teacher mix-in. Sampling from p suppresses low-quality generation with the teacher’s help and alleviates reward hacking. We re-write (∇J )Main and (∇J )Reg with importance sampling to get to an unbiased estimator of the gradient [PSS00]:

T

(∇J )Main = − E

wtRt+1∇ log qθ(yt|y<t, x),

y∼p(·|x) t=1

(5)

T

(∇J )Reg = − E

wt∇ E [rt] ,

y∼p(·|x) t=1

yt∼qθ (t)

where wt =

t t′ =1

qθ (yt′ |y<t′ ,x) p(yt′ |y<t′ ,x)

is

the

importance

weight.

However,

in

practice,

using

wt

has

been

found to be sensitive to hyper-parameters and converge slowly. Therefore, we approximately set

wt

≈

qθ (yt|y<t,x) p(yt |y<t ,x)

to

reduce

the

variance

of

the

estimator

in

Equation

5

[LKTF20].

Length Normalization We found that long sequences tend to have small Rt+1, which encourages the model to produce short responses. Therefore, we add length normalization to Rt+1 in Equation 3:

RtN+o1rm

=

T

1 −t−1

T
log
t′ =t+1

p(yt′ |y<t′ , x) . qθ(yt′ |y<t′ , x)

(6)

In Summary Combining the strategies listed above, we have the final optimization gradient:







∇J

(θ)

=

T
−E y∼p(·|x) t=1

wt

RtN+o1rm

∇qθ(yt|y<t, x) qθ(yt|y<t, x)

+

∇
y′ ∈V

qθ(y′|y<t, x) log

p(y′|y<t, x) qθ(y′|y<t, x)



,

(7)

where V is the vocab size of the language model.

2.3 Training Algorithm
The training algorithm of MINILLM is shown in Algorithm 2.3. We initialize the student model from a checkpoint fine-tuned on the training data with the lowest validation loss and add the PPO clipping strategy [SWD+17] to (∇J )Main to improve training stability. Note that we do not use the value network and the KL regularization in PPO to improve the training efficiency. Same as [OWJ+22], we add a language modeling loss LPT on the pre-training corpus.

4

Algorithm 1 MINILLM: Knowledge Distillation of LLMs

Input: Conditional generation dataset D consisting of prompts and ground truth responses

Pre-training corpus DPT consisting of long-document plain texts

A teacher model with output distribution p

An initial student model with the output distribution qθ0 Output: A student model with the output distribution qθ

Fine-tune the student model from θ0 on D and select the model θ with the lowest validation loss.

repeat

Sample batch of prompts from D and collect responses from p to get S = {(xm, ym)}M m=1

Sample mini-batch D′PT = {dm}M m=1 from DPT

Compute

(∇J

)Main

=

−

1 |M

|

x,y∈S

T t=1

RtN+or1m ∇

min[ρt(θ),

clip(ρt(θ),

1

−

ϵ,

1

+

ϵ)],

where ρt(θ)

=

qθ (yt|y<t,x) p(yt |y<t ,x)

Compute

(∇J )Reg

=

−

1 M

x,y∈S

T t=1

wt∇

yt ∈V

qθ(yt|y<t, x) log

p(yt |y<t ,x) qθ (yt|y<t,x)

Compute

the

gradient

of

the

pre-training

loss

∇LPT

=

−

1 M

d∈DP′T ∇ log qθ(d)

Update model parameters: θ ← θ − (∇J )Main − (∇J )Reg − ∇LPT

▷ Eq. 5, Eq. 6 ▷ Eq. 3

until converge and return qθ

3 Experiments
3.1 Experimental Setup
We conduct experiments by first fine-tuning a large model on the instruction-response dataset D as the teacher p. Then, we compare different KD methods to distill a smaller student model on D with the teacher’s guidance by evaluating the instruction-following performance of the distilled model.
Base Models We distill three kinds of models with various sizes: GPT-2 [RWC+19] (120M, 340M, 760M), OPT [ZRG+22] (1.3B, 2.7B, 6.7B), and LLaMA [TLI+23] (7B), using GPT-2-1.5B, OPT13B, and LLaMA-13B as the teacher for each model type respectively. We also present the results using GPT-J [WK21] as the teacher in Appendix C.1.
Training We construct the training data from databricks-dolly-15k3 consisting of 15K humanwritten instruction-response pairs. We randomly split 14K samples as the training set D and left 500 samples for validation and testing, respectively. For DPT, we use the OpenWebText [GCPT19] for the GPT-2 family and the RoBERTa training corpus [LOG+19] for other models. We set the teacher-mix-in strength α = 0.2 throughout the experiments. We use the Rouge-L [Lin04] score on the validation set to select the hyper-parameters because it aligns with human preference better than the validation loss [WMA+22]. More training details are shown in Appendix B.1.
Evaluation We evaluate the trained models on five instruction-following datasets: • DollyEval: the 500-sample test set we split from the databricks-dolly-15k dataset. • SelfInst [WKM+22]: A user-oriented instruction-following set with 252 samples. • VicunaEval [CLL+23]: The 80 challenging questions used in the Vicuna evaluation. • S-NI: The test set of SUPER-NATURALINSTRUCTIONS [WMA+22] consisting of 9K samples
ranging from 119 tasks. Following [PLH+23], we split the set into 3 subsets whose ground truth response lengths lie in [0, 5], [6, 10], and [11, +∞]. We use the [11, +∞] subset in Section 3.2 and conduct an analysis on all subsets in Section 3.3. • UnNI: The core set of UNNATURALINSTRUCTIONS [HSLS22] containing 60K samples. Similar to S-NI, we first conduct the evaluations on the [11, +∞] subset, followed by an analysis of the performance on all subsets in Appendix C.2. We adopt two metrics to evaluate the model-generated responses: • R-L: The Rouge-L [Lin04] score to measure the precision of the model generation. [WMA+22] has shown that Rouge-L is suitable for large-scale instruction-following evaluation. • GPT4: The GPT-4 [Ope23] feedback by asking GPT-4 to compare model-generated responses with the ground truth answers4 and raise 1-10 scores for both responses (see Appendix B.2 for
3https://github.com/databrickslabs/dolly/tree/master 4We use the ChatGPT’s generation [Ope22] for VicunaEval’s ground truth responses
5

Model GPT2
OPT LLaMA

#Params 1.5B 120M
340M
760M 13B 1.3B
2.7B
6.7B 13B 7B

Method
Teacher
SFT w/o KD KD SeqKD MINILLM
SFT w/o KD KD SeqKD MINILLM
SFT w/o KD KD SeqKD MINILLM
Teacher
SFT w/o KD KD SeqKD MINILLM
SFT w/o KD KD SeqKD MINILLM
SFT w/o KD KD SeqKD MINILLM
Teacher
SFT w/o KD KD SeqKD MINILLM

DollyEval GPT4 R-L
58.4 27.6
38.6 23.3 40.3 22.8 41.2 22.7 44.7 24.6
51.9 25.5 51.6 25.0 50.5 25.3 52.2 25.4
50.7 25.4 53.4 25.9 52.0 25.6 54.7 26.4
70.3 29.2
52.6 26.0 52.7 25.4 51.0 26.1 60.7 26.7
55.4 27.1 60.5 25.9 57.6 27.5 63.2 27.4
67.9 27.6 68.6 28.3 69.6 28.5 70.8* 29.0
79.0 29.7
73.0 26.3 73.7 27.4 73.6 27.5 76.4 29.0

SelfInst GPT4 R-L
42.9 14.3
26.3 10.0 27.8 10.8 26.2 10.1 29.2 13.2
39.6 13.0 39.2 12.0 39.0 12.6 40.5 15.6
38.3 12.4 40.4 13.4 38.9 14.0 44.6* 15.9
56.1 18.4
37.7 11.4 36.0 12.2 36.6 12.7 47.0 14.8
38.9 13.9 48.6 13.8 40.5 13.3 52.7 17.2
56.4 16.4 58.0 17.0 54.0 17.0 58.5* 17.5
75.5 23.4
69.2 20.8 70.5 20.2 71.5 20.8 73.1 23.2

VicunaEval GPT4 R-L

48.6 16.3

32.8 14.7 31.9 13.4 31.0 14.3 34.1 16.9*

42.3 16.0 42.8 15.4 43.0 16.9* 42.6 17.7*

43.1 16.1 43.4 16.9* 42.4 15.9 45.7 18.3*

58.0 17.8

40.5 15.6 40.8 14.9 42.6 16.6 50.6 17.9*

44.8 16.6 51.3 16.7 44.5 16.5 55.9 19.1*

57.3 57.0 57.6 60.1*

17.8 17.5 17.9* 18.7*

65.1 19.4

61.6 17.5 62.7 18.4 62.6 18.1 64.1 20.7*

S-NI R-L
27.6
16.3 19.7 16.4 25.3
25.1 23.7 22.9 27.4
21.5 25.3 26.1 29.3*
30.4
23.1 21.9 21.4 28.6
24.9 26.3 25.3 30.7*
30.3 30.7* 30.4 32.5*
35.8
32.4 33.7 33.7 35.5

UnNI R-L
34.9
21.4 24.8 21.0 30.1
32.0 31.0 30.2 34.5
27.1 31.7 32.9 37.7*
36.1
28.4 27.0 28.2 33.4
32.3 30.2 32.3 35.1
28.6 26.7 28.2 36.7*
38.5
35.8 37.9 37.6 40.2*

Table 1: Evaluation results. GPT4 and R-L stand for the average GPT-4 feedback scores and Rouge-L scores across 5 random seeds. The best scores of each model size are boldfaced, and the scores where the student model outperforms the teacher are marked with *.

the prompt we use). We report the ratio of the total score of model responses and ground truth answers. This metric is only applied to DollyEval, SelfInst, and VicunaEval. For all test sets, we sample the responses with the temperature = 1 and report the average scores of 5 generations for each prompt with different random seeds.
Baselines We consider three baselines in our main experiment: • SFT w/o KD directly fine-tunes the student model on D supervised with the golden responses. • KD [SDCW19] fine-tunes the student model on D using the teacher distribution as the supervision
at each token step, also known as word-level KD. • SeqKD [KR16] fine-tunes the student model on the teacher-generated data.

3.2 Results
We present the evaluation results in Table 1, from which we have four observations. First, by comparing SFT with KD and SeqKD that approximately minimize the forward KLD, we can see that these standard KD methods successfully distill knowledge from the teacher model in

6

ExAccErr (%)

40 20 0
0

SFT KD SeqKD MiniLLM
50 100 150 200 250 Generation Length

Teacher
KD SeqKD MINILLM

SST2 ECE Acc.
0.025 93.0
0.191 84.7 0.243 66.5 0.099 89.7

BoolQ ECE Acc.
0.356 74.5
0.682 63.5 0.681 62.8 0.502 67.8

Figure 4: The excess error caused by the training- Table 2: The ECE and accuracy scores on decoding discrepancy (ExAccErr) accumulated SST2 and BoolQ datasets. The best scores with the generation length. Lower ExAccErr among student models are boldfaced. means the method introduces less exposure bias.

most cases, achieving better Rouge-L and GPT-4 feedback scores, which restates the conclusions in previous works [KR16, SDCW19, SST+20].
Second, by comparing the GPT-4 feedback score of MINILLM with the baselines, we observe that the model distilled by our method outperforms the baselines in almost all cases when trained with different base models and tested on various evaluation sets. This indicates that MINILLM is a general method to distill small models with high overall performance. We also find that MINILLM generally works better on datasets other than DollyEval compared with the baselines, indicating the good out-of-distribution generalization of our method.
Third, the Rouge-L scores show that the MINILLM models produce the most precise responses that have high overlaps with the ground truth. We notice that in some cases, especially on VicunaEval, S-NI, and UnNI, student models reach higher Rouge-L scores than the teacher, which matches the observation in [FLT+18]. We conjecture the reason is that the standard teacher-forcing fine-tuning brings the teacher training-inference discrepancy, also known as exposure bias [BVJS15]. On the contrary, MINILLM is optimized with policy optimization methods, which alleviates exposure bias [PH21]. We include further analysis on exposure bias in Section 3.3.
Fourth, comparing the results across model sizes and model families, we can see that the improvement of MINILLM is consistent when the base model sizes vary from 120M to 13B across three model families. This tendency is also illustrated in Figure 1, which demonstrates the excellent scalability and generalization of our method in the era of LLMs.
3.3 Analysis
Exposure Bias Language generation models trained to minimize forward KLD are known to suffer from exposure bias [BVJS15] caused by the discrepancy between teacher-forcing training and free-run generation. In MINILLM, we collect the samples from the student model during the training stage, which alleviates the mismatch between the training and evaluation [PH21]. In Figure 4, we use the ExAccErr metric [AEABC22] defined in Appendix B.3 to measure the excess accumulated error due to exposure bias in auto-regressive decoding. The experiment is based on GPT-2-125M, with GPT-2-1.5B as the teacher, using DollyEval as the test set. For each prompt, we sample 10 responses to reduce the variance. We can see that the ExAccErr of the fine-tuned model continuously grows during generation, while MiniLLM has a much lower ExAccErr, and the error stops accumulating for long text generation (> 150 tokens).
Calibration [Ope23] has shown that the RL-trained model is likely to be poorly calibrated. We test the calibration of MINILLM and the KD baselines on two widely-used text classification datasets: SST2 [SPW+13] and BoolQ [CLC+19], based on the LLaMA-7B model. We design zero-shot classification instructions (see Appendix B.2) and take the probability of the label words to compute the ECE scores [NDZ+19]. From Figure 2, we can see that the models trained with KD and SeqKD are worse calibrated than the teacher model, which potentially explains their low performance on canonical benchmarks [GWS+23]. We suspect the reason is that minimizing forward KLD causes the models to push high probabilities to zero-probability points of the target distribution, which leads to significant distribution difference between the student and the teacher (see the intuitive example
7

R-L Against SFT

3

KD SeqKD

2

MiniLLM

1

0 [0, 5]

[6, 10] [11, + ]

Ground Truth Length Range

Figure 5: The Rouge-L scores of the distilled models against SFT on the different subsets of S-NI split by the golden responses’ length.

Teacher SFT MINILLM

DollyEval Dist-4 Loss
99.3 3.55 99.5 3.89 99.0 3.95

SelfInst Dist-4 Loss
99.1 4.44 99.0 5.28 98.6 5.33

Table 3: The distinct 4-grams (Dist-4) and language modeling loss (Loss) on the test sets based on the LLaMA family. MINILLM preserves generation diversity.

in Figure 2). In contrast, MINILLM focuses on accurately learning the major parts of the target distribution, which narrows the ECE scores gap between the student and the teacher.

Performance on Different Response Length We study the models’ performance when the golden response lengths belong to different ranges. In Figure 5, we illustrate the Rouge-L scores of different KD models against the SFT models on three S-NI subsets split by the length of the ground truth responses. We can see that all methods achieve low scores on prompts that expect short responses (≤ 5 tokens), probably because most responses in our training set are long sentences, which introduces a distribution shift between training and testing [PLH+23]. Furthermore, the output spaces of these prompts are relatively small, allowing the student model to cover most modes of the teacher, and thus reverse KLD and forward KLD have similar performance. For prompts with longer responses (≥ 6 tokens), the teacher distribution contains more modes than the students due to the complex output spaces, which shows the advantage of MINILLM against standard KD approaches. Similar results on UnNI are shown in Appendix C.2.
Generation Diversity [CCF+20] has found that the model optimized by minimizing reverse KLD is likely to lose modes, which affects the generation diversity. We follow [PH21] to discuss generation diversity from three aspects: (i) generating multiple distinct responses given a prompt. (ii) generating linguistically complex responses. (iii) the ability to generate contents that have high coverage of the real data distribution. For (i), we argue that for many NLP applications, generating one correct response is sufficient, especially for those scenarios demanding high truthfulness and reliability [JLF+23]. For (ii) and (iii), we report the responses’ distinct 4-gram proportion and the language modeling loss on the test sets in Table 3, using the base models from the LLaMA family. We can see that MINILLM preserves the distinct 4-gram proportion in the generated responses and does not cause the language modeling loss on the test set to increase much.

3.4 Ablations
Effect of Optimization Strategies We conduct ablation studies on the three strategies proposed to stabilize and accelerate optimization in Section 2.2 by distilling a GPT-2-125M model from the GPT-2-1.5B model. In Table 4, we report the best Rouge-L scores on the validation set of each run and the evaluation results of the corresponding checkpoints. We also plot the reverse KLD between the student and the teacher during training in Figure 6, where the lines are smoothed by 32 steps. We can see that Teacher-Mixed Sampling and Length Normalization are critical to stabilizing training. Although the reverse KLDs also decrease without these strategies, we find that the models quickly learn to generate repeated, short, or meaningless strings that have high probabilities in the teacher distribution (see examples in Appendix D), which is known as reward hacking [SHKK22]. This also leads to the low generation performance in Table 4. From Figure 6, we also observe that the Single-Step Regularization effectively reduces the variance of the training process, which also results in higher performance on the validation and test sets.

Effect of Teacher-Mix-in Strength α In Figure 7, we plot the best Rouge-L scores on the validation set of GPT-2-125M, OPT-1.3B, and LLaMA-7B using GPT-2-1.5B, OPT-13B, and LLAMA-13B as the teachers, with different teacher-mix-in strength α in MINILLM. α = 0.0 means we only sample from the student distribution, and when α = 1.0, we sample entirely from the teacher distribution.

8

Validation Rouge-L
Forward KLD

MINILLM w/o Length Norm. w/o Teacher-Mixed w/o Single-Step Reg.

Valid. R-L
27.4 17.4 22.3 27.0

DollyEval R-L GPT-4
24.6 44.7 14.7 22.4 20.4 36.1 23.7 41.7

MiniLLM w/o Length Norm.

6

MiniLLM w/o Teacher-Mixed MiniLLM w/o Single-step Reg.

MiniLLM

4

2 0 1000 2000 3000 4000 5000 Training Steps

Table 4: The performance on the validation Figure 6: The forward KLD between the teacher and test set when different combinations of and the students during MINILLM training when MINILLM optimization strategies are applied. different optimization strategies are applied.

35

30

25

GPT-2-125M OPT-1.3B

LLaMA-7B

CLS Inst.

1.3B

MINILLM w/o PT Loss

70.2 65.7

52.8 53.2

7B

MINILLM

78.8 71.2

w/o PT Loss 74.3 71.1

0.0 0.2 0V.4alue of0.6 0.8 1.0

Table 5: The effect of adding the pre-training

Figure 7: The effect of the α value in the teacher loss. “CLS” is the average accuracy scores on

mix-in exploration on the validation Rouge-L SST2 and BoolQ. “Inst.” is the average Rouge-L

score. Larger models to more robust to α.

score on DollyEval, SelfInst, and VicunaEval.

We find that α = 0.2 is generally suitable across different model families and sizes, and larger models are more robust to the choice of α.
Effect of Adding Pre-Training Loss In Table 5, we study the effect of adding the pre-training loss in Algorithm 2.3 by comparing MINILLM with its variant where the language modeling loss on the pre-training corpus is removed (w/o PT Loss). We have a similar observation as [OWJ+22] that adding the pre-training loss helps to preserve the abilities on canonical NLP tasks while keeping the performance on instruction-following tasks nearly unchanged.
4 Related Work
Large Language Models Large language models (LLMs; BMR+20, Ope23, CND+22, ADF+23, TDFH+22) have shown their superior performance by solving various NLP tasks in a generative manner. Recent works apply instruction tuning [WBZ+22, SWR+22, CHL+22] or learning from human feedback [OWJ+22, BJN+22] to improve the alignment of LLMs with humans further and create general AI assistants [Ope22, Goo23]. There are also efforts to build open-source LLMs [TLI+23, ZRG+22, SFA+22, BSA+23, Mos23] to facilitate research and industry development. Although appealing, the broad capacities of LLMs usually only emerge with large parameter sizes [KMH+20, WTB+22] that require massive computational resources [SGM19]. Therefore, model compression is critical for the practical deployment and further research of LLMs.
Knowledge Distillation Knowledge distillation (KD; HVD15) aims at training a student model with the guidance of a teacher model and is widely used as a model compression technique in many fields of deep learning [JBMD21, SDCW19, RCG+15]. In the NLP community, many works apply KD to text classification tasks by training the student to mimic the teacher’s output distribution difference [SST+20, ZSL+23], hidden states [JYS+20, SCGL19], or attention scores [WWD+20, WBH+21]. For text generation, the standard KD method is to approximately minimize the forward KLD between the student and the teacher generation distribution by using the teacher’s output at each time step as supervision [SDCW19] or direct training on the teacher generations [KR16, TGZ+23, CLL+23, PLH+23]. In this paper, we propose to minimize the reverse KLD, which is more suitable for generative large language models when the teacher distribution is available.
9

Distribution Discrepancy Metrics in Text Generation The distribution discrepancy metrics play a significant role in learning text generation models. The forward Kullback-Leibler divergence (KLD) is the standard metric due to its simplicity when derived as the Maximum Likelihood Estimate (MLE) objective [ZZ19]. However, previous works show that minimizing forward KLD leads to zero-forcing behavior where models try to cover all modes of the target distribution and sacrifice the accuracy of major modes [Hus15]. Some works resort to using other metrics to remedy this problem, such as reverse KLD [JHC+20], Total Variation Distance [JKH+23], and Optimal Transport [LLW+20]. Our paper is the first to tackle this problem for knowledge distillation of LLMs.
5 Conclusion
In this work, we investigate the problem of distilling knowledge from larger LLMs to smaller ones. We find that the standard distillation methods that minimize the forward KLD is sub-optimal in language generation scenarios because the teacher’s output distribution contains much more modes than the student’s, and forward KLD forces the student distribution to over-estimate the low-probability regions of the teacher distribution. Therefore, we propose MINILLM that minimizes the reverse KLD between the teacher and student distribution and develop an algorithm to optimize this objective. Extensive experiments in the instruction-following setting show that MINILLM models produce more precise responses that have higher overall quality than standard KD approaches. We also find that MINILLM has lower exposure bias, better calibration, and higher performance in long-text generation with good diversity.
References
[ADF+23] Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. Palm 2 technical report. arXiv preprint arXiv:2305.10403, 2023.
[AEABC22] Kushal Arora, Layla El Asri, Hareesh Bahuleyan, and Jackie Cheung. Why exposure bias matters: An imitation learning perspective of error accumulation in language generation. In Findings of the Association for Computational Linguistics: ACL 2022, pages 700–710, Dublin, Ireland, May 2022. Association for Computational Linguistics.
[BHA+21] Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258, 2021.
[BJN+22] Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. Training a helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862, 2022.
[BLW+21] Sid Black, Gao Leo, Phil Wang, Connor Leahy, and Stella Biderman. GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow, March 2021.
[BMR+20] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, et al. Language models are few-shot learners. In Proceedings of NeurIPS, 2020.
[BSA+23] Stella Biderman, Hailey Schoelkopf, Quentin Anthony, Herbie Bradley, Kyle O’Brien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, et al. Pythia: A suite for analyzing large language models across training and scaling. arXiv preprint arXiv:2304.01373, 2023.
[BVJS15] Samy Bengio, Oriol Vinyals, Navdeep Jaitly, and Noam Shazeer. Scheduled sampling for sequence prediction with recurrent neural networks. Advances in neural information processing systems, 28, 2015.
[CCF+20] Massimo Caccia, Lucas Caccia, William Fedus, Hugo Larochelle, Joelle Pineau, and Laurent Charlin. Language gans falling short. In International Conference on Learning Representations, 2020.
10

[CHL+22] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instructionfinetuned language models. arXiv preprint arXiv:2210.11416, 2022.
[CLC+19] Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. BoolQ: Exploring the surprising difficulty of natural yes/no questions. In Proceedings of NAACL-HLT, 2019.
[CLL+23] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, March 2023.
[CND+22] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022.
[CPO+19] Wojciech M Czarnecki, Razvan Pascanu, Simon Osindero, Siddhant Jayakumar, Grzegorz Swirszcz, and Max Jaderberg. Distilling policy distillation. In The 22nd international conference on artificial intelligence and statistics, pages 1331–1340. PMLR, 2019.
[FLT+18] Tommaso Furlanello, Zachary Lipton, Michael Tschannen, Laurent Itti, and Anima Anandkumar. Born again neural networks. In International Conference on Machine Learning, pages 1607–1616. PMLR, 2018.
[GCPT19] Aaron Gokaslan, Vanya Cohen, Ellie Pavlick, and Stefanie Tellex. Openwebtext corpus, 2019.
[Goo23] Google. Bard, 2023.
[GWS+23] Arnav Gudibande, Eric Wallace, Charlie Snell, Xinyang Geng, Hao Liu, Pieter Abbeel, Sergey Levine, and Dawn Song. The false promise of imitating proprietary llms. arXiv preprint arXiv:2305.15717, 2023.
[HBD+20] Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. The curious case of neural text degeneration. In International Conference on Learning Representations, 2020.
[HSLS22] Or Honovich, Thomas Scialom, Omer Levy, and Timo Schick. Unnatural instructions: Tuning language models with (almost) no human labor. arXiv preprint arXiv:2212.09689, 2022.
[HTAL17] Tuomas Haarnoja, Haoran Tang, Pieter Abbeel, and Sergey Levine. Reinforcement learning with deep energy-based policies. In International conference on machine learning, pages 1352–1361. PMLR, 2017.
[Hus15] Ferenc Huszár. How (not) to train your generative model: Scheduled sampling, likelihood, adversary? arXiv preprint arXiv:1511.05101, 2015.
[HVD15] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531, 2015.
[HZD+21] Xu Han, Zhengyan Zhang, Ning Ding, Yuxian Gu, et al. Pre-trained models: Past, present and future. AI Open, 2021.
[JBMD21] Gou Jianping, Yu Baosheng, Stephen J Maybank, and Tao Dacheng. Knowledge distillation: A survey. International Journal of Computer Vision, 129(6):1789–1819, 2021.
[JHC+20] Haoming Jiang, Pengcheng He, Weizhu Chen, Xiaodong Liu, Jianfeng Gao, and Tuo Zhao. Smart: Robust and efficient fine-tuning for pre-trained natural language models through principled regularized optimization. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 2177–2190, 2020.
11

[JKH+23] Haozhe Ji, Pei Ke, Zhipeng Hu, Rongsheng Zhang, and Minlie Huang. Tailoring language generation models under total variation distance. In The Eleventh International Conference on Learning Representations, 2023.
[JLF+23] Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea Madotto, and Pascale Fung. Survey of hallucination in natural language generation. ACM Computing Surveys, 55(12):1–38, 2023.
[JYS+20] Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, Fang Wang, and Qun Liu. Tinybert: Distilling bert for natural language understanding. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 4163–4174, 2020.
[KMH+20] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020.
[KR16] Yoon Kim and Alexander M Rush. Sequence-level knowledge distillation. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1317–1327, 2016.
[KW13] Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013.
[Lin04] Chin-Yew Lin. ROUGE: A package for automatic evaluation of summaries. In Proceedings of Text Summarization Branches Out (ACL 2004), 2004.
[LKTF20] Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu. Offline reinforcement learning: Tutorial, review, and perspectives on open problems. arXiv preprint arXiv:2005.01643, 2020.
[LLW+20] Jianqiao Li, Chunyuan Li, Guoyin Wang, Hao Fu, Yuhchen Lin, Liqun Chen, Yizhe Zhang, Chenyang Tao, Ruiyi Zhang, Wenlin Wang, et al. Improving text generation with student-forcing optimal transport. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 9144–9156, 2020.
[LOG+19] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. RoBERTa: A robustly optimized BERT pretraining approach. arXiv preprint arXiv:1907.11692, 2019.
[M+05] Tom Minka et al. Divergence measures and message passing. Technical report, Citeseer, 2005.
[MG19] Andrey Malinin and Mark Gales. Reverse kl-divergence training of prior networks: Improved uncertainty and adversarial robustness. Advances in Neural Information Processing Systems, 32, 2019.
[Mos23] MosaicML. Introducing mpt-7b: A new standard for open-source, commercially usable llms, 2023.
[NDZ+19] Jeremy Nixon, Michael W Dusenberry, Linchuan Zhang, Ghassen Jerfel, and Dustin Tran. Measuring calibration in deep learning. In CVPR workshops, 2019.
[Ope22] OpenAI. Openai: Introducing chatgpt, 2022.
[Ope23] OpenAI. Gpt-4 technical report, 2023.
[OWJ+22] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. In Proceedings of NeurIPS, 2022.
[PH21] Richard Yuanzhe Pang and He He. Text generation by learning from demonstrations. In International Conference on Learning Representations, 2021.
12

[PLH+23] Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao. Instruction tuning with gpt-4. arXiv preprint arXiv:2304.03277, 2023.
[PSS00] Doina Precup, Richard S Sutton, and Satinder P Singh. Eligibility traces for off-policy policy evaluation. In Proceedings of the Seventeenth International Conference on Machine Learning, pages 759–766, 2000.
[RAB+23] Rajkumar Ramamurthy, Prithviraj Ammanabrolu, Kianté Brantley, Jack Hessel, Rafet Sifa, Christian Bauckhage, Hannaneh Hajishirzi, and Yejin Choi. Is reinforcement learning (not) for natural language processing: Benchmarks, baselines, and building blocks for natural language policy optimization. In The Eleventh International Conference on Learning Representations, 2023.
[RCG+15] Andrei A Rusu, Sergio Gomez Colmenarejo, Caglar Gulcehre, Guillaume Desjardins, James Kirkpatrick, Razvan Pascanu, Volodymyr Mnih, Koray Kavukcuoglu, and Raia Hadsell. Policy distillation. arXiv preprint arXiv:1511.06295, 2015.
[RWC+19] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. OpenAI Technical report, 2019.
[SCGL19] Siqi Sun, Yu Cheng, Zhe Gan, and Jingjing Liu. Patient knowledge distillation for bert model compression. arXiv preprint arXiv:1908.09355, 2019.
[SDCW19] Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter. arXiv preprint arXiv:1910.01108, 2019.
[SFA+22] Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilic´, Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François Yvon, Matthias Gallé, et al. Bloom: A 176b-parameter open-access multilingual language model. arXiv preprint arXiv:2211.05100, 2022.
[SGM19] Emma Strubell, Ananya Ganesh, and Andrew McCallum. Energy and policy considerations for deep learning in nlp. arXiv preprint arXiv:1906.02243, 2019.
[SHKK22] Joar Max Viktor Skalse, Nikolaus HR Howe, Dmitrii Krasheninnikov, and David Krueger. Defining and characterizing reward gaming. In Advances in Neural Information Processing Systems, 2022.
[SMSM99] Richard S Sutton, David McAllester, Satinder Singh, and Yishay Mansour. Policy gradient methods for reinforcement learning with function approximation. Advances in neural information processing systems, 12, 1999.
[SPW+13] Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and Christopher Potts. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of EMNLP, October 2013.
[SST+20] Kaitao Song, Hao Sun, Xu Tan, Tao Qin, Jianfeng Lu, Hongzhi Liu, and Tie-Yan Liu. Lightpaff: A two-stage distillation framework for pre-training and fine-tuning. arXiv preprint arXiv:2004.12817, 2020.
[SWD+17] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.
[SWR+22] Victor Sanh, Albert Webson, Colin Raffel, Stephen H Bach, Lintang Sutawika, Zaid Alyafeai, et al. Multitask prompted training enables zero-shot task generalization. In Proceedings of ICLR, 2022.
[TDFH+22] Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al. Lamda: Language models for dialog applications. arXiv preprint arXiv:2201.08239, 2022.
13

[TGZ+23] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanford alpaca: An instructionfollowing llama model. https://github.com/tatsu-lab/stanford_alpaca, 2023.
[TLI+23] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.
[WBH+21] Wenhui Wang, Hangbo Bao, Shaohan Huang, Li Dong, and Furu Wei. MiniLMv2: Multi-head self-attention relation distillation for compressing pretrained transformers. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 2140–2151, Online, August 2021. Association for Computational Linguistics.
[WBZ+22] Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and Quoc V Le. Finetuned language models are zero-shot learners. In Proceedings of ICLR, 2022.
[Wil92] Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Reinforcement learning, pages 5–32, 1992.
[WK21] Ben Wang and Aran Komatsuzaki. GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model, 2021.
[WKM+22] Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, and Hannaneh Hajishirzi. Self-instruct: Aligning language model with self generated instructions. arXiv preprint arXiv:2212.10560, 2022.
[WMA+22] Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, Anjana Arunkumar, Arjun Ashok, Arut Selvan Dhanasekaran, Atharva Naik, David Stap, et al. Benchmarking generalization via in-context instructions on 1,600+ language tasks. In Proceedings of EMNLP, 2022.
[WTB+22] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. Emergent abilities of large language models. Transactions on Machine Learning Research, 2022.
[WWD+20] Wenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan Yang, and Ming Zhou. MiniLM: Deep self-attention distillation for task-agnostic compression of pre-trained transformers. In Proceedings of the 34th International Conference on Neural Information Processing Systems, NIPS’20, Red Hook, NY, USA, 2020. Curran Associates Inc.
[WWZ+23] Minghao Wu, Abdul Waheed, Chiyu Zhang, Muhammad Abdul-Mageed, and Alham Fikri Aji. Lamini-lm: A diverse herd of distilled models from large-scale instructions. arXiv preprint arXiv:2304.14402, 2023.
[ZRG+22] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068, 2022.
[ZSL+23] Rongzhi Zhang, Jiaming Shen, Tianqi Liu, Jialu Liu, Michael Bendersky, Marc Najork, and Chao Zhang. Do not blindly imitate the teacher: Using perturbed loss for knowledge distillation. arXiv preprint arXiv:2305.05010, 2023.
[ZZ19] Huan Zhang and Hai Zhao. Minimum divergence vs. maximum margin: an empirical comparison on seq2seq models. In International Conference on Learning Representations, 2019.
14

A Derivations

A.1 Derivation of Equation 1

We compute the gradient of J (θ) = KL[qθ||p] with respect to θ using the Policy Gradient Theorem [SMSM99]:

p(y|x)

∇J

(θ)

=

−∇

E
y∼qθ (·|x)

log

qθ (y|x)

(8)

p(y|x)

= − ∇ qθ(y|x) log qθ(y|x) dy

(9)

p(y|x)

p(y|x)

= − qθ(y|x)∇ log qθ(y|x) dy − log qθ(y|x) ∇qθ(y|x)dy

(10)

p(y|x)

=

qθ(y|x)∇ log qθ(y|x)dy −

qθ (y|x)

log

∇ qθ (y|x)

log

qθ (y|x)dy

(11)

p(y|x)

=

− E (log y∼qθ (·|x)

qθ (y|x)

−

1)∇ log

qθ (y|x)

(12)

=

−E y∼qθ (·|x)

TT
( log
t=1 t′=1

p(yt′ |y<t′ , x) qθ(yt′ |y<t′ , x)

−

1)∇ log

qθ (yt |y<t ,

x)

(13)

=

−E y∼qθ (·|x)

TT
( log
t=1 t′=t

p(yt′ |y<t′ , x) qθ(yt′ |y<t′ , x)

−

1)∇

log

qθ(yt|y<t, x),

(14)

where Equation 14 is based on the fact that log qθ(yt|y<t, x) can only affect tokens at ≥ t positions

in y. By setting Rt =

T t′ =t

log

, p(yt′ |y<t′ ,x)
qθ (yt′ |y<t′ ,x)

we

obtain

Equation

2.

A.2 Derivation of Equation 3

To derive Equation 3, we first denote:

T

(∇J )Main = − E

Rt+1∇ log qθ(yt|y<t, x),

y∼qθ (·|x) t=1

T

(∇J )Reg = − E

∇ E [rt] .

y∼qθ (·|x) t=1 yt∼qθ (t)

(15)

Then, we re-write ∇J (θ) as:

T

∇J (θ) = − E

(Rt − 1)∇ log qθ(yt|y<t, x)

(16)

y∼qθ (·|x) t=1

T

=− E

Rt+1∇ log qθ(yt|y<t, x)

(17)

y∼qθ (·|x) t=1

T
−E y∼qθ (·|x) t=1

log p(yt|y<t, x) − 1 qθ(yt|y<t, x)

∇ log qθ(yt|y<t, x)

(18)

T

=(∇J )Main − E

E

y∼qθ (·|x) t=1 yt∼qθ (·|y<t,x)

log p(yt|y<t, x) − 1 qθ(yt|y<t, x)

∇ log qθ(yt|y<t, x)

(19)

T

=(∇J )Main − E

∇

E

y∼qθ (·|x) t=1 yt∼qθ (·|y<t,x)

− log qθ(yt|y<t, x) p(yt|y<t, x)

(20)

T

=(∇J )Main − E

∇ E [rt]

y∼qθ (·|x) t=1 yt∼qθ (t)

(21)

=(∇J )Main + (∇J )Reg,

(22)

where

Equation

20

uses

the

product

rule

of

the

gradient

and

rt

=

log

. p(yt |y<t ,x)
qθ (yt|y<t,x)

15

Below is an instruction that describes a task. Write a response that appropriately completes the request.
### Instruction: {instruction}
### Input: {input}
### Response:

Figure 8: The prompt wrapper for training and evaluation.

B Experimental Details
B.1 Training Details
Baselines For models with less than 1.3B parameters, we search for the learning rates in [5e-4, 1e-4, 5e-5], the batch sizes in [32, 64], and train these models for 20 epochs. For other models, we search for the learning rate in [5e-5, 1e-5, 5e-6], the batch sizes in [32, 64], and train these models for 10 epochs. For KD, we follow [SST+20] to mix the distillation loss with the language modeling loss on the ground truth responses by a mixture rate of 0.5. The checkpoints of each baseline are selected by the Rouge-L scores on the validation set.

MINILLM As shown in Algorithm 2.3, we first fine-tune the model on the training set using the vanilla language modeling objective to get a starting point of the subsequent MINILLM training. We fine-tune the model for 3 epochs using the best learning rate and batch size of the corresponding SFT baselines. We select the checkpoint with the lowest validation loss, not the Rouge-L score. Then, we train the model as described in Algorithm 2.3 using a learning rate 5e-6, a mini-batch size 64 in all cases. Similar to PPO [SWD+17], we collect 256 sentences at once and adopt 4 inner epochs. The clipping rate ϵ is set to 0.2, and the max length of the model is 512. We use temperature = 1 when sampling from qθ. We train the model for at most 5000 steps and select the final checkpoint using the Rouge-L score on the validation set. Our experiments are based on the NVIDIA V100 32G GPUs.

B.2 Evaluation Details
During the evaluation, we sample the responses from each model using temperature = 1, a max-length limit of 512, and random seeds [10, 20, 30, 40, 50]. Similar to [TGZ+23], we adopt a prompt wrapper shown in Figure 8 to convert each instruction-response pair to a sentence. For the GPT-4 feedback, we apply the prompt in Figure 9 and set the temperature = 0.7. For the classification tasks in the calibration paragraph of Section 3.3, we prompt the model to do zero-shot text classification with the prompt in Figure 10 and 11.

B.3 Exposure Bias Analysis

Following [AEABC22], we compute the ExAccErr with the following formula:

R(l) − lϵ(l)

ExAccErr(l) =

× 100%,

(23)

lϵ(l)

where R(l) is the accumulated regret of imitating the teacher distribution p at the time step l during the free-run generation:

R(l)

=

T t=1

E log
y<t∼qθ (·|x) yt∼qθ (·|y<t,x)

p(yt|y<t, x) , qθ(yt|y<t, x)

(24)

16

We would like to request your feedback on the performance of two AI assistants in response to the user instruction and input displayed above. Please rate the helpfulness, relevance, accuracy, and level of detail of their responses. Each assistant receives an overall score on a scale of 1 to 10, where a higher score indicates better overall performance. Please first output a single line containing only two values indicating the scores for Assistant 1 and 2, respectively. The two scores are separated by a space. In the subsequent line, please provide a comprehensive explanation of your evaluation, avoiding any potential bias and ensuring that the order in which the responses were presented does not affect your judgment.
Figure 9: GPT-4 evaluation prompt.
Below is an instruction that describes a task. Write a response that appropriately completes the request. ### Instruction: Determine the sentiment of the input sentence. Please respond as positive or negative. ### Input: {sentence} ### Response:
Figure 10: Zero-shot text classification prompt for SST2.
Below is an instruction that describes a task. Write a response that appropriately completes the request. ### Instruction: Read the input passage and answer the question: {question}? Your answer should be “Yes” or “No”. ### Input: {passage} ### Response:
Figure 11: Zero-shot text classification prompt for BoolQ.
17

Model GPT-J-6B GPT-2-760M
GPT-2-1.5B
GPT-Neo-2.7B

Method
Teacher
SFT w/o KD KD SeqKD MINILLM
SFT w/o KD KD SeqKD MINILLM
SFT w/o KD KD SeqKD MINILLM

DollyEval GPT4 R-L
65.8 27.3
50.7 25.4 51.6 26.7 51.4 26.0 54.0 25.8
58.4 27.6* 56.5 26.6 58.5 27.0 59.6 25.9
60.7 26.8 61.5 26.7 60.8 25.6 63.4 28.5*

SelfInst GPT4 R-L
57.4 17.3
38.3 12.4 38.9 13.4 39.2 14.0 43.7 16.3
42.9 14.3 46.0 14.5 43.2 13.6 48.5 16.6
45.4 15.8 47.0 16.0 47.2 16.2 52.5 17.1

VicunaEval GPT4 R-L
55.8 17.4
43.1 16.1 43.4 16.4 42.0 15.3 44.3 19.1*
48.6 16.3 47.2 16.5 46.6 16.9 48.9 19.4*
51.5 17.0 52.1 16.9 53.0 16.9 54.1 18.6*

S-NI R-L
28.0
21.5 25.9 25.5 27.1
27.6 27.6 28.0 28.5*
26.5 27.2 26.1 29.8*

UnNI R-L
33.6
27.1 33.2 32.5 35.5*
34.6* 34.9* 34.2* 35.9*
31.6 32.7 32.9 35.4*

Table 6: Evaluation results when GPT-J is the teacher. GPT4 and R-L stand for the average GPT-4 feedback scores and Rouge-L scores across 5 random seeds. The best scores of each model size are boldfaced, and the scores where the student model outperforms the teacher are marked with *.

and ϵ(l) is the average per-step error between qθ and p using the oracle context sampled from p as the

prefix:

ϵ(l)

=

1 l

T

E log

t=1

y<t ∼p(·|x) yt∼qθ (·|y<t,x)

p(yt|y<t, x) . qθ(yt|y<t, x)

(25)

Intuitively, the regret of qθ during generation is made of two parts: the error to estimate p given the oracle context and the error caused by the low-quality model-generated prefix. The former is calculated by lϵ(l), and the latter reflects the exposure bias. Therefore, ExAccErr measures the

relative error caused only by exposure bias.

C Additional Results
C.1 GPT-J as the Teacher
We present the evaluation results when using GPT-J as the teacher and GPT-2-760M, GPT-2-1.5B, and GPT-Neo-2.7B [BLW+21] as the student in Table 6. MINILLM outperforms the baselines in most cases.
C.2 Performance of Response Length on U-NI
The performance on different U-NI subsets split by the length of the ground truth response is shown in Figure 12. We have the same observation as in Section 3.3 that on short responses, all KD methods perform similarly, and on long responses, MINILLM outperforms other methods.

D Cases
We provide some cases generated by the models distilled by different methods based on LLaMA in Table 7. We find that MiniLLM generates more detailed and accurate responses compared with the baselines.

18

R-L Against SFT

4

KD SeqKD

MiniLLM

2

0 [0, 5]

[6, 10] [11, + ]

Ground Truth Length Range

Figure 12: The Rouge-L scores of the distilled models against the SFT models on the different evaluation subsets of UnNI split by the golden responses’ length.

19

Instruction Input

Case #1
During writing, we added an asterisk for the word that did not come to mind. You will need to provide several examples to demonstrate all the words that can be used in the sentence instead of the asterisk.
we * to know

SFT w/o KD KD
SeqKD
MINILLM
MINILLM w/o teacher-mixed

We can use “know” in the sentence instead of “we * to know” we went to know\n we will go to know\n we can go to know\n we may go to know\n we should go to know\n we must go to know we do to know\n we can do to know\n we should do to know\n we would do to know\n we ought to do to know We can use the following words in place of the asterisk:\n - want\n have\n - do\n - used\n - ought\n - desire\n - need\n - must
We to know

Instruction Input

Case #2
Take the title of the product and extract its attributes. The attributes in this case refer to the characteristics of the products, such as their brand and color. Your answer must a list following the format of “attribute: value”.
SAMSUNG 980 PRO SSD 2TB PCIe NVMe Gen 4 Gaming M.2 Internal Solid State Hard Drive Memory Card, Maximum Speed, Thermal Control, MZ-V8P2T0B

SFT w/o KD KD SeqKD MINILLM
MINILLM w/o teacher-mixed

Attribute: Maximum Speed\n Value: 3500 MB/s
attribute: brand, attribute: color, attribute: memory card, attribute: maximum speed, attribute: thermal control
brand: Samsung, color: black, size: 2TB, interface: PCIe NVME Gen 4, design: M.2 Internal
brand: SAMSUNG\n color: 980 PRO\n capacity: 2TB\n interface: PCIe NVME Gen 4\n speed: Maximum Speed\n threshold: Thermal Control\n form factor: M.2 Internal
SAMSUNGC 980 Pro SSD 2TB PCIe NVME Gen 4 Gaming M.II Internal Solid State Hard Drive memory Card, Max Speed, Thermal Control, AP1500K

Table 7: Instruction-following cases from the SelfInst dataset. MINILLM better follows the instructions and provides more detailed and accurate responses. Without the teacher-mixed sampling strategy in Section 2.2, the distilled model outputs short responses (Case #1) or simply repeats the input (Cases #2).

20

