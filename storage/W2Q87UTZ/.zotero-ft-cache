VideoControlNet: A Motion-Guided Video-to-Video Translation Framework by Using Diffusion Model with ControlNet

Zhihao Hu
Beihang University, China

Dong Xu
University of Hong Kong, China

â€œsoft painting, art by hidari and krenz cushart and wenjun lin and akihiko yoshida" â€œcyberpunk, portrait art of blade runner 8 k ultra realisticâ€

arXiv:2307.14073v2 [cs.CV] 3 Aug 2023

â€œd & d, fantasy, art by artgerm and greg rutkowski and alphonse muchaâ€ â€œultra realistic, trending on instagram, trending on tumblrâ€

Figure 1: Various generation results when using different input videos and different prompts. The first and the fourth rows are the input videos and the other rows contain the generation results when using the corresponding prompts.

ABSTRACT
Recently, diffusion models like StableDiffusion have achieved impressive image generation results. However, the generation process of such diffusion models is uncontrollable, which makes it hard to generate videos with continuous and consistent content. In this work, by using the diffusion model with ControlNet, we proposed a new motion-guided video-to-video translation framework called VideoControlNet to generate various videos based on the given prompts and the condition from the input video. Inspired by the video codecs that use motion information for reducing temporal redundancy, our framework uses motion information to prevent the regeneration of the redundant areas for content consistency. Specifically, we generate the first frame (i.e., the I-frame) by using the diffusion model with ControlNet. Then we generate other key frames (i.e., the P-frame) based on the previous I/P-frame by using our newly proposed motion-guided P-frame generation (MgPG)

method, in which the P-frames are generated based on the motion information and the occlusion areas are inpainted by using the diffusion model. Finally, the rest frames (i.e., the B-frame) are generated by using our motion-guided B-frame interpolation (MgBI) module. Our experiments demonstrate that our proposed VideoControlNet inherits the generation capability of the pre-trained large diffusion model and extends the image diffusion model to the video diffusion model by using motion information. More results are provided at our project page.
CCS CONCEPTS
â€¢ Information systems â†’ Multimedia content creation.
KEYWORDS
diffusion model, video generation, control net

,
1 INTRODUCTION
Video generation is an essential task in computer vision. Most previous works [40, 41] leverage the generative adversarial networks (GAN) [5] for generating continuous and content-consistent videos. Recently, diffusion models [8, 31] have attracted increasing attention, which has shown strong generation capability over generative adversarial networks. More recently, the StableDiffusion model is released and achieves state-of-the-art generation performance, which is trained on the large-scale text-image dataset and thus can generate various types of images based on the given text prompt. Although image diffusion models have achieved promising results these days, recently released video diffusion models [3, 9, 33] fail to generate continuous and content-consistent videos with high quality.
The main reason leading to the failure of video diffusion models is the uncontrollable diffusion process. Given the input text prompt, the diffusion process is uncontrollable and various types of images may be generated. Fortunately, ControlNet [44] is recently proposed for controlling the generation process of diffusion model based on different conditions (e.g., canny map, depth map or segmentation map, etc.). Therefore, it is intuitive to generate the output video based on the condition from the given input video. However, when the output video is directly generated frame-by-frame, it is still hard to promise the content consistency of the neighbor frames. One possible explanation is that one condition still corresponds to various output content and thus also results in inconsistent content between the independently generated frames.
Inspired by the video coding process [10, 12, 22] that adopts the motion information for reducing temporal redundancy, we propose a new motion-guided video-to-video translation framework called VideoControlNet by using the diffusion model with ControlNet, in which the motion information is adopted for content-consistency and the diffusion-model-based inpainting is used for covering the residual information. Therefore, by using the video coding paradigm that uses motion information for reducing redundancy, our method prevents the regeneration of the redundant areas based on the motion information and thus we can keep better content consistency. Specifically, we set the first frame as the I-frame and divide the following frames into different groups of pictures (GoP), in which the last frame of different GoPs is set as the key frame (i.e., P-frame) and other frames are set as B-frames. We first generate the I-frame independently by directly using the diffusion model with ControlNet, in which the condition is extracted from the I-frame of the input video and thus the output I-frame has the same content structure as the input I-frame. Then we generate the rest P-frames by using our newly proposed motion-guided Pframe generation (MgPG) module, in which the motion information is used for motion compensation of the redundant areas and the diffusion-model-based inpainting is performed for the generation of the newly occurred areas. Finally, the B-frames are generated based on our motion-guided B-frame interpolation (MgBI) module. Our proposed framework follows the paradigm of the B-frame-based video decoding process and inherits the generation capability of the image diffusion model, which makes it able to generate high-quality videos with continuous and consistent content. Therefore, guided by the motion information of the input video, our framework can

Zhihao Hu and Dong Xu
generate different videos with different styles or contents based on different given text prompts.
To demonstrate the effectiveness of our proposed framework, we perform experiments based on the current most well-known StableDiffusion model, which is trained on a large-scale text-image dataset and achieves state-of-the-art generation performance for image diffusion. The generation results of our VideoControlNet based on the StableDiffusion Model with ControlNet are provided in Figure 1. For example, for the first video of a woman walking in the forest, our method can generate the output video that has the same content as the input video and different styles (e.g., the soft-painting style of the artwork or the cyberpunk style of realistic photo). The results demonstrate that our method can keep the content consistent and generate various videos based on different text prompts, which inherit the spirit of the StableDiffusion Model and our VideoControlNet framework makes it able to extend the StableDiffusion model to the video diffusion model.
Our contributions are summarized as follows,
â€¢ We proposed a new motion-guided video-to-video translation framework called VideoControlNet by using the diffusion model with ControlNet following the paradigm of video coding.
â€¢ To generate the P-frames based on the given I-frame, we propose the motion-guided P-frame generation (MgPG) module, in which the motion information is extracted from the input video for keeping the content consistency, and the residual areas are generated by diffusion-model-based inpainting.
â€¢ We also propose the motion-guided B-frame interpolation (MgBI) module for generating the rest B-frames based on the reference I/P-frame.
â€¢ Experimental results demonstrate that our method inherits the generation capability of the pre-trained large diffusion model (i.e., StableDiffusion) and is able to translate the input video into diverse videos with different styles or contents.
2 RELATED WORKS
2.1 Diffusion Model
Image Generation. Diffusion models [8, 31, 34, 35] perturb data during the forward process and recover the data during the inversion process, which achieves the state-of-the-art image generation performance. Although the generation speed of the first few works [8, 34] are extremely slow due to their large number of diffusion steps, some works [20, 21] study to accelerate the generation speed by introducing new sampling strategies. Recently, LatentDiffusion [31] introduced VQ-VAE [38] to diffusion models and performs the time-consuming diffusion process in latent space, which is further extended to StableDiffusion by training on large-scale text-image dataset.
Control of Diffusion Model. Most recent state-of-the-art image diffusion models [1, 17, 26] are guided by the control of the text information, which can be achieved by extracting CLIP features [29] from the text and then concatenate the CLIP feature during the diffusion steps or use cross-attention module. SDEdit [23] achieved controllable image editing by adding noise to the given stroke without extra training steps for diffusion models. EGSDE [46] adopted

VideoControlNet: A Motion-Guided Video-to-Video Translation Framework by Using Diffusion Model with ControlNet

Input I-frame ğ‘‹ ğ‘ƒğ‘Ÿğ‘œğ‘šğ‘ğ‘¡

Optical Flow Net

Input P-frame ğ‘‹

Optical Flow Net

Condition Generation
Condition ğ‘‹
StableDiffusion with ControlNet

ğ‘€â†’

ğ‘€â†’

Motion-guided B-frame
Interpolation

Condition Generation
Condition ğ‘‹
Motion-guided P-frame Generation

ğ‘€âˆ—â†’

ğ‘€â†’ âˆ—

Motion-guided B-frame
Interpolation

Input P-frame ğ‘‹ âˆ—

,
â€¦

Condition Generation
Condition ğ‘‹ âˆ—
Motion-guided P-frame Generation

â€¦

Output I-frame ğ‘‹
(a) Generation of I frame

Output B-frames

Output P-frame ğ‘‹

(b) Generation of the first GoP

Output B-frames

Output P-frame ğ‘‹ âˆ—

(c) Generation of the second GoP

Figure 2: Overview of our proposed motion-guided video-to-video translation framework. (a) The generation process of I-frame: Taking the first input frame ğ‘‹0 as the I-frame, we first generate the condition image ğ‘‹0ğ‘ . Then we use the StableDiffusion with ControlNet to generate the output I-frame ğ‘‹Ë†0, which has the same content structure as the input I-frame ğ‘‹0. (b) The generation process of the first Group of Pictures (GoP): We take the output I-frame as the reference frame and perform the motion-guided P-frame generation (MgPG) to generate the output P-frame ğ‘‹Ë†ğ‘”. After that, the rest B-frames in the current GoP are generated
by using our newly proposed motion-guided B-frame interpolation (MgBI) module. (c) The generation processes of the rest
GoPs are the same as the generation process of the first GoP.

the energy function to control the generation during the denoising process of a pre-trained SDE. Recently, ControlNet [44] is proposed for adding extra conditions (e.g., canny map, depth map, segmentation map, etc.) to the pre-trained diffusion models, which makes it able to control the content structure of the generated results without sacrificing the generation ability.
Video Generation. With the success of text-to-image generation, a number of works [3, 4, 9, 19, 25, 32, 33, 42, 47] are proposed for generating videos based on image diffusion models. Some of these works [3, 9, 33] achieved the video diffusion model by changing the 2D Unet from the image diffusion models to 3D Unet and then training the 3D Unet structure on video datasets. Imagen Video [7] proposed a cascaded video generation framework by performing temporal super-resolutions and spatial super-resolutions on the initially generated low frame-rate and low-resolution videos. Tune-A-Video [42] proposed a one-shot tuning strategy to enhance temporal consistency. MMDiffusion [32] achieved simultaneous audio and video generation. [25] proposed the latent flow diffusion model to generate video from a single reference image, while they can only achieve low-resolution generation on specific datasets. VideoLDM [3] achieves the state-of-the-art text-to-video generation results by adding additional temporal layers to the pre-trained latent diffusion model [31] and uses a cascaded framework for generating high-resolution and high frame-rate videos. However, the state-of-the-art methods do not leverage the motion information for preventing the regeneration of redundant areas, which still leads to content inconsistency.
2.2 Video-to-Video Translation
Image-to-image translation algorithms like pix2pix [14] are able to achieve video generation by processing the input video frame by

frame, which cannot consider the temporal consistency of neighbor frames. Early video-to-video translation methods [40, 41] are proposed based on the generative adversarial networks (GAN) [5]. However, such GAN-based networks can only synthesize videos based on specific training data. Recently, inspired by the strong generation capability of diffusion models, some methods also adopt diffusion models for video editing [19] and video-to-video translation [4]. Gen-1 [4] proposed the structure-guided video generation algorithm by introducing the depth map as the condition, which needs to retrain the diffusion model on video data. Different from Gen-1, our framework is built upon the pre-trained large image diffusion model (i.e., StableDiffusion) and inherits the strong generation capability of StableDiffusion. Additionally, our framework follows the paradigm of the video coding [10, 11, 22] framework and prevents regeneration of the redundant areas by using motion information from the input video, which can better keep the content consistent.
2.3 Optical Flow Estimation
Our proposed VideoControlNet relies on the pre-trained optical flow estimation network for modulating the motion of the video. Therefore, it is necessary to select an effective optical flow estimation network for motion estimation. SpyNet [30] is widely used in learning-based video coding frameworks like DVC [22]. Although such optical flow estimation networks can achieve promising results on specific datasets, they are not general for estimating the optical flow on different types of videos. Recently, FlowFormer [13] is proposed and some models are trained on all the existing optical flow datasets, which is more general. To this end, we directly use the FlowFormer as our optical flow network. In the future, our

,

Zhihao Hu and Dong Xu

Inpainting Mask Generation

ğ‘€â†’

ğ‘‹

Backward

Warping

Input I Frame ğ‘‹

Warped Frame ğ‘‹

Forward Warping
ğ‘€â†’

Inpainting Mask ğ¼

ğ‘€â†’

Backward Warping

Reference Frame ğ‘‹

ğ‘€â†’

P-frame Generation

Warped Frame ğ‘‹

Stable Diffusion with ControlNet

ğ‘‹

ğ‘ğ‘Ÿğ‘œğ‘šğ‘ğ‘¡

Output Frame ğ‘‹

Figure 3: Details of our Motion-guided P-frame Generation (MgPG) module. In the inpainting mask generation module (i.e.,

the upper green block), we first warp the input frame ğ‘‹ğ‘–âˆ’ğ‘” by using the estimated optical flow ğ‘€ğ‘–âˆ’ğ‘”â†’ğ‘– and then calculated the residual information ğ‘…ğ‘– between the warped frame ğ‘‹Ëœğ‘– and the input frame ğ‘‹ğ‘– . We also generate an occlusion map ğ‘‚ğ‘– by using

forward warping based on the optical flow ğ‘€ğ‘–â†’ğ‘–âˆ’ğ‘”. Based on the residual information ğ‘…ğ‘– and the occlusion map ğ‘‚ğ‘– , we generate

the inpainting mask ğ¼ğ‘– . Then we perform the P-frame generation in the lower blue block by first warping the output reference

frame

ğ‘‹Ë†ğ‘–

based

on

the

optical

flow

ğ‘€ğ‘– âˆ’ğ‘”â†’ğ‘–

to

generate

the

wrapped

frame

ğ‘‹Â¯ğ‘– .

Based

on

the

inpainting

mask

ğ¼ğ‘– ,

the

condition

ğ‘‹ğ‘
ğ‘–

and the given prompt, we inpaint the uncertain areas by using the StableDiffusion with ControlNet and generate the final

output P-frame ğ‘‹Ë†ğ‘– .

method can be further enhanced by using more effective optical flow estimation networks.

3 METHOD

3.1 Overview

In this work, we propose a new video-to-video translation frame-

work called VideoControlNet to translate the input video to the

output video based on the given prompt. The problem formulation

and the overall pipeline are provided as follows.

Problem Formulation. Given the input video ğ‘‹ = {ğ‘‹0, ğ‘‹1, ..., ğ‘‹ğ‘› },

in which ğ‘‹ğ‘– denotes the input frame at the timestep ğ‘–. We denote

the first input frame ğ‘‹0 as the I frame. Then we divide the rest

frames into different groups of pictures (GoP), in which we set

the GoP size as ğ‘”. We also set the last frame of each GoP as P-

frame, and the other frames are denoted as B-frames. Our goal is to generate output video ğ‘‹Ë† = {ğ‘‹Ë†0, ğ‘‹Ë†1, ..., ğ‘‹Ë†ğ‘› } based on the given

text prompt. The corresponding frames of the output video are also

called I-frames, P-frames and B-frames.

Generation of I-frame. The generation process of the I-frame is provided in Figure 2(a). We first generate the I-frame ğ‘‹Ë†0 by

directly performing the pre-trained StableDiffusion model with

ControlNet,

in

which

the

condition

ğ‘
ğ‘‹0

(e.g.,

canny

map

or

depth

map) is extracted from the input I-frame ğ‘‹0. As the ControlNet is

able to control the content structure based on the given condition

ğ‘‹0ğ‘ , The generated I frame ğ‘‹Ë†0 has the same content structure as the

input frame ğ‘‹0.

Generation of the first GoP. As shown in Figure 2(b), taking the
output I-frame ğ‘‹Ë†0 as the reference frame, we generate the output P-frame ğ‘‹Ë†ğ‘” by using our newly proposed motion-guided P-frame

generation (MgPG) module based on the optical flow information extracted from the optical flow net [13] and the condition image ğ‘‹ğ‘”ğ‘ . After that, our motion-guided B-frame Interpolation (MgBI) module takes the output I-frame ğ‘‹Ë†0 and the output P-frame ğ‘‹Ë†ğ‘” as the reference frames and generates the output B frames in this GoP. Generation of the rest GoPs. The generation processes of the rest GoPs are similar to the generation process of the first GoP, in which the only difference is that we use the output P-frame in the previous GoP to replace the output I-frame as the reference frame for the MgPG module and MgBI module.
3.2 Motion-guided P-frame Generation
As the generation process of the diffusion model is unstable, which leads to content inconsistency when generating the output video frame-by-frame. Therefore, we propose the motion-guided P-frame generation (MgPG) method, which leverages the motion information from the input video to keep content consistent by preventing the regeneration of the redundant areas. Additionally, for the areas that do not appear in the previous frames (e.g., the occurrence of the occlusion areas), we propose the inpainting mask generation module to generate the inpainting mask and then perform inpainting based on StableDiffusion with ControlNet.
As shown in Figure 3, to generate the output P-frame ğ‘‹Ë†ğ‘– in the P-frame generation block, we first take the previously generated I/P-frame ğ‘‹Ë†ğ‘–âˆ’ğ‘” as the reference frame and perform the backward warping operation based on the motion information (i.e., optical flow) ğ‘€ğ‘–âˆ’ğ‘”â†’ğ‘– to generate the warped frame ğ‘‹Â¯ğ‘– . It is observed that the warped frame ğ‘‹Â¯ğ‘– is not perfect due to the occurrence of the occlusion areas (e.g., at the left side of the woman and the right

VideoControlNet: A Motion-Guided Video-to-Video Translation Framework by Using Diffusion Model with ControlNet

,

border in the warped frame ğ‘‹Â¯ğ‘– in Figure 3). In the video coding task, the residual information will be added in such areas, while it is hard to generate the residual information for our generated P frame. Fortunately, the diffusion model can achieve image inpainting based on the given inpainting mask. Therefore, the key point is to generate the inpainting mask ğ¼ğ‘– . Inpainting Mask Generation.
The inpainting mask generation process is shown in the upper green block of Figure 3. Similar to the P-frame generation process, we first warp the input frame ğ‘‹ğ‘–âˆ’ğ‘” based on the motion information ğ‘€ğ‘–âˆ’ğ‘”â†’ğ‘– . Then the residual information can be obtained by calculating ğ‘…ğ‘– = (ğ‘‹ğ‘– âˆ’ ğ‘‹Ëœğ‘– )2. Therefore, it is intuitive to inpaint the areas with large residuals. However, only using the residual information to generate the inpainting mask is not reliable. The reason is that the RGB values in some occlusion areas are not changed too much, which makes it hard to find out all the inpainting areas by only using the residual map ğ‘…ğ‘– (more discussions will be provided in Section 4.4). Therefore, we additionally use the forward optical flow to find out the areas that do not appear in the reference frame. Specifically, we first estimate the inverse optical flow ğ‘€ğ‘–â†’ğ‘–âˆ’ğ‘” and then perform the forward warping operation based on the map with values of all ones, which is formulated as follows,

ğ‘‚ğ‘– = ğ¹ğ‘œğ‘Ÿğ‘¤ğ‘ğ‘Ÿğ‘‘ğ‘Š ğ‘ğ‘Ÿğ‘ (ğ‘‚ğ‘›ğ‘’ğ‘ , ğ‘€ğ‘–â†’ğ‘–âˆ’ğ‘”)

(1)

in which ğ‘‚ğ‘›ğ‘’ğ‘  denotes the map with values of all ones. In the generated occlusion map ğ‘‚ğ‘– , areas with zeros denote they do not occur in the reference frame, which should be inpaint. After that, we generate the inpainting mask ğ¼ğ‘– by considering both residual information ğ‘…ğ‘– and the occlusion map ğ‘‚ğ‘– , which is formulated as follows,

ğ¼ğ‘–,ğ‘˜ =

1 0

if ğ‘‚ğ‘–,ğ‘˜ âˆ’ ğ›¼ğ‘…ğ‘–,ğ‘˜ > ğ‘¡â„ğ‘Ÿğ‘’ğ‘ â„ğ‘œğ‘™ğ‘‘ otherwise

(2)

in which ğ¼ğ‘–,ğ‘˜ , ğ‘‚ğ‘–,ğ‘˜ , ğ‘…ğ‘–,ğ‘˜ denotes their corresponding values at the
spatial location ğ‘˜. ğ›¼ and ğ‘¡â„ğ‘Ÿğ‘’ğ‘ â„ğ‘œğ‘™ğ‘‘ are hyper-parameters. The areas
with value zeros in ğ¼ğ‘– will be inpaint in the diffusion model.
Finally, guided by the inpainting mask ğ¼ğ‘– , we inpaint the newly occurred areas in the warped frame ğ‘‹Â¯ğ‘– by using StableDiffusion with ControlNet and generate the output P-frame ğ‘‹Ë†ğ‘– .

3.3 Motion-guided B-frame Interpolation

Based on the generated output I-frame and P-frames, we generate

the rest B-frames by using our motion-guided B-frame interpolation

(MgBI) module. In the video coding task, the B-frame always needs

less residual information than the P-frames. Therefore, in our MgBI

method, we directly interpolate the B-frames based on the reference

I/P frames and the motion information extracted from the input

frames without using the time-consuming diffusion model.

The detailed generation process is shown in Figure 4. When generating the output frame ğ‘‹Ë†ğ‘— , we first take the two nearest I/P frames ğ‘‹Ë†ğ‘–âˆ’ğ‘” and ğ‘‹Ë†ğ‘– as the reference frames. Then we generate the

corresponding motion information ğ‘€ğ‘–âˆ’ğ‘”â†’ğ‘— and ğ‘€ğ‘–â†’ğ‘— by using the

optical flow net [13]. After that, we perform the backward warping

operation to generate the warped frames ğ‘‹Â¯ ğ‘“ ğ‘Ÿğ‘œğ‘›ğ‘¡ and ğ‘‹Â¯ğ‘ğ‘ğ‘ğ‘˜ , in

ğ‘—

ğ‘—

which some areas in the warped frames are inaccurate due to the

occlusion areas. Considering the occlusion areas from one reference

Reference Frame ğ‘‹

ğ‘€â†’

ğ‘€â†’

Reference Frame ğ‘‹

Backward Warping Backward Warping

Warped Frame ğ‘‹

Warped Frame ğ‘‹

Match Score ğ‘†

Match Score ğ‘†

Output Frame ğ‘‹

Figure 4: Details of our Motion-guided B-frame Interpolation (MgBI) module. Given two reference I/P frames ğ‘‹Ë†ğ‘–âˆ’ğ‘” and ğ‘‹Ë†ğ‘– , we aims to generate the output frame ğ‘‹Ë†ğ‘— . We first

estimate the motion information ğ‘€ğ‘–âˆ’ğ‘”â†’ğ‘— and ğ‘€ğ‘–â†’ğ‘— from the

corresponding input frames. Then we perform the backward

warping operation to generate the warped frames ğ‘‹Â¯ ğ‘“ ğ‘Ÿğ‘œğ‘›ğ‘¡ and
ğ‘—
ğ‘‹Â¯ğ‘ğ‘ğ‘ğ‘˜ based on the corresponding reference frames and mo-
ğ‘—

tion

information.

Based

on

the

match

score

ğ‘“ ğ‘Ÿğ‘œğ‘›ğ‘¡
ğ‘†

and ğ‘†ğ‘ğ‘ğ‘ğ‘˜ ,

ğ‘—

ğ‘—

we generate the output B-frame ğ‘‹Ë†ğ‘— .

frame always occur in another reference frame, we simply generate

the match score of each warped frame to produce the final output frame ğ‘‹Ë†ğ‘— .

The match score generation process is similar to the inpainting

mask generation process that also uses both residual information

and forward warping operation to generate the inpainting mask.

Take the match score calculation process of the warped frame

ğ‘‹Â¯ ğ‘“ ğ‘Ÿğ‘œğ‘›ğ‘¡ as an example, we first perform the backward warping op-
ğ‘—
eration based on the input frame ğ‘‹ğ‘–âˆ’ğ‘” and therefore the residual

ğ‘“ ğ‘Ÿğ‘œğ‘›ğ‘¡
ğ‘…

can

be

calculated.

Similar

to

Eq.

1,

we

use

the

motion

infor-

ğ‘—

mation ğ‘€ğ‘–âˆ’ğ‘”â†’ğ‘— to perform the forward warping operation based

on the map with values of ones and thus we generate the occlusion

map ğ‘‚ ğ‘“ ğ‘Ÿğ‘œğ‘›ğ‘¡ . The residual map ğ‘…ğ‘ğ‘ğ‘ğ‘˜ and the occlusion map ğ‘‚ğ‘ğ‘ğ‘ğ‘˜

ğ‘—

ğ‘—

ğ‘—

are also generated in the same process. Then the intermediate score

ğ‘†Â¯ğ‘“ ğ‘Ÿğ‘œğ‘›ğ‘¡ and ğ‘†Â¯ğ‘ğ‘ğ‘ğ‘˜ are calculated as follows,

ğ‘—

ğ‘—

ğ‘†Â¯ğ‘“ ğ‘Ÿğ‘œğ‘›ğ‘¡ = ğ‘‚ ğ‘“ ğ‘Ÿğ‘œğ‘›ğ‘¡ âˆ’ ğ›½ğ‘…ğ‘“ ğ‘Ÿğ‘œğ‘›ğ‘¡

(3)

ğ‘—,ğ‘˜

ğ‘—,ğ‘˜

ğ‘—

ğ‘†Â¯ğ‘ğ‘ğ‘ğ‘˜

=

ğ‘ğ‘ğ‘ğ‘˜
ğ‘‚

âˆ’

ğ‘ğ‘ğ‘ğ‘˜
ğ›½ğ‘…

ğ‘—,ğ‘˜

ğ‘—,ğ‘˜

ğ‘—

(4)

And we use the softmax operation with temperature to generate the final match score ğ‘† ğ‘“ ğ‘Ÿğ‘œğ‘›ğ‘¡ ,
ğ‘—

ğ‘’ğ‘¥ğ‘ (ğ‘†Â¯ğ‘“ ğ‘Ÿğ‘œğ‘›ğ‘¡ )/ğœ

ğ‘“ ğ‘Ÿğ‘œğ‘›ğ‘¡
ğ‘†

=

ğ‘—,ğ‘˜

(5)

ğ‘—,ğ‘˜

ğ‘’ğ‘¥ğ‘ (ğ‘†Â¯ğ‘“ ğ‘Ÿğ‘œğ‘›ğ‘¡ /ğœ) + ğ‘’ğ‘¥ğ‘ (ğ‘†Â¯ğ‘ğ‘ğ‘ğ‘˜ /ğœ)

ğ‘—,ğ‘˜

ğ‘—,ğ‘˜

and the match score ğ‘†ğ‘ğ‘ğ‘ğ‘˜ can be calculated by ğ‘†ğ‘ğ‘ğ‘ğ‘˜ = 1 âˆ’ ğ‘† ğ‘“ ğ‘Ÿğ‘œğ‘›ğ‘¡ ,

ğ‘—

ğ‘—

ğ‘—

in which the temperature ğœ is the hyper-parameter.

, Input Video
Text2Live

â€œwinter scene, beautiful snow, snow everywhereâ€

Zhihao Hu and Dong Xu â€œnighttime, nightâ€

Ours

Figure 5: Generation results of Text2LIVE [2] and our method when using the input video at the first row and the corresponding prompts.

Finally, the output B-frame ğ‘‹Ë†ğ‘— is calculated by adding up the weighted warped frame, which is also formulated as follows

Table 1: User preference of Text2Video-Zero [16], CCPL [43] and our proposed method.

ğ‘‹Ë† ğ‘—

=

ğ‘“ ğ‘Ÿğ‘œğ‘›ğ‘¡
ğ‘†
ğ‘—

Ã— ğ‘‹Â¯ ğ‘“ ğ‘Ÿğ‘œğ‘›ğ‘¡
ğ‘—

+

ğ‘ğ‘ğ‘ğ‘˜
ğ‘†

ğ‘—

Ã— ğ‘‹Â¯ğ‘ğ‘ğ‘ğ‘˜
ğ‘—

(6)

Therefore, our motion-guided B-frame generation module is able to generate the B-frames efficiently and effectively without using the time-consuming StableDiffusion.

4 EXPERIMENTS
4.1 Experimental Setup
Datasets. Due to the strong generation capability of the large StableDiffusion Model, our VideoControlNet is also general and can be applied to any type of input video. Therefore, we evaluate our method on various video datasets including the video coding datasets HEVC dataset [36], UVG dataset [24], and MCL-JCV datasets [39]. Following the previous video translation works [2], we evaluate our VideoControlNet framework on the DAVIS dataset [27], which also contains various types of videos.
Implementation Details. In this work, we use the pre-trained StableDiffusion model [31] in version 1.5 with the ControlNet [44] for I-frame generation and our motion-guided P-frame generation (MgPG) module. Specifically, we use the released models from the official GitHub repository of ControlNet, in which we use the depth map and canny map condition model. We use the DDIM Sampler [34] as the sampling strategy and sample 20 steps for both I-frame generation and the inpainting process of P-frames. For optical flow estimation, we adopt the pre-trained model from the official GitHub repository of Flowformer [13].
In our experiments, we set the GoP size ğ‘” as 10. The weights of ğ›¼ and ğ›½ are set as 5 and 10, respectively. The ğ‘¡â„ğ‘Ÿğ‘’ğ‘ â„ğ‘œğ‘™ğ‘‘ for generating the inpainting mask is simply set as 0.5. We set the temperature ğœ as 20. In our inpainting mask generation module, we further apply the Gaussian Blur operation to expand the inpainting area of the occupancy map and use the min-pooling operation to generate the inpainting mask in latent space. All of our experimental results are generated on the machine with Tesla V100 GPU with 16GB GPU memory. We resize the input videos to 960 Ã— 540 for evaluation.

Text2Video-Zero CCPL VideoControlNet (Ours)

User Preference 9.4% 15.8% 74.7%

4.2 Quantitative Results
User Study. We conduct a user study to evaluate the generation quality of our method, the SOTA diffusion-based video-to-video translation method Text2Video-Zero [16] and the SOTA video style transfer method CCPL [43]. Considering the previous non-diffusionbased video generation methods cannot support text-based instruction, we evaluate the SOTA video style transfer method CCPL [43] by using the most relevant image of the text prompt as the style image.
We selected 100 video-prompt pairs for evaluation and use the official code and the default parameters for each method, in which the videos are from the DAVIS dataset [27]. For each user, 30 video-prompt combinations are randomly selected. Finally, we collected 720 votes from 24 users. The preference percentages of Text2Video-Zero [16], CCPL [43] and our VideoControlNet are provided in Table 1. Our VideoControlNet outperforms previous methods Text2Video-Zero [16] and CCPL [43] by a large margin and achieves 74.7% user preference. It is observed that the generated videos of Text2Video-Zero lack temporal consistency. Although CCPL sometimes achieves comparable results on specific prompts, most generated videos are much worse than the results of our VideoControlNet.
Objective Metrics. We further provide the quantitative results on the DAVIS dataset [27] in Table 2, in which the video names (e.g., bus, dogs-jump, hike, paragliding-launch) are used as the text prompts. We use the objective metrics including FrÃ©chet Video Distance (FVD) [37], Inception Score (IS), FrÃ©chet Inception Distance (FID) [6], average CLIP [28] similarity between video frames and text (CLIPSIM), LPIPS [45] and the L2 distance between the optical flow [30] of the input video and the generated video (Optical Flow Error).

VideoControlNet: A Motion-Guided Video-to-Video Translation Framework by Using Diffusion Model with ControlNet

,

Input Video

"cyberpunk, portrait art of blade runner 8 k ultra realistic"

Depth Map Condition

â€œultra realistic, trending on instagram, trending on tumblrâ€

Canny Map Condition

Figure 6: Generated results when using different conditions. The input videos are provided in the first row. The sentence below the input video is the input prompt of the StableDiffusion. The last two rows are the generated results, in which the middle row is the results when using the ControlNet with the depth map condition, and the last row contains the results when using the ControlNet with the canny map condition.

Table 2: Quantitative results on the DAVIS dataset [27].

Text2Video-Zero Ours

FVD(â†“) 1670.39 981.99

IS(â†‘) 13.23 18.02

FID(â†“) 119.01 92.17

CLIPSIM(â†‘) 25.66 26.14

LPIPS(â†“) 0.56 0.50

Optical Flow Error(â†“) 17.99 7.91

Speed (â†‘) 0.19fps 0.30fps

Considering the video style transfer method CCPL [43] directly takes the video as input while our method and Text2Video-Zero [16] only take the conditions (e.g., depth map, edge map) of the input video as input, we only compare our method with the SOTA videoto-video translation method Text2Video-Zero [16]. We also report the running speed of both methods when generating the video with 40 frames and with the resolution of 960 Ã— 540. It is observed that our method outperforms the SOTA diffusion-based method Text2Video-Zero [16] in terms of all metrics including FVD, IS, FID, CLIPSIM, LPIPS and Optical Flow Error on the DAVIS dataset with faster inference speed, which demonstrate the effectiveness of our method.
4.3 Qualitative Results
We take the Text2LIVE [2] method as the baseline method to demonstrate the effectiveness of our proposed VideoControlNet. Text2Live [2] is recently proposed for text-guided video editing that adopts layered neural atlases [15], which needs to fine-tune on each video and runs extremely slow. We use their official code and the provided configuration to generate the video.
The experimental results are provided in Figure 5, in which simple prompts below the input video are used for both Text2LIVE and our VideoControlNet. It is observed that the generated video of our method has better visual quality than the generation results from Text2LIVE due to the strong generation quality of StableDiffusion. For example, in the snow scene, the road of our generated video is more realistic than the output of Text2LIVE. We also observe that the Text2LIVE method achieves good temporal smoothness due to its reliance on Layered Neural Atlases [15]. However, the generation quality of Text2Live outputs varies a lot on different types of videos. For example, in the nighttime scene, the road is illuminated without street lights. Moreover, the inference speed of Text2Live is

extremely slow and it even requires more than 10 hours for editing a single video, while our method generates the video at about 3.4 seconds per frame. Finally, it is shown that the generated content of our method is consistent with the input video, which makes it able to use the corresponding optical flow to generate the output video by using our proposed VideoControlNet. The experimental results demonstrate that our method achieves better generation quality than the previous methods and can also keep the content consistent.
4.4 Model Analysis
Results when using Different Conditions. In this work, we use the canny maps and depth maps as the condition information for the ControlNet [25] to generate the output video that has the same content as the input video. In Figure 6, we provide the generated results when using different conditions. It is observed that when using the depth map as the condition of ControlNet, the generated results are more spatial and three-dimensional. For example, the generated fishes conditioned on the depth map are more threedimensional than the results conditioned on the canny map. When using the canny map as the condition of ControlNet, the generated results contain more details (e.g., the bear fur). Therefore, we can condition the canny map for more detailed 2D image generation and use the depth map for generating 3D results.
Generation of the Inpainting Mask. Our inpainting masks are generated based on both occlusion maps and residual maps for generating the newly occurred areas for the current frame. To better illustrate our inpainting mask generation process, we visualize the residual map, the occlusion map and our final inpainting mask ğ¼ğ‘– in Figure 7. It is observed that most occlusion areas can be found in our occlusion map (e.g., the left side of the woman), which is generated by using the forward warping operation based on the

,

(a) Residual Map

(b) Ground Truth

(c) Occulusion Map

(d) Warped Image

(e) Inpainting Mask

(f) Warped Image with Mask

Figure 7: Visualization of our inpainting mask generation during our motion-guided P-frame generation (MgPG). (a) is the residual map that represents the difference between the warped frame and the ground truth frame. (b) is the ground truth of the current frame. (c) is the occlusion map calculated by using the forward warping operation. (d) is the wrapped frame by using the backward warping operation based on the reference frame. (e) is the inpainting mask calculated based on the residual map and the occlusion map. (f) is our warped frame with the inpainting mask.

reverse optical flow. However, the motion information between the neighbor P frames may be very large and thus the optical flow estimation network may not generate accurate motion information. To this end, we additionally use the residual map for detecting the occlusion areas. Additionally, the residual map shown in Figure 7 is very sparse. Therefore, it is also not reliable if we only use the residual map to generate the inpainting mask. To this end, we use both the residual map and the occlusion map for generating the inpainting mask. As shown in Figure 7(f), the occlusion areas are well masked by the inpainting mask, which demonstrates the effectiveness of our inpainting mask generation module.
Running Speed. The detailed running time of different modules is provided in Table 3. We evaluate our inference speed on the machine with a single Tesla V100 GPU. The input videos are resized to the resolution of 960 Ã— 540. The GoP size ğ‘” is set as 10 and we use 20 diffusion steps. It is observed that the StableDiffusion model with ControlNet needs 13.7s for generating the I-frame or inpainting the occlusion areas of P-frames. The inpainting mask generation module costs 0.97s, in which the main part of the time is spent on the optical flow estimation network [13]. Due to the fast speed of our motion-guided B-frame interpolation which costs 1.9s per frame, our average generation speed is about 3.4s per frame when generating 4 GoP of images with ğ‘” = 10. Therefore, our proposed VideoControlNet framework is even faster than generating the

Zhihao Hu and Dong Xu

Table 3: Running Time of different modules in which we use 20 sampling steps for the StableDiffusion with ControlNet. â€œPixel to Latent" denotes encoding the image to latent space for the diffusion networks. â€œLatent to Pixel" denotes decoding the image from latent space. We also provide the inference time of I-frame generation, P-frame generation and B-frame generation. The average time is calculated when the GoP size is set as 10.

StableDiffusion with ControlNet Pixel to Latent Latent to Pixel Inpainting Mask Generation I-frame Generation Motion-Guided B-frame Interpolation Motion-Guided P-frame Generation Average Time Per Frame

Time 13.7s 0.13s 0.01s 0.97s 13.7s 1.9s 14.8s 3.4s

output video frame-by-frame by using StableDiffusion Model with ControlNet, which requires 13.7s per frame.
Discussion of Applications. Our VideoControlNet achieves videoto-video translation by using motion information, in which the content of the input video should be consistent with the output video. Therefore, the most straightforward application is the video style transfer, which can achieve various styles based on different given prompts. Additionally, our method can also achieve video editing with the extra mask of the object to be edited. In conclusion, our method can be regarded as a video version of the ControlNet, that is able to control the content and motion information based on the given input video and prompts. Unfortunately, due to the detailed motion information, the condition should be more strong that can control the details of the output photo. Therefore, the segmentation map or human pose can not be used as the condition of our VideoControlNet. More results are provided in our appendix.
5 CONCLUSION
In this work, we propose a new video-to-video translation framework called VideoControlNet based on the StableDiffusion model with ControlNet, in which the motion information is adopted for better content consistency. We first generate the I-frame and then divide the rest frames into different groups of pictures (GoP), in which the last frame of each GoP is set as the P-frame and others are Bframes. For the P-frame generation, we propose the motion-guided P-frame generation (MgPG) module to prevent the regeneration of the occlusion areas and only inpaint the occlusion areas. For the B-frame generation, we propose the motion-guided B-frame interpolation (MgBI) module to directly interpolate the B-frames by using the two nearest I/P frames as the reference frame. The experimental results demonstrate that our VideoControlNet framework achieves impressive video generation results with high-quality content and good content consistency with the motion information from the input video. In our future work, we will study adding more learnable networks for better content consistency.

VideoControlNet: A Motion-Guided Video-to-Video Translation Framework by Using Diffusion Model with ControlNet

,

REFERENCES
[1] Omri Avrahami, Dani Lischinski, and Ohad Fried. 2022. Blended diffusion for text-driven editing of natural images. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 18208â€“18218.
[2] Omer Bar-Tal, Dolev Ofri-Amar, Rafail Fridman, Yoni Kasten, and Tali Dekel. 2022. Text2live: Text-driven layered image and video editing. In Computer Visionâ€“ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23â€“27, 2022, Proceedings, Part XV. Springer, 707â€“723.
[3] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis. 2023. Align your Latents: High-Resolution Video Synthesis with Latent Diffusion Models. arXiv preprint arXiv:2304.08818 (2023).
[4] Patrick Esser, Johnathan Chiu, Parmida Atighehchian, Jonathan Granskog, and Anastasis Germanidis. 2023. Structure and content-guided video synthesis with diffusion models. arXiv preprint arXiv:2302.03011 (2023).
[5] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 2020. Generative adversarial networks. Commun. ACM 63, 11 (2020), 139â€“144.
[6] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. 2017. Gans trained by a two time-scale update rule converge to a local nash equilibrium. Advances in neural information processing systems 30 (2017).
[7] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik P Kingma, Ben Poole, Mohammad Norouzi, David J Fleet, et al. 2022. Imagen video: High definition video generation with diffusion models. arXiv preprint arXiv:2210.02303 (2022).
[8] Jonathan Ho, Ajay Jain, and Pieter Abbeel. 2020. Denoising diffusion probabilistic models. Advances in Neural Information Processing Systems 33 (2020), 6840â€“6851.
[9] Jonathan Ho, Tim Salimans, Alexey A Gritsenko, William Chan, Mohammad Norouzi, and David J Fleet. 2022. Video Diffusion Models. 31 (2022).
[10] Zhihao Hu, Guo Lu, and Dong Xu. 2021. FVC: A New Framework towards Deep Video Compression in Feature Space. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 1502â€“1511.
[11] Zhihao Hu and Dong Xu. 2023. Complexity-Guided Slimmable Decoder for Efficient Deep Video Compression. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 14358â€“14367.
[12] Zhihao Hu, Dong Xu, Guo Lu, Wei Jiang, Wei Wang, and Shan Liu. 2022. FVC: An End-to-End Framework towards Deep Video Compression in Feature Space. IEEE Transactions on Pattern Analysis and Machine Intelligence (2022).
[13] Zhaoyang Huang, Xiaoyu Shi, Chao Zhang, Qiang Wang, Ka Chun Cheung, Hongwei Qin, Jifeng Dai, and Hongsheng Li. 2022. Flowformer: A transformer architecture for optical flow. In Computer Visionâ€“ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23â€“27, 2022, Proceedings, Part XVII. Springer, 668â€“685.
[14] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros. 2017. Image-toimage translation with conditional adversarial networks. In Proceedings of the IEEE conference on computer vision and pattern recognition. 1125â€“1134.
[15] Yoni Kasten, Dolev Ofri, Oliver Wang, and Tali Dekel. 2021. Layered neural atlases for consistent video editing. ACM Transactions on Graphics (TOG) 40, 6 (2021), 1â€“12.
[16] Levon Khachatryan, Andranik Movsisyan, Vahram Tadevosyan, Roberto Henschel, Zhangyang Wang, Shant Navasardyan, and Humphrey Shi. 2023. Text2video-zero: Text-to-image diffusion models are zero-shot video generators. arXiv preprint arXiv:2303.13439 (2023).
[17] Gwanghyun Kim, Taesung Kwon, and Jong Chul Ye. 2022. Diffusionclip: Textguided diffusion models for robust image manipulation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2426â€“2435.
[18] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. 2023. Segment anything. arXiv preprint arXiv:2304.02643 (2023).
[19] Shaoteng Liu, Yuechen Zhang, Wenbo Li, Zhe Lin, and Jiaya Jia. 2023. Video-P2P: Video Editing with Cross-attention Control. arXiv preprint arXiv:2303.04761 (2023).
[20] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. 2022. DPM-Solver: A Fast ODE Solver for Diffusion Probabilistic Model Sampling in Around 10 Steps. 31 (2022).
[21] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. 2022. Dpm-solver++: Fast solver for guided sampling of diffusion probabilistic models. arXiv preprint arXiv:2211.01095 (2022).
[22] Guo Lu, Wanli Ouyang, Dong Xu, Xiaoyun Zhang, Chunlei Cai, and Zhiyong Gao. 2019. Dvc: An end-to-end deep video compression framework. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 11006â€“ 11015.
[23] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. 2021. Sdedit: Guided image synthesis and editing with stochastic differential equations. In International Conference on Learning Representations.

[24] A. Mercat, Marko Viitanen, and J. Vanne. 2020. UVG dataset: 50/120fps 4K sequences for video codec analysis and development. Proceedings of the 11th ACM Multimedia Systems Conference (2020).
[25] Haomiao Ni, Changhao Shi, Kai Li, Sharon X Huang, and Martin Renqiang Min. 2023. Conditional Image-to-Video Generation with Latent Flow Diffusion Models. arXiv preprint arXiv:2303.13744 (2023).
[26] Alexander Quinn Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob Mcgrew, Ilya Sutskever, and Mark Chen. 2022. GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models. In International Conference on Machine Learning. PMLR, 16784â€“16804.
[27] Jordi Pont-Tuset, Federico Perazzi, Sergi Caelles, Pablo ArbelÃ¡ez, Alex SorkineHornung, and Luc Van Gool. 2017. The 2017 davis challenge on video object segmentation. arXiv preprint arXiv:1704.00675 (2017).
[28] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. 2021. Learning transferable visual models from natural language supervision. In International conference on machine learning. PMLR, 8748â€“8763.
[29] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. 2022. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125 (2022).
[30] Anurag Ranjan and Michael J Black. 2017. Optical flow estimation using a spatial pyramid network. In Proceedings of the IEEE conference on computer vision and pattern recognition. 4161â€“4170.
[31] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and BjÃ¶rn Ommer. 2022. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 10684â€“10695.
[32] Ludan Ruan, Yiyang Ma, Huan Yang, Huiguo He, Bei Liu, Jianlong Fu, Nicholas Jing Yuan, Qin Jin, and Baining Guo. 2022. MM-Diffusion: Learning Multi-Modal Diffusion Models for Joint Audio and Video Generation. arXiv preprint arXiv:2212.09478 (2022).
[33] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, et al. 2022. Make-a-video: Text-to-video generation without text-video data. arXiv preprint arXiv:2209.14792 (2022).
[34] Jiaming Song, Chenlin Meng, and Stefano Ermon. 2020. Denoising diffusion implicit models. arXiv preprint arXiv:2010.02502 (2020).
[35] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. 2020. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456 (2020).
[36] Gary J Sullivan, Jens-Rainer Ohm, Woo-Jin Han, and Thomas Wiegand. 2012. Overview of the high efficiency video coding (HEVC) standard. IEEE Transactions on circuits and systems for video technology 22, 12 (2012), 1649â€“1668.
[37] Thomas Unterthiner, Sjoerd van Steenkiste, Karol Kurach, Raphael Marinier, Marcin Michalski, and Sylvain Gelly. 2018. Towards Accurate Generative Models of Video: A New Metric & Challenges. arXiv preprint arXiv:1812.01717 (2018).
[38] Aaron Van Den Oord, Oriol Vinyals, et al. 2017. Neural discrete representation learning. Advances in neural information processing systems 30 (2017).
[39] Haiqiang Wang, Weihao Gan, Sudeng Hu, Joe Yuchieh Lin, Lina Jin, Longguang Song, Ping Wang, Ioannis Katsavounidis, Anne Aaron, and C-C Jay Kuo. 2016. MCL-JCV: a JND-based H.264/AVC video quality assessment dataset. In 2016 IEEE International Conference on Image Processing (ICIP). IEEE, 1509â€“1513.
[40] Ting-Chun Wang, Ming-Yu Liu, Andrew Tao, Guilin Liu, Bryan Catanzaro, and Jan Kautz. 2019. Few-shot Video-to-Video Synthesis. Advances in Neural Information Processing Systems 32 (2019).
[41] Ting-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Guilin Liu, Andrew Tao, Jan Kautz, and Bryan Catanzaro. 2018. Video-to-Video Synthesis. Advances in Neural Information Processing Systems 31 (2018).
[42] Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Weixian Lei, Yuchao Gu, Wynne Hsu, Ying Shan, Xiaohu Qie, and Mike Zheng Shou. 2022. Tune-A-Video: One-Shot Tuning of Image Diffusion Models for Text-to-Video Generation. arXiv preprint arXiv:2212.11565 (2022).
[43] Zijie Wu, Zhen Zhu, Junping Du, and Xiang Bai. 2022. CCPL: contrastive coherence preserving loss for versatile style transfer. In European Conference on Computer Vision. Springer, 189â€“206.
[44] Lvmin Zhang and Maneesh Agrawala. 2023. Adding conditional control to text-to-image diffusion models. arXiv preprint arXiv:2302.05543 (2023).
[45] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. 2018. The unreasonable effectiveness of deep features as a perceptual metric. In Proceedings of the IEEE conference on computer vision and pattern recognition. 586â€“595.
[46] Min Zhao, Fan Bao, Chongxuan Li, and Jun Zhu. 2022. Egsde: Unpaired imageto-image translation via energy-guided stochastic differential equations. arXiv preprint arXiv:2207.06635 (2022).
[47] Daquan Zhou, Weimin Wang, Hanshu Yan, Weiwei Lv, Yizhe Zhu, and Jiashi Feng. 2022. Magicvideo: Efficient video generation with latent diffusion models. arXiv preprint arXiv:2211.11018 (2022).

,

Zhihao Hu and Dong Xu

â€œOil painting styleâ€ â€œCartoon styleâ€
â€œChinese painting styleâ€ â€œWatercolor styleâ€

Figure 8: Video style transfer results by using the prompts to control the style. The first row is the input video and the other rows are the generated videos based on the input video and given prompts.

Optical Flow ğ‘€â†’

Forward warping

ğ‘‚ğ‘›ğ‘’ğ‘ 

Occlusion map ğ‘‚

Figure 9: Illustration of our occlusion map generation by using forward warping operation. Taking a map full of values one and the optical flow ğ‘€ğ‘–â†’ğ‘–âˆ’ğ‘” as input, the forward warping operation generates the occlusion map ğ‘‚ğ‘– .

A VIDEO EXAMPLES
In our main paper, we provide some examples of the generated videos in Figure 1, in which only a few images in the video are shown. To further demonstrate the effectiveness of our proposed VideoControlNet, we provide the video examples in our project page. It is observed that our generated videos have the same motion and content as the input video and our VideoControlNet can generate various types of videos based on any type of input video and prompts.

B ILLUSTRATION OF OCCLUSION MAP GENERATION
In our inpainting mask generation module, a ğ‘‚ğ‘›ğ‘’ğ‘  map is forward warped based on the optical flow ğ‘€ğ‘–â†’ğ‘–âˆ’ğ‘” for generating the occlusion map ğ‘‚ğ‘– . Backward warping is the widely used operation for motion compensation and the optical flows are always generated for backward warping. However, we cannot figure out the locations that are newly occurred in the current frame by using the backward warping operation. Therefore, we use the forward warping operation to figure out the newly occurred areas. As shown in Figure 9, the background is moving towards the left side and the woman has only little movement. Therefore, by using the forward warping based on the ğ‘‚ğ‘›ğ‘’ğ‘  map, the values on the left side of the woman move to the left, and the values inside the woman still stay inside, which makes it easy to find out the occlusion areas at the left side of the woman. By using both the occlusion map and the residual map, we can find out the areas to be inpainted.
C APPLICATIONS
As discussed in our main paper, our VideoControlNet framework is able to achieve applications like style transfer and video editing. Therefore, we provide more visualization results in the supplementary materials.

VideoControlNet: A Motion-Guided Video-to-Video Translation Framework by Using Diffusion Model with ControlNet

,

Figure 10: The video editing results of our proposed method. The first row and the fourth row are the input videos. The second row and the fifth row are the masks to be edited. The third row and the last row are the output videos, which are generated by using the prompts â€œA robot is hiking" and â€œpolar bear", respectively.

C.1 Video Style Transfer
As shown in Figure 8, we provide the style transfer results of our proposed VideoControlNet. The first row contains the input video and the other rows are the generated results of different styles, in which the styles are controlled by the prompts. It is observed that our VideoControlNet framework is able to translate the input video into different styles including the oil painting style, cartoon style, Chinese painting style and watercolor style, which further demonstrate the effectiveness of our method. We also provide more video style transfer results in our project.

C.2 Video Editing
The video editing results are provided in Figure 10. Given the input video and the masks that needed to be edited, our VideoControlNet can generate the output video that edits the content based on the given masks and prompts. For example, for the output video in the last row of Figure 10, we masked the man that is hiking and the corresponding prompt is â€œa robot is hiking". Therefore, our method can generate a robot that is hiking. Note that our masks are generated by using the official code of â€œSegment Anything" [18]. We also provide more video editing results in our project.

