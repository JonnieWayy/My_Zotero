arXiv:2305.18583v1 [cs.CV] 29 May 2023

Controllable Text-to-Image Generation with GPT-4
Tianjun Zhang1 Yi Zhang2 Vibhav Vineet2 Neel Joshi2 Xin Wang2 1UC Berkeley 2Microsoft Research
Abstract
Current text-to-image generation models often struggle to follow textual instructions, especially the ones requiring spatial reasoning. On the other hand, Large Language Models (LLMs), such as GPT-4, have shown remarkable precision in generating code snippets for sketching out text inputs graphically, e.g., via TikZ. In this work, we introduce Control-GPT to guide the diffusion-based text-to-image pipelines with programmatic sketches generated by GPT-4, enhancing their abilities for instruction following. Control-GPT works by querying GPT-4 to write TikZ code, and the generated sketches are used as references alongside the text instructions for diffusion models (e.g., ControlNet) to generate photo-realistic images. One major challenge to training our pipeline is the lack of a dataset containing aligned text, images, and sketches. We address the issue by converting instance masks in existing datasets into polygons to mimic the sketches used at test time. As a result, Control-GPT greatly boosts the controllability of image generation. It establishes a new state-of-art on spatial arrangement and object positioning generation and enhances users’ control of object positions, sizes, etc., nearly doubling the accuracy of prior models. As a first attempt, our work shows the potential for employing LLMs to enhance the performance in computer vision tasks. 1
1 Introduction
Researchers have made remarkable progress in text-to-image generation in recent years, with significant advancements in Generative Adversarial Networks (GANs) [2, 7, 11], autoregressive models [20, 29], and Diffusion Models [21, 23, 22]. These models have shown impressive capabilities in synthesizing photorealistic images. More recently, large-scale pretrained text-to-image models [19, 22, 23] have garnered considerable interest, leading to a multitude of downstream applications, such as image editing [3, 9] and image inpainting [22].
However, precise control during image generation from textual inputs remains a formidable challenge [6]. As illustrated in Figure 1, specifying the exact location, size, or shape of objects using natural language is inherently difficult and prevalent models like DALL-E 2 [21] or Stable Diffusion [22] often lead to unsatisfactory results. To address this problem, existing approaches often depend on extensive prompt engineering [22] or manually created image sketches [30, 12]. They are both inefficient and difficult to generalize, as they demand substantial manual effort.
On the other hand, users typically have greater control over code generation, as they can write programs to manipulate various aspects of objects, such as their shapes, sizes, locations, and more. This allows for precise adjustments and customization according to specific requirements. In recent times, large language models (LLMs) have demonstrated remarkable capabilities in code generation tasks. Models such as GPT-4 [17] have achieved near-human-level performance in coding contests and have been successful in solving numerous complex coding problems [4, 15]. This progress encourages us to investigate the potential of harnessing LLMs to enhance the controllability of text-to-image generation models.
1Project website with code: https://github.com/tianjunz/Control-GPT
Preprint. Under review.

a microwave above a sink

a sports ball above a book

a tv to the right of a microwave

(a) DALL-E-2

(b) Stable Diffusion

(c) Control-GPT

Figure 1: Controllable text-to-image generation with GPT-4 in the loop. Among all the models, ControlGPT is both good at generating multiple objects, and the generated image follows exactly the TikZ sketch. Both DALL-E 2 and Stable Diffusion are not only unable to generate all the objects stated in the text consistently, but it is also hard to control their generated image layout.

In this work, we introduce Control-GPT, a simple yet effective framework that harnesses the power of LLMs to generate sketches based on text prompts, illustrated in Figure 2. Control-GPT works by first employing GPT-4 to generate sketches in the form of TikZ code, inspired by [4]. As we can see from Figure 1 (c), the programmatic sketches are drawn following the exact text instructions, despite the rareness of the concepts among natural images. Subsequently, these sketches are fed into Control-GPT, a variant of Stable Diffusion that accepts additional inputs, such as reference images, edges, and segmentation maps. These generated sketches act as reference points for diffusion models, enabling them to better understand spatial relationships and unusual concepts, as opposed to relying solely on text prompts. This process eliminates the need for human intervention in prompt engineering or sketch creation and helps to improve the controllability of diffusion models.
In practice, utilizing off-the-shelf pretrained models to synthesize images from textual prompts and TikZ sketches can often lead to unsatisfactory results. For example, the GPT-4 generated sketches achieve close to 97% accuracy in following the spatial relations described in [6]. However, directly using the generated sketches as segmentation maps and feeding them to the pretrained ControlNet cannot translate the performance improvement. This is due to text-to-image models possibly lacking a full understanding of the content within TikZ sketches, as the pretrained models haven’t seen such sketches during training. To address this issue, we convert instance masks from existing instance segmentation datasets, such as COCO [14] and LVIS [8], into polygon representations, which are similar to sketches generated by GPT-4. We construct a dataset containing triplets of images, textual captions, and polygon-represented sketches and finetune ControlNet, which takes additional polygons and grounding tokens with object names and locations [12] on the constructed dataset. We find this approach can mitigate the gap in understanding the GPT-generated sketches and can help the model better follow the text prompt instructions.
For evaluation, we first investigate the capabilities of widely-used LLMs (e.g., LLaMA [28] and Alpaca [27] in addition to the GPT series of models) in the context of sketch generation. It is found that only the GPT-4 model can consistently generate reasonable TikZ images, while the majority of open-source models face difficulty in producing compilable TikZ code. We then evaluate our image generation framework, which incorporates GPT-4, on the spatial relation benchmark created by [6]. Our proposed model sets a new state-of-the-art with an accuracy rate of 44.2%, almost doubling the performance of the standard stable diffusion models (18.8%). Moreover, human evaluation reveals that our model is capable of handling out-of-distribution prompts and generating intricate scenes comprising multiple objects. As an initial effort, our work demonstrates the potential of integrating the coding abilities of LLMs within visual domains to enhance model controllability.
2 Related Work
Text-to-image models. Large-scale text-to-image models, which are categorized into autoregressive models [29], and diffusion based models [19, 22, 23] have shown impressive results. The diffusionbased model has recently attracted increasing attention for text-to-image generation. Text-to-image diffusion models often work by encoding text prompts into latent vectors and then using a diffusion model to diffuse pixels. Notable large-scale models include Stable Diffusion [22], Imagen [23],
2

Language Model
\begin{tikzpicture} \draw[red, fill=red] (01.5, 2.56) …

Grounding Text {Unicorn, (C0.a5Ct, 0a.t5)}
Sketch Image

Grounding Tokens Text Encoder

Image Encoder (Sketch)

Sketch Tokens

Self Attention
Self Attention Self Attention

ControlNet Architecture …

Generated Image

Draw a unicorn.

Stable Diffusion

Pass

Figure 2: Control-GPT Architecture. Our model is built on top of ControlNet to take additional grounding text. The model takes in both reference images and grounding object text, fusing them using attention layers before feeding to Stable Diffusion.

DALL-E [22], etc. We take diffusion-based text-to-image generation models as a basis and explore how to increase their controllability for precise instruction following.
Controllable generation. Current state-of-the-art image diffusion models predominantly focus on text-to-image methods, making text-guided approaches the most direct method for enhancing control over diffusion models. Achieving precise control via text is inherently challenging. ControlNet [30] is a neural network architecture that controls pretrained large diffusion models to support additional input conditions like Canny edges, Hough line, human poses, segmentation maps, etc. This model design enjoys greater flexibility of the condition signals and allows for more precise control of the generated images. However, such condition signals often need additional human effort to obtain and may not be available for open-world concepts. Our work addresses this issue by integrating diffusion models with LLMs to generate sketches at test time automatically.
In a similar line, GLIGEN [12], an approach for open-set grounded image generation, adds an additional grounding token to encode the bounding box coordinates of the objects. Our work complements GLIGEN and incorporates its grounding token design. Our work is also in line with image generation from layouts [31, 25, 26, 10, 13]. These techniques typically operate in a closed-world environment characterized by a fixed vocabulary and access to predefined layouts.
Programmatic sketch generation. Large language models (LLMs) have exhibited remarkable performance on text-related generation tasks [17], and GPT-4 stands out as the most advanced LLM to date. Recently researchers have shown that GPT-4 exhibits “sparks” of human-level intelligence [4] in a range of application domains. In particular, GPT-4 exhibits near-human level coding capability as well as an unexpected surprise —its “image generation” via writing code, where examples include GPT-4 successfully writing a snippet of TikZ commands in LATEX that draws a “unicorn” graphic. However, the resulting images are usually far from photo-realistic. Our work further quantifies GPT4’s programming ability on sketch generation and integrates it into diffusion-based image generation models for better controllability. Our work, as a first attempt, shows the potential of integrating LLMs with other domains with a code interface.
3 Controllable Text-to-Image Generation with GPT-4
3.1 Preliminary: ControlNet
ControlNet [30] is a variant of diffusion models that takes additional input conditions. Built upon the Stable Diffusion model, ControlNet adds a conditional pathway, a trainable copy of the 12 encoding blocks, and one middle block of Stable Diffusion while locking the parameters of the trained Stable Diffusion model. The neural network blocks are connected by a special convolution layer, “zero convolution”, which is a 1×1 convolution layer with both weight and bias initialized with zeros.
During training, ControlNet is finetuned on image-text pairs with conditions represented as Canny edges, Hough lines, human poses, user sketches, semantic segmentation maps, etc. Experiments show that ControlNet can generate images following the conditions. In this work, we use ControlNet as the base image generation model and extend it with pathways for programmatic sketches and grounding tokens, detailed in the following section.
3

3.2 Our Framework
We now introduce our framework that integrates LLM-generated sketches into ControlNet for more precise controllability. As illustrated in Figure 2, we first prompt GPT-4 to generate sketches in TikZ code following the text descriptions and additionally output the object positions. We compile the TikZ code in LATEXand convert the sketches into image formats. We then feed the programmatic sketches, text descriptions, and grounding tokens of object positions to a tuned ControlNet model to generate images following the conditions.
Training a ControlNet with GPT-4 generated sketches is necessary as pretrained ControlNets do not understand the generated sketches and cannot translate them into realistic images. The challenge is that there are no aligned text, images, and sketches data available for training. In the following sections, we will describe the components in our framework and our training process to mitigate the gap in understanding the programmatic sketches.

3.2.1 Training Data Construction

Feeding the Language Model-generated Tikz output directly into ControlNet enhances the controllable generation process. However, ControlNet often struggles to accurately interpret the sketches provided. Common issues include the model failing to adhere to the layout of the sketch or the prompts given. Therefore, finetuning ControlNet on sketches is necessary for the model to accept the guidance from programmatic sketches at test time.

Two zebras seem to be embracing in the wild
A stove with a lighted hood in the kitchen

When fine-tuning text-to-image models, researchers often use massive datasets of text and

(a) Captions

(b) Images (c) Polygon Sketches

image pairs from the Internet (e.g., Flicker [18], Figure 3: Training data construction. We convert

LIAON-5B [24]). In our case, although GPT- the instance masks in LVIS data into polygons and use

4 can synthesize unlimited sketches, it’s hard the corresponding images and captions from COCO to

to find ground-truth images that align with the construct the training data to fine-tune ControlNet.

sketches. Therefore, we explore methods that

utilize annotations from existing image datasets and try to mimic the sketches to mitigate the gap

between the training and testing data.

Specifically, we find that representing instance masks in polygons and turning them into sketch-like images can reduce the gap. As shown in Figure 3, we construct a dataset of caption, image and polygon sketch triplets from the COCO [14] and LVIS [8] dataset and use it to fine-tune ControlNet. LVIS and COCO share the same set of images and LVIS provides a larger object vocabulary of over 1200 classes than COCO. We take roughly 120K images and captions from COCO and convert the object masks from the corresponding LVIS annotations. This enables the model to take in the polygons representing objects in an image and generate an output that adheres to the desired style. We will release our constructed dataset to the community.

3.2.2 Additional Object Grounding

Complementary to sketches and text instructions as a condition, we find that adding additional object grounding tokens to ControlNet is useful to further eliminate ambiguity. The grounding token is defined to associate the sketches with object names and positions, serving as an explicit semantic label for the sketches. This technique is particularly helpful when the LLM-generated sketches might be too abstract and not accurately represent the intended objects (Figure 4). Without the grounding token, we can see generation errors like the model successfully follows the layout of the provided sketch but generates incorrect objects. Detailed ablations can be found in Appendix.

To incorporate the grounding tokens in ControlNet, we design grounding tokens in the form of {object_name, object_position}:

Grounding Tokens : lgd = {lo, lp}

(1)

Image Tokens : limg = {lpatch1 , lpatch2 , ..., lpatchk }

(2)

4

All Control Tokens : l = Concat(lgd, limg)

(3)

Here, lo represents the object name grounding token, lp denotes the position embedding token, and lpatchi denotes the ith image patch token in the visual transformer outputs, as illustrated in Figure 2. We permit a maximum of 30 tokens per image and feed these tokens, along with the image tokens,

into the transformer blocks to generate control signals.

Fine-tuning process. During training, we fine-tune ControlNet with our constructed dataset based on COCO. We freeze the stable diffusion blocks and the text branch in ControlNet while fine-tuning the control branch of the network with all the control tokens l, a concatenation of image tokens, and grounding tokens. This design is flexible and effectively separates the control branch from the general stable diffusion, allowing for more flexible updates on either side.

3.2.3 Querying GPT-4 for Programmatic Sketches at Inference
Once the ControlNet is trained, we can now integrate the GPT-4 pipeline into the text-to-image generation process. Beyond the objects and scenes that are trained, users can query GPT-4 with novel prompts in a zero-shot manner and request Control-GPT to generate photo-realistic images.
To prompt GPT-4, we ask users to follow the prompt example below, which will request GPT-4 to request structured outputs of TikZ code snippets, and the associated object names and positions. We can then use the output from GPT-4 to compile sketch images and obtain the grounding tokens. Note that we specify the image size as 5.12-by-5.12 to make sure LLMs do not get confused by large values. We later scale the output by 100× to match the size of the COCO images used at training. We will present examples of the TikZ code in Appendix and release the code together with the rest of the dataset to the community.
1 "input": "Draw a tv above a surfboard using TikZ without adding labels . The entire image should be inside a 5.12*5.12 bounding box. First, you need to provide a step -by -step drawing guide. Then, you need to generate the code following the guide. Finally, summarize the drawing with: Summary of the drawing, {‘object name ’: $OBJECT_NAME, ’position ’: $(X, Y)} Make sure each object is separted and filled with red color."

4 How Accurate are LLMs at Drawing Sketches?

The precision of Control-GPT largely relies on the precision and controllability of LLMs at generating the sketches at first hand. In this section, we conduct a human evaluation of the outputs of prevalent LLMs and benchmark the performance of LLMs on sketch generation. We find that the GPT-series models are significantly better than open-sourced models like LLaMa [28] and Alpha [27] on sketch generation and GPT-4 exhibits astonishingly high accuracy (∼97%) at following text instructions.

Human evaluation. We randomly sample 100 queries from the Visor Dataset [6], which includes 25K text prompts specifying the spatial relationships of two objects like “a carrot above a boat”, “a bird below a bus”. These text prompts are challenging partially because many of them are rare compositions of two unrelated objects, and associating the spatial deixis in text with regions in images is not easy. Shown in Table 2 in Section 5, DALL-E 2 and Stable Diffusion can only achieve 37.89% and 18.81% on this benchmark. We will defer detailed comparison in the later section.

Models

# of errors # of empty image or w.r.t instructions non-runnable code

A bird below

BUS

a bus

GPT-4 [17]

3

5

GPT-3.5

24

7

ChatGPT [1]

60

7

LLaMA [28]

-

100

Alpaca [27]

-

100

Table 1: Human evaluation on sketch generations.

A carrot above
a boat

A handbag above a
potted plant (Failure)

GPT-4

GPT-3.5

Chat-GPT

Figure 4: Visualization of the generated sketches

We examine the performance of LLMs by querying them to generate a TikZ code snippet to describe the given text description. As shown in Table 1, most of the code snippets from GPT-series models can be compiled to valid sketch images, while the outputs from LLaMA and Alpaca are either empty or

5

Uncond (%) Cond (%) OA (%) Visor 1 (%) Visor 2 (%) Visor 3 (%) Visor 4 (%)

Control-GPT
44.17 65.97 48.33 69.80 51.20 35.67 20.48

GLIGEN
13.76 31.41 19.78 35.64 13.80 4.55 1.06

GLIDE
1.98 59.06 3.36 62.86 9.54 1.59 0.26

GLIDE+CDM
6.43 63.21 10.17 70.52 16.48 2.92 0.40

DalleM
16.17 59.67 27.10 71.22 32.54 12.82 3.65

CV2
12.17 65.89 18.47 74.37 25.40 7.15 1.26

DALL-E 2
37.89 59.27 63.93 83.92 53.86 26.52 8.54

SD
18.81 62.98 29.86 75.67 32.67 11.19 2.65

SD+CDM
14.99 64.41 23.27 74.89 27.65 9.19 2.12

∆
25.36 2.99 18.57 -5.09 18.53 24.48 17.83

Table 2: Results on the Visor spatial dataset. We compare different image generation models on their abilities to understand spatial locations. Results show that Control-GPT has a significant win on unconditional and conditional accuracy, even compared to the closed-source models. For object generation accuracy, Control-GPT also reaches a SoTA performance (except for DALL-E 2, a model trained in private data).

not runnable. Within the GPT-series models, we find the latest GPT-4 only fails three times out of the 95 queries, which have succeeded in generating a valid sketch, giving a roughly 97% in successfully following the text instruction. ChatGPT, a finetuned version of GPT-3.5 with reinforcement learning with human feedback (RLHF), significantly underperforms the original GPT-3.5. There might be a trade-off between the chatting ability and the code generation during the tuning process.
Illustration. In Figure 4, we provide a visualization of sketch examples from the GPT-series models. Although the generated sketches are not photo-realistic, they often capture the semantic meaning and correctly reason the spatial relationship of the objects. The generated sketches often surprisingly get object shapes correct with simple code snippets. In the last row of the figure, we show one of the failure cases of GPT-4, where the model fails to generate the object shape, while GPT-3.5 manages to give a correct sketch. The high precision of GPT-4 at sketch generation encourages us to use it to enhance the controllability of image generation models.
5 Experiments
In this section, we evaluate Control-GPT on a range of experimental settings to test its controllability regarding to spatial relations, object positions, and sizes based on the Visor dataset [6]. We also extend the evaluation to multiple objects and out-of-distribution prompts. Extensive experiments show that Control-GPT can significantly boost the controllability of diffusion models.
Baselines. We compare with a wide range of baselines, including open-source and closed-source models. Open-sourced models include (1) GLIGEN [12]: a fine-tuned Stable Diffusion [10] model that takes bounding box and object name as additional control information for grounded text-to-image generation; (2) ControlNet w/Segmentation [30]: a variant of the Stable Diffusion model using instance segmentation maps as control information; (3) ControlNet w/ Canny Detection: a variant of ControlNet using canny edge detection images as condition; (4) Stable Diffusion: an open-source image generation model conditioned on text trained on LAION-5B dataset [24] without additional conditions except for text; (5) CogView2 [5]: a pretrained image generation model via hierarchical transformers; and (6)GLIDE [16]: a text-conditioned image generation diffusion model by OpenAI. For the closed-sourced model, we compare with (7) DALL-E 2 [21]: OpenAI’s most advanced model for image generation. Our model is based on ControlNet, and we use DALL-E 2 for benchmark comparison only.
Dataset. As mentioned before, our model is finetuned on COCO [14] images and captions with LVIS [8]’s instance annotations. We extract the polygon of each object in the dataset and use the bounding box to generate the grounding tokens. The size of our dataset is approximately 120k images and captions. We finetune the model with four epochs of a learning rate 2e-5. We also freeze the Stable Diffusion backbone and only finetune the control branch in the network. The details for training can be found in Appendix.
5.1 Following Spatial Relations in Text Prompts
We adopt the Visor benchmark defined in [6] to examine the models’ ability to follow spatial relations in the text instructions. We follow [6] for the evaluation metrics. Given an object A, an object B and their spatial relationship in text, Object Accuracy (OA) is used to determine whether the model can generate both objects A and B. Unconditional Accuracy (Uncod) is used to examine whether
6

the model can generate object A and B, as well as get their spatial relationship correct. Conditional Accuracy (Cond) is conditioned on object A and B being correctly generated and determines whether the model generate their spatial relationship. Visor k(k = {1, 2, 3, 4}) means for a given relationship, at least k generation has the correct object and relationship. For each text prompt, we sample four images to make the evaluation more consistent.
We present the evaluation results in Table 2. The Control-GPT gets the SoTA performance under different evaluation metrics. It achieves 44.17% on Uncod scores while the base image generation model, Stable Diffusion, achieves only 18.81%. Control-GPT also outperforms DALL-E 2, a proprietary model from OpenAI, which has an Uncod score of 37.89%. Control-GPT also achieves SoTA on other scores on Cond compared to other diffusion model variants. We can see that our fine-tuned model can largely translate the performance gain from the programmatic sketches and mitigate the gap between the training and testing data.

5.2 Following Object Position, Size and Color in Text Prompts
We extend the evaluation to further study more fine-grained control on object sizes and positions specified in text prompts. For evaluation, we randomly sample 100 samples from the Visor dataset and associate them with the position randomly chosen from 4 options and size from 3 options. Figure 5 shows our prompt for querying GPT-4. Detailed setup is presented in Appendix.
Draw a giraffe to the left of an apple. The entire image should be 5.12*5.12. The giraffe should be centered at the position (1.5, 2.5) of size (1.0, 1.0). The apple should be centered at position (3.5, 2.5) of size (0.5, 0.5).

Figure 5: Example prompt for controlling object positions and sizes. Example prompt for benchmarking object position and size for different models. This is directly passed to the GPT-4 to draw the sketch or ControlNet/Stable Diffusion for generating an image.

Quantitative evaluation. Following the convention in [6], we use OWL-ViT to measure whether the model follows the text instruction on the objects’ size and location. We run the object detection model for each image and measure whether the detected object size and position match the ones specified in the prompt. Note that we tolerate ϵ% error relative to the image size (512×512) for both size and position. That is, if the detected size and position are within ϵ% absolute distance relative to the entire figure size, we consider it a success. For baselines, we compare with ControlNet and Stable Diffusion. The ControlNet takes in the same GPT-4 TikZ files and prompts with location and size. Stable diffusion only takes in detailed prompts.
We present the quantitative evaluation results in Table 3. We can see that our Control-GPT model can better control the size and location of the objects given some specifications. Compared to Stable Diffusion (SD-v1.5), which hardly has control over object positions and sizes, we improve the overall accuracy from 0% to 14.18%. Compared to the off-the-shelf ControlNet, Control-GPT also achieves better performance across all metrics, obtaining an overall improvement from 8.46% to 4.18%. These results show the potential of our LLM-integrated framework on more fine-grained and precise control of the image generation process.

ϵ = 3.9%

Accuracy
SD-v1.5 ControlNet Control-GPT (ours) ∆ (vs. SD)

Obj1 Pos
1.84 13.60 16.42 14.58

Obj1 Size
1.84 15.07 21.64 19.80

Obj2 Pos
2.57 15.44 17.91 15.34

Obj2 Size
2.20 15.07 23.13 20.93

All Pos
1.84 15.07 21.64 19.80

All Size
2.20 15.07 23.13 20.93

Pos & Size
0.00 8.46 14.18 14.18

Table 3: Results on controlling object positions and sizes. Control-GPT outperforms the base Stable Diffusion model and the off-the-shelf ControlNet model under all metrics by a large margin. Still, we can see the overall accuracy of fine-grained control over object positions and sizes is relatively low, and our work shows the potential for using LLMs to improve precise control of image generation.

Visualization. We additionally present the qualitative results in Figure 6, where we can see ControlGPT can draw objects following the specification of object positions and sizes. In contrast, ControlNet can also follow but struggles to generate the correct objects and Stable Diffusion cannot follow the specifications.

7

a Suitcase above a car

a car above a donut

a sports ball to the left of a bird

(a) Control-GPT

(b) ControlNet

(c) Stable Diffusion

Figure 6: Controlling object positions and sizes. Images generated by Control-GPT can follow exactly along the specification of position and size. ControlNet can also follow, but struggles to generate the correct object, and Stable Diffusion isn’t able to follow the specifications.

Model
GLIDE GLIDE + CDM CV2 DALL-E 2 SD SD + CDM GLIGEN Control-GPT

left
57.78 65.37 68.50 56.47 64.44 69.05 36.90 72.50

Visor Score (%)

right

above

61.71 65.46 68.03 56.51 62.73 66.52 36.37 70.28

60.32 59.40 63.72 60.99 61.96 62.51 36.64 67.85

below
56.24 59.84 62.51 63.24 62.94 59.94 33.05 65.70

left
3.10 12.78 20.34 64.30 29.00 23.66 19.78 49.80

Object Acc (%)

right

above

3.46 12.46 19.30 64.32 29.89 21.17 19.63 48.27

3.49 7.75 17.71 65.66 32.77 23.66 21.20 47.97

below
3.39 7.68 16.54 61.45 27.8 24.61 18.77 46.95

Table 4: More analysis on different spatial relations. We compare our model with the aforementioned baselines on four variants of spatial relations. Control-GPT consistently outperforms all the baselines in Visor scores. In terms of object accuracy, it also reaches dominant performance except for DALL-E 2.

5.3 Ablation Study and Analysis
Ablation on spatial relations. We also study whether the model has a preference for different types of spatial relationships (e.g., left/right/above/below) as part of the analysis for the spatial relation benchmark in Section 5.1. As we can see from Table 4, Control-GPT works consistently better than all the baseline models in terms of Visor Score and object accuracy. One exception is the DALL-E 2 model, which is possibly trained on private data with higher-quality object categories and, therefore, good at generating different objects compared to the rest of open-sourced models. We also notice that text-to-image models often struggle more with above and below compared to left and right.
Relationship between multiple objects. In previous sections, we have shown evaluations of generating spatial relations of two objects. In this section, we conduct a further evaluation on benchmarking Control-GPT capability of generating multiple objects, with their spatial relationship specified by prompts. We show some qualitative examples in Figure 7. We see that Control-GPT exhibits better performance in understanding the spatial relationship between different objects and putting them in the layout with the help of GPT-4. While DALL-E 2 and Stable Diffusion are often missing generating some objects or wrongly layout them in the figure. One example can be seen as a sandwich below a TV with a spoon to the right of the sandwich, where DALL-E 2 and Stable Diffusion fail to generate all the objects, but Control-GPT can manage to do it. The experiment shows the potential of Control-GPT in handling complex scene generations.
Controllability vs. photo-realism. We notice that there is often a tradeoff between generating photorealistic images versus following the exact layout, especially for out-of-distribution text prompts. As shown in Figure 8, the subfigure (a) is an example that the generated image follows exactly the layout, but this results in some artifacts in the image. While in (b), the photo tends to look realistic but doesn’t follow the sketch well. We notice this phenomenon in Control-GPT, which leads to the majority of the spatial errors in the experiments. Since Control-GPT tries to balance the photo-realism and instruction following, it explains why Control-GPT didn’t fully translate the performance gain of the programmatic sketches (∼ 97%).
8

stop sign to the left of a skis and a skis to the left of a kite
couch above a toilet and a toilet to the right of a clock
an apple above giraffe with a boat to the right of the giraffe
a sandwich below a TV with a spoon to the right of the sandwidch

(a) DALL-E-2

(b) Stable Diffusion

(c) Control-GPT

Figure 7: Image generation with multiple objects. Control-GPT is better than DALL-E 2 and Stable Diffusion in generating complex scenes involving multiple objects. The two baseline model either suffers from object spatial location misinterpretation or fail to generate the corresponding object.

Person Motor
Person Plane

Car
Person Person
Case

Person Handbag
Person Surfboard

Dog
Handbag Tie
Dog

(a) Following Layout w.o. Meaningful Content

(b) Meaningful Content w.o. Following Layout

Figure 8: Controllability vs. photo-realism . One interesting phenomenon is that Control-GPT has a balance of generating images that follows the guideline versus producing images that look photorealistic. (a) Some examples are that the generated image follows exactly the layout but suffers from artifacts. (b) Examples that look photorealistic but doesn’t follow the guideline well.

6 Conclusion
We present a novel method Control-GPT, a controllable text-to-image generation method with the guidance of programmatic sketches generated by GPT-4. We augment the ControlNet architecture by grounding tokens and training it with polygons from the existing dataset. By leveraging GPt-4 for generating TikZ sketch and grounding tokens, our method achieves state-of-the-art performance on benchmarks focusing on the spatial locations of different objects. It also demonstrates great potential in controlling object size/position and generating complex scenes. This has large implications for using text-to-image models in many more situations, such as ones where a greater need for creative and editorial control is needed (for example in arts and other creative applications). The paper is also the first to demonstrate a possible way for joint optimization over different AI models, opening up opportunities in the domain.
7 Limitations & Social Impacts
In this section, we discuss the limitations and social impact of the developed method and the released model and how we can better handle social effects. One major limitation of our method is optimizing the model requires a labeled dataset that consists of polygons. This prevents the model from leveraging large-scale unlabeled datasets. We will seed the potential to utilize unlabeled datasets in the future. As a generative AI agent, one of the potential impacts will always be generating malicious content or automated disinformation.
9

References
[1] Chatgpt, 2022. URL https://openai.com/blog/chatgpt.
[2] Andrew Brock, Jeff Donahue, and Karen Simonyan. Large scale GAN training for high fidelity natural image synthesis. In International Conference on Learning Representations, 2019. URL https://openreview.net/forum?id=B1xsqj09Fm.
[3] Tim Brooks, Aleksander Holynski, and Alexei A. Efros. Instructpix2pix: Learning to follow image editing instructions. In CVPR, 2023.
[4] Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. Sparks of artificial general intelligence: Early experiments with gpt-4. arXiv preprint arXiv:2303.12712, 2023.
[5] Ming Ding, Wendi Zheng, Wenyi Hong, and Jie Tang. Cogview2: Faster and better text-to-image generation via hierarchical transformers. arXiv preprint arXiv:2204.14217, 2022.
[6] Tejas Gokhale, Hamid Palangi, Besmira Nushi, Vibhav Vineet, Eric Horvitz, Ece Kamar, Chitta Baral, and Yezhou Yang. Benchmarking spatial relationships in text-to-image generation. arXiv preprint arXiv:2212.10015, 2022.
[7] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Z. Ghahramani, M. Welling, C. Cortes, N. Lawrence, and K.Q. Weinberger, editors, Advances in Neural Information Processing Systems, volume 27. Curran Associates, Inc., 2014. URL https://proceedings.neurips.cc/paper_files/paper/2014/file/ 5ca3e9b122f61f8f06494c97b1afccf3-Paper.pdf.
[8] Agrim Gupta, Piotr Dollar, and Ross Girshick. Lvis: A dataset for large vocabulary instance segmentation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 5356–5364, 2019.
[9] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Prompt-to-prompt image editing with cross attention control. 2022.
[10] Manuel Jahn, Robin Rombach, and Björn Ommer. High-resolution complex scene synthesis with transformers. arXiv preprint arXiv:2105.06458, 2021.
[11] Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative adversarial networks. In 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 4396–4405, 2019. doi: 10.1109/CVPR.2019.00453.
[12] Yuheng Li, Haotian Liu, Qingyang Wu, Fangzhou Mu, Jianwei Yang, Jianfeng Gao, Chunyuan Li, and Yong Jae Lee. Gligen: Open-set grounded text-to-image generation. arXiv preprint arXiv:2301.07093, 2023.
[13] Zejian Li, Jingyu Wu, Immanuel Koh, Yongchuan Tang, and Lingyun Sun. Image synthesis from layout with locality-aware mask adaption. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 13819–13828, 2021.
[14] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C. Lawrence Zitnick. Microsoft coco: Common objects in context. In David Fleet, Tomas Pajdla, Bernt Schiele, and Tinne Tuytelaars, editors, Computer Vision – ECCV 2014, pages 740–755, Cham, 2014. Springer International Publishing. ISBN 978-3-319-10602-1.
[15] Shuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey Svyatkovskiy, Ambrosio Blanco, Colin Clement, Dawn Drain, Daxin Jiang, Duyu Tang, Ge Li, Lidong Zhou, Linjun Shou, Long Zhou, Michele Tufano, MING GONG, Ming Zhou, Nan Duan, Neel Sundaresan, Shao Kun Deng, Shengyu Fu, and Shujie LIU. Codexglue: A machine learning benchmark dataset for code understanding and generation. In J. Vanschoren and S. Yeung, editors, Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks, volume 1. Curran, 2021. URL https://datasets-benchmarks-proceedings.neurips.cc/paper_files/ paper/2021/file/c16a5320fa475530d9583c34fd356ef5-Paper-round1.pdf.
10

[16] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. arXiv preprint arXiv:2112.10741, 2021.
[17] OpenAI. Gpt-4 technical report, 2023.
[18] Bryan A Plummer, Liwei Wang, Chris M Cervantes, Juan C Caicedo, Julia Hockenmaier, and Svetlana Lazebnik. Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models. In Proceedings of the IEEE international conference on computer vision, pages 2641–2649, 2015.
[19] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In Marina Meila and Tong Zhang, editors, Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pages 8821–8831. PMLR, 18–24 Jul 2021. URL https://proceedings.mlr.press/v139/ramesh21a.html.
[20] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In International Conference on Machine Learning, pages 8821–8831. PMLR, 2021.
[21] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 2022.
[22] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10684–10695, 2022.
[23] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. Advances in Neural Information Processing Systems, 35:36479–36494, 2022.
[24] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion5b: An open large-scale dataset for training next generation image-text models. arXiv preprint arXiv:2210.08402, 2022.
[25] Wei Sun and Tianfu Wu. Image synthesis from reconfigurable layout and style. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 10531–10540, 2019.
[26] Wei Sun and Tianfu Wu. Learning layout and style reconfigurable gans for controllable image synthesis. IEEE transactions on pattern analysis and machine intelligence, 44(9):5070–5087, 2021.
[27] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model. https://github.com/tatsu-lab/stanford_alpaca, 2023.
[28] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.
[29] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, et al. Scaling autoregressive models for content-rich text-to-image generation. arXiv preprint arXiv:2206.10789, 2022.
[30] Lvmin Zhang and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. arXiv preprint arXiv:2302.05543, 2023.
[31] Bo Zhao, Lili Meng, Weidong Yin, and Leonid Sigal. Image generation from layout. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8584–8593, 2019.
11

8 Appendix

8.1 Network Architecture

In this section, we delve into the intricate details of our network architecture, modeled after the ControlNet architecture. In adhering to the design principles of ControlNet, we implement a unique strategy where all the weights associated with Stable Diffusion are ”frozen” or fixed, meaning they do not undergo any changes during the training phase.
But our design doesn’t stop there. We’ve added an innovative component - a controlling branch - that runs parallel to the main network. This supplementary branch is not merely a structural addition; it adds complexity and control to the overall architecture. It allows for intricate manipulations and adjustments that can substantially influence the overall performance and functionality of the network.
By integrating these elements, we believe our architecture balances stability (through freezing Stable Diffusion weights) and adaptability (through the controlling branch), creating a versatile and robust network model.

Table 5: Control-GPT Structure

Network Block Name
SD Encoder Block_1 SD Encoder Block_2 SD Encoder Block_3 SD Encoder Block_4 SD Middle Block

Resolution
64x64 32x32 16x16 8x8 8x8

Number of Blocks
3 3 3 3 1

8.1.1 Bounding Box Embedding
In this section, we elucidate our bounding box embedding process, which aligns with the methodology detailed in the GLIGEN work. The fundamental strategy we adopt involves the use of Fourier embedding to transform the bounding boxes, thereby enabling more nuanced interaction with the network architecture.
To give a holistic view of the process, consider the bounding box. Rather than including extensive details, we have chosen to only incorporate the central position of the object within each bounding box. This results in a set of pairs representing the x and y coordinates for each object’s center denoted as [Xobj1, Yobj1, Xobj2, Yobj2, ..., Xobjk, Yobjk].
Each pair from this set is then individually processed through the Fourier embedding network. This network serves a role analogous to a tokenizer in a language model, transforming each input into a form that can be effectively processed and interpreted by the subsequent layers of our architecture.
Following this Fourier embedding process, we append the resulting vector representations to the object name embeddings. The concatenated vector, representing spatial information and the object identity, is fed into the higher-level attention network. This multi-layered approach ensures that our model fully comprehends and effectively leverages the rich information within the bounding boxes and associated object identities.
8.1.2 Object Name Embedding
In this segment, we illuminate how the object names associated with each bounding box are embedded. To accomplish this, we rely on the tokenizer and language model encoder from the original stable diffusion model. Given its inherent design to interpret and encode text data, this model offers an organic and efficient means of creating text embeddings for the object names.
Following the text embedding, our process doesn’t halt. We move forward to concatenate these text embeddings with two other crucial components - the bounding box embeddings and the image patch embeddings. Each of these elements adds an additional layer of information, integrating spatial data from the bounding box and visual data from the image patch with the textual data from the object name.
12

Once the concatenation process is complete, we have a multi-dimensional vector that encapsulates a comprehensive range of data. This enriched vector representation is then supplied to the attention network. This network is designed to parse this multi-faceted input, emphasizing important features while downplaying less critical information. Through this comprehensive and layered approach, our model ensures a thorough understanding and utilization of the diverse data it is provided with.
8.1.3 Token Fusion
We simply concat all the tokens before feeding them to the ControlNet structure.

Embedding = Concat([lbbox, lname, limage])

(4)

8.2 Dataset Details
In refining our model, we employ the LVIS dataset, a richly annotated subset of the COCO dataset, renowned for its broad array of image categories and detailed labels. This dataset forms the bedrock of our fine-tuning process, as we incorporate all the images from the training and evaluation segments.
The depth of our dataset extends beyond mere images. We also utilize the bounding boxes, object names, and polygons provided within the LVIS dataset. Each component adds a layer of richness and complexity to our data. The bounding boxes provide spatial information, indicating the position and extent of each object within the images. The object names offer categorical data, identifying the type of object encapsulated by each bounding box. The polygons provide even more granular spatial data, detailing the precise shape and orientation of the objects.
By combining these diverse data types – image, spatial, categorical, and shape – we construct a dataset that is not only varied but also highly detailed. This enriched dataset allows us to fine-tune our model more effectively, optimizing its performance across a wide array of scenarios.

8.3 Training Details
For training, we use the default training parameters as ControlNet. Detailed hyperparameters are listed below in Table 6.

Hyperparameter Name
batch size learning rate sd locked only mid control Fourier Embedding Dim Object Embedding Dim

Value
8 1e-5 True False 16 768

Table 6: Control-GPT hyperparameters

8.4 More Examples for Control-GPT
To further illustrate our points, we present additional examples derived from Control-GPT, a modification of GPT-4 designed to enhance precision in control. These examples are visually depicted in Figure 9 and Figure 10 for your perusal.
Interestingly, the TikZ sketch associated with the GPT-4 model doesn’t necessarily offer comprehensive control over the output. This presents a unique opportunity for Control-GPT. In response to this gap, Control-GPT has been devised to strike a fine balance between two pivotal factors: the quality of the generated image and the adherence to the prescribed layout.
By navigating this delicate balance, Control-GPT excels at generating high-quality images that closely follow the given layout specifications. In essence, Control-GPT augments the core functionality of GPT-4, enhancing control without compromising image quality, thereby offering an upgraded solution for more precise and aesthetically pleasing outputs.
13

A bear to the right of a bench
A bird to the left of a bag
A bird to the right of a bottle

A bear above a bench
A bottle above a bench
A bird above a tottle

A bird to the right of a microwave

A bird to the left of a camera

A bread above a microwave

A bicycle to the left of a snowboard

TikZ Sketch

Control-GPT

TikZ Sketch

Control-GPT

Figure 9: More sketch examples Control-GPT in the spatial relation dataset. Control-GPT is being able to generate images very precisely without any missing objects. It also balances the visual quality and layout following to make the image look vivid.

8.5 TikZ Code Snippets
We also show some code snippets in Figure 11, which are entirely generated by GPT-4. We see that the code structure is almost correct with proper comments. The code compiles without any error. This is quite impressive. In addition to that, it captures all the necessary components. This demonstrates GPT-4’s strong capability of drawing TikZ code.
8.6 Vanilla ControlNet Examples
In addition, we have conducted an experiment on directly passing the TikZ images to the vanilla ControlNet trained on segmentation maps. We found that the Vanilla ControlNet is being able to follow the outline of the sketch very well. This is not surprising as it is trained to do this. However, it doesn’t understand the concept of the image well as it almost fails to generate all the objects specified in the prompt. The results in Figure 12 demonstrate some examples. ControlNet rarely generates the correct objects.
14

A mouse below an umbrella

A surfboard below an umbrella

A horse to the left of a sportsball
A dog above a bench

A dog to the left of a laptop
A dog to the left of a remote control

A dog above a remote control

A dog to the left of a keyboard

A dog above a laptop

A dog to the left of a keyboard

TikZ Sketch

Control-GPT

TikZ Sketch

Control-GPT

Figure 10: Control-GPT is able to generate images very precisely without any missing objects in the spatial relation dataset. It also balances the visual quality and layout following to make the image look vivid.

15

\begin{tikzpicture} \path[use as bounding box] (0,0) rectangle (5.12,5.12); % Draw the person \fill[red] (1,1) circle (0.5); \fill[red] (1,0.5) rectangle (1.1,1.5); \fill[red] (1.1,1.5) – (1.5,2.5) – (1.7,1.5) – cycle; \fill[red] (1.1,0.5) – (1.5,-0.5) – (1.7,0.5) – cycle; % Draw the bus \fill[red] (3,1) rectangle (4.5,2.5); \fill[red] (3.25,1.25) circle (0.25); \fill[red] (4.25,1.25) circle (0.25); \end{tikzpicture}
\begin{tikzpicture} \path[use as bounding box] (0,0) rectangle (5.12,5.12); % Truck \fill[red] (0.5,1) rectangle (3,2.5); % Truck body \fill[red] (2.5,2.5) rectangle (3.5,3.5); % Truck cabin \fill[red] (1,1) circle (0.5); % Wheel 1 \fill[red] (2.5,1) circle (0.5); % Wheel 2 \fill[red] (3.5,1) circle (0.5); % Person \fill[red] (4,3) circle (0.5); % Head \fill[red] (3.75,1.5) rectangle (4.25,2.5); % Body \draw[red, line width=2pt] (4,2.5) – (3.5,1); % Leg 1 \draw[red, line width=2pt] (4,2.5) – (4.5,1); % Leg 2 \draw[red, line width=2pt] (4,3) – (3.5,3.5); % Arm 1 \draw[red, line width=2pt] (4,3) – (4.5,3.5); % Arm 2 \end{tikzpicture}
\begin{tikzpicture} \draw[red, fill=red] (1,2) circle (0.25); % head \draw[red] (1,2) – (1,1); % body \draw[red] (1,1.5) – (0.5,1.5); % left arm \draw[red] (1,1.5) – (1.5,1.5); % right arm \draw[red] (1,1) – (0.5,0.5); % left leg \draw[red] (1,1) – (1.5,0.5); % right leg \draw[red, fill=red] (3.5,0.5) – (4.5,0.5) – (4.12,1) – (3.88,1) – cycle; % boat \useasboundingbox (0,0) rectangle (5.12,5.12); \end{tikzpicture}
Figure 11: Examples of TikZ code GPT-4 generates. It almost gets all the syntax correct: the code compiles without any error in LateX.
16

An airplane to the right of a clock
A backpack below a tie

A giraffe below an orange
A cow to the right of an umbrella

A book below a train

A bus above a book

A boat below a lizza

A remote to the left of a boat

A sheep to the right of a snowboard

A sandwich to the left of a horse

TikZ Sketch

ControNet

TikZ Sketch

ControNet

Figure 12: More examples from the vanilla ControlNet trained on segmentation maps. It hardly generates the correct objects, as we see in the figure. Despite that, its ability to follow the outline sketch is pretty good.

17

a frisbee to the left of a
tie
a car above a suitcase
a skis to the right of a vase

a kite below a book
a tv above a surfboard

a carrot to the right of a
pizza

GPT-4

GPT-3.5

Chat-GPT

Figure 13: More sketch examples from GPT-4, GPT-3.5, and ChatGPT. GPT-4 consistently outperforms other models in following text instructions and understanding the spatial relations
8.7 More TikZ Examples
We also plot more examples from different LLMs we tested in Figure 13. GPT-4 demonstrates strong capability in plotting more details in the figure, including all the objects, and getting their spatial location correct. Compared to that, GPT-3.5 and Chat-GPT tend to either miss some objects or don’t use enough details in the figure.

18

