MFR-Net: Multi-faceted Responsive Listening Head Generation via Denoising Diffusion Model

Jin Liu
Institute of Information Engineering, Chinese Academy of Sciences &
School of Cyber Security, University of Chinese Academy of Sciences Beijing, China liujin@iie.ac.cn

Xi Wangâˆ—
Institute of Information Engineering, Chinese Academy of Sciences Beijing, China wangxi1@iie.ac.cn

Xiaomeng Fu
Institute of Information Engineering, Chinese Academy of Sciences &
School of Cyber Security, University of Chinese Academy of Sciences Beijing, China fuxiaomeng@iie.ac.cn

arXiv:2308.16635v1 [cs.CV] 31 Aug 2023

Yesheng Chai
Institute of Information Engineering, Chinese Academy of Sciences &
School of Cyber Security, University of Chinese Academy of Sciences Beijing, China chaiyesheng@iie.ac.cn

Cai Yu
Institute of Information Engineering, Chinese Academy of Sciences &
School of Cyber Security, University of Chinese Academy of Sciences Beijing, China caiyu@iie.ac.cn

Jiao Dai
Institute of Information Engineering, Chinese Academy of Sciences Beijing, China daijiao@iie.ac.cn

Jizhong Han
Institute of Information Engineering, Chinese Academy of Sciences Beijing, China hanjizhong@iie.ac.cn

ABSTRACT
Face-to-face communication is a common scenario including roles of speakers and listeners. Most existing research methods focus on producing speaker videos, while the generation of listener heads remains largely overlooked. Responsive listening head generation is an important task that aims to model face-to-face communication scenarios by generating a listener head video given a speaker video and a listener head image. An ideal generated responsive listening video should respond to the speaker with attitude or viewpoint expressing while maintaining diversity in interaction patterns and accuracy in listener identity information. To achieve this goal, we propose the Multi-Faceted Responsive Listening Head Generation Network (MFR-Net). Specifically, MFR-Net employs the probabilistic denoising diffusion model to predict diverse head pose and expression features. In order to perform multi-faceted response to the speaker video, while maintaining accurate listener identity preservation, we design the Feature Aggregation Module to boost listener identity features and fuse them with other speaker-related features. Finally, a renderer finetuned with identity consistency loss produces the final listening head videos. Our extensive experiments demonstrate that MFR-Net not only achieves multi-faceted
âˆ—Corresponding author.
This work is licensed under a Creative Commons Attribution International 4.0 License.
MM â€™23, October 29-November 3, 2023, Ottawa, ON, Canada Â© 2023 Copyright held by the owner/author(s). ACM ISBN 979-8-4007-0108-5/23/10. https://doi.org/10.1145/3581783.3612123

responses in diversity and speaker identity information but also in attitude and viewpoint expression.
CCS CONCEPTS
â€¢ Computing methodologies â†’ Animation; Computer vision.
KEYWORDS
listening head generation, image synthesis, denoising diffusion model
ACM Reference Format: Jin Liu, Xi Wang, Xiaomeng Fu, Yesheng Chai, Cai Yu, Jiao Dai, and Jizhong Han. 2023. MFR-Net: Multi-faceted Responsive Listening Head Generation via Denoising Diffusion Model. In Proceedings of the 31st ACM International Conference on Multimedia (MM â€™23), October 29-November 3, 2023, Ottawa, ON, Canada. ACM, New York, NY, USA, 10 pages. https://doi.org/10.1145/ 3581783.3612123
1 INTRODUCTION
Face-to-face communication [11, 30] is a common human activity, regularly occurring in various contexts such as between teachers and students, doctors and patients, and employers and employees. During such communication, participants take turns to act as speakers and listeners, engaging in an exchange of information. Typically, speakers articulate their thoughts verbally, while listeners convey their opinions non-verbally through head movements.
There exists extensive research works on speaker modeling, i.e. talking head generation [6, 25], including improving the lip-syn quality [9, 28, 31], achieving free pose control [21, 23, 43, 51] or enhancing the naturalness by adding emotions [17, 18, 41, 44]. In

MM â€™23, October 29-November 3, 2023, Ottawa, ON, Canada

Jin Liu et al.

Listener
Generated Results #1

Speaker Video
+ Positive Attitude

Generated Results #2

+ Positive Attitude

Generated Results #3

+ Neutral Attitude

Generated Results #4

+ Negative Attitude

Figure 1: Example results generated by MFR-Net. Given speaker video, audio, listener identity image and specific attitude label, our method generates natural multi-faceted responsive listening head videos. Results #1 and #2 display diverse results indicating positive attitude (smiling and nodding) while Results #3 and #4 show neutral (calm) and negative (serious) results.

face-to-face communication scenarios, the listenerâ€™s role is also significant as that of the speaker. However, research on listener modeling remains less explored. Actually, the modeling of a listener is distinct from that of a speaker, as the former focuses on response to others while the latter primarily emphasizes lip synchronization. The responsive listening head generation task aims to produce new listener head videos given the speaker talking head video and listener identity head image. Apart from modeling the daily scenario, the technique can also be used in digital avatar generation in Metaverse, customer representatives, robot communication, virtual audience modeling, wherever involves responsive listeners.
Listening is a conditioned action to reflect human behavior according to the principle proposed in social psychology and anthropology [3]. The listener head videos should also contain the listener identity and opinion information at the same time. In light of the aforementioned background, there are multi-faceted requirements for the generation of responsive listener heads. 1) Viewpoint Expression: The generated responsive listener heads should convey certain viewpoints as a corresponding response to the speakerâ€™s head videos.Non-verbal behaviors such as nodding, smiling, frowning, head shaking, or neutral heads are generally used to convey those viewpoints. 2) Speaker Interaction: To achieve dynamic interaction between a speaker and listener in virtual communication, it is important that the pattern of listener motions exhibit a high correlation with the speakerâ€™s head video signal, as well as the speakerâ€™s attitude. The rhythm of the speakerâ€™s audio and the flow movement of their head can influence the action of the listener. Moreover, the attitude of the speaker, whether expressing agreement or disagreement, can prompt corresponding response

actions in the listener, such as nodding, shaking, frowning, or other movements. 3) Responsive Diversity: For a given speaker video, listener identity, and attitude, there exist various natural response listener head videos. The expressive method and range of head movement may differ in every response scenario. In a virtual online meeting, it can be jarring if the avatars of each participant exhibit the same response head reactions, as this could detract from the sense of authenticity and spontaneity. 4) Generation Naturalness: The generated responsive listener heads should be of high image quality and free of any visual artifacts. Additionally, the identity information of the generated videos should match the given listener head image in order to ensure consistency and accuracy.
Recently, Zhou et al. [52] explore this task and collect the audiovisual ViCo dataset containing pairs of speaker and listener videos. To guide the generation of listening heads, they utilize an LSTMbased model to process the input signal and output listener head pose features. Later, PCHG [16] post-process the generated frames with face segmentation model [32] to improve the stability of the background. However, the deterministic nature of their models fails to model the generation diversity, and the direct concatenation fusion between identity and speaker-related feature causes inaccurate identity preserving, leading to facial contour artifacts.
To tackle the aforementioned problems and meet the above multifaceted requirements, we design the Multi-Faceted Responsive listening head generation network (MFR-Net). Based on the denoising diffusion model [15], MFR-Net is designed to predict the speakerâ€™s head pose and expression features. By leveraging the probabilistic nature of the denoising diffusion model, we manage to generate diverse listening head results. Except for the generation diversity, the

MFR-Net: Multi-faceted Responsive Listening Head Generation via Denoising Diffusion Model

MM â€™23, October 29-November 3, 2023, Ottawa, ON, Canada

generated listening head videos should also interact with speakers with viewpoint or attitude expression and keep accurate listener identity information. To achieve the multi-faceted response, we propose the Feature Aggregation Module to embed the constraint conditions including speaker-related features, listenerâ€™s attitude and listener identity information. The proposed module is applied to each denoising and diffusion process to predict the noise term. In this way, multi-faceted constraints can be integrated into the diverse generated results. Finally, the renderer trained with identity consistency loss is adopted to generate photo-realistic listener head images. As shown in Fig. 1, MFR-Net manages to achieve natural and diverse multi-faceted listening head generation with different listener attitudes and accurate listener identity preservation.
Our contributions are summarized as follows: 1) We propose MFR-Net, the first diffusion-based model for solving the task of generating responsive listener heads, and produces diverse and high-quality listener head videos. 2) The Feature Aggregation Module is designed to integrate speaker-related features, listener attitude and head image, leading to natural interaction with viewpoint expression and accurate listener identity information. 3) The stateof-the-art performance is achieved on the ViCo dataset in terms of visual quality, identity-preserving and generation diversity.
2 RELATED WORK
2.1 Responsive Listening Head Generation
Responsive Listening Head Generation aims to produce a head video of the listener, given a corresponding talking-head video of the speaker and face image of the listener. Zhou et al. [52] first propose this task and construct the ViCo dataset for evaluation. The proposed baseline utilizes LSTM to process the streaming input of visual and audio information of the speaker and produces facial 3DMM coefficients of the listener. Later, Huang et al. [16] utilizes pre-trained foreground-background segmentation model U2Net [33] to fuse and improve the background of generated results. However, the above method could merely generate solitary listening head videos given certain speaker talking videos, while MFR-Net manages to produce diverse listening head videos.
2.2 Audio-Driven Head Synthesis
Audio-driven head synthesis produces lip-sync talking head videos given source faces and driving speech signals. Some previous methods [12, 13, 19, 47] generated talking head videos of specific identity used in the training process. Guo et al. [13] utilize NeRF-based [24] network to model the head and torso separately and combine two generated parts. Zhang et al. [47] perform 3D face reconstruction over each frame in the video and generate new expression coefficients to control mouth shape. Though the aforementioned techniques preserve accurate source identity, they necessitate requiring high-quality videos of each subject for minutes to hours and only produce a limited range of identities, resulting in significant limitations on their applicability and generalization.
Therefore, some recent methods [1, 7, 8, 31, 50] try to relieve the identity restriction and explore to generate any source subject. Prajwal et al. [31] adopt a pre-trained lip-sync discriminator to improve the lip-sync quality of generated talking head videos. Alghamdi et al. [1] model each frame into the latent space of StyleGAN, map

audio signals into displacements in the space and generate final talking images. Though the above methods have no identity mismatch problem since they merely edit the mouth areas, they generate unnatural talking heads given the still facial parts.
Later, to improve the diversity and naturalness of talking heads, current methods [21, 23, 45, 48, 49, 51] explore to generate videos with head pose changes. Some methods [21, 23, 51] rely on auxiliary pose video to provide explicit guidance of pose sequences, while others [45, 48, 49] explore to infer pose sequence from the audio signal. Among them, OPT [23] try to solve the identity mismatch problem by disentangling identity and content feature from the audio signal to eliminate the effect of audio identity. To improve the diversity of generated results, some works [22, 48] utilize the VAE structure to map the audio signal into diverse pose sequences. However, MFR-Net shows diversity not only in head poses, but also in facial expression parts as well.
2.3 Diffusion Generative Models
Denoising diffusion probabilistic model [15] is first proposed for unconditional image generation. Due to the stochastic property of the initialized noise in the reversion process, it manages to generate images of great diversity and soon becomes popular in a variety of creativity-oriented generation tasks. To name a few, GLIDE [26] introduces text conditions and proves the effectiveness of classifierfree guidance. DALLE-2 [35] further generate semantics-consistent images conditioned on CLIP [34] guidance. Later, Latent Diffusion Models [37] perform the diffusion process in latent space rather than pixel space to improve the efficiency. The above works could generate diverse and natural images. However, creative as they are, diffusion models adopted in such creativity-oriented tasks are not stable enough for responsible listening head generation, which aims to generate a natural video of a fixed listener while keeping identity information unchanged. Our proposed MFR-Net turns to generate facial coefficient features and impose explicit identity restrictions over the generation process to solve the above problems.
3 METHOD
Given speaker video clip ğ‘‰ğ‘  containing visual and audio information, listener head image ğ¼ğ‘™ and attitude label, MFR-Net aims to produce multi-faceted responsive listening head videos. An overview of MFR-Net is shown in Fig. 2, which contains four major components. The diffusion process and denoising process act as training and inference modules, respectively. The Feature Aggregation Module receives the inputs containing speaker video ğ‘‰ğ‘  , listener head image ğ¼ğ‘™ , listener attitude and last intermediate state in the denoising or diffusion process, then adopts attention-based blocks to predict noise of current step ğ‘¡. The output of the denoising process is listener head pose and expression features, which will be fed into the generator to generate new listener head videos.
3.1 Denoising Diffusion Model for Feature Generation
Previous responsive listener head generation methods adopt deterministic LSTM-based modules to predict listener head features.

MM â€™23, October 29-November 3, 2023, Ottawa, ON, Canada

Jin Liu et al.

Denoising Process

Listener ğ‘°ğ‘°ğ’ğ’

ğ‘­ğ‘­ğ’ğ’

Identity Feature

Feature Aggregation Module

ğ‘¥ğ‘¥ğ‘‡ğ‘‡ ~ ğ’©ğ’©(0, ğ¼ğ¼)
ğ‘¥ğ‘¥ğ‘‡ğ‘‡ â€¦

SPADE
ğ‘¥ğ‘¥ğ‘¡ğ‘¡

Feature Enhancing

+

Feature Fusion

+

+ Feed
Forward

ğ‘ğ‘ğœƒğœƒ(ğ‘¥ğ‘¥ğ‘¡ğ‘¡âˆ’1|ğ‘¥ğ‘¥ğ‘¡ğ‘¡) ğ‘¥ğ‘¥ğ‘¡ğ‘¡âˆ’1

Generated Listener Feature {ğ‘­ï¿½ğ‘­ğ’ğ’‘ğ’ğ’‘, ğ‘­ï¿½ğ‘­ğ’ğ’†ğ’ğ’†}
â€¦ ğ‘¥ğ‘¥0

Speaker Clip ğ‘½ğ‘½ğ’”ğ’”

ğ‘­ğ‘­ğ’”ğ’‘ğ’”ğ’‘
Pose Feature

ğ‘­ğ‘­ğ’”ğ’†ğ’”ğ’†
Expression Feature

ğ‘­ğ‘­ğ’”ğ’‚ğ’”ğ’‚
Audio Feature

C

ğ‘­ğ‘­ğ’ğ’‚ğ’ğ’‚ğ’‚ğ’‚ğ’‚ğ’‚

Listener Attitude

ïŠ

ï‹

ïŒ

+

Output

C Concatenate

F Feature

+ Element-wise Addition ğ“›ğ“› Loss function

SPADE

Spatial Adaptive Normalization

G Generator

Diffusion Process

ğœ–ğœ– ~ ğ’©ğ’©(0, ğ¼ğ¼) Noise

{ğ‘­ğ‘­ğ’ğ’, ğ‘­ğ‘­ğ’ğ’‚ğ’ğ’‚ğ’‚ğ’‚ğ’‚ğ’‚, ğ‘­ğ‘­ğ’”ğ’‘ğ’”ğ’‘, ğ‘­ğ‘­ğ’”ğ’†ğ’”ğ’†, ğ‘­ğ‘­ğ’”ğ’‚ğ’”ğ’‚}

Renderer

Listener Feature {ğ‘­ğ‘­ğ’ğ’‘ğ’ğ’‘, ğ‘­ğ‘­ğ’ğ’”ğ’ğ’”}
ğ‘¥ğ‘¥0

ğ‘ğ‘(ğ‘¥ğ‘¥ğ‘¡ğ‘¡|ğ‘¥ğ‘¥0)

Timestep ğ’•ğ’•

ğ‘¥ğ‘¥ğ‘¡ğ‘¡

â€¦

Listener

Feature

â„’ğ‘›ğ‘›ğ‘›ğ‘›ğ‘›ğ‘›ğ‘›ğ‘›ğ‘›ğ‘›

ğ‘°ğ‘°ğ’ğ’

Aggregation ğğğœ½ğœ½(ğ’™ğ’™ğ’•ğ’•, ğ’•ğ’•, ğ’Šğ’Šğ’Šğ’Šğ’Šğ’Šğ’Šğ’Šğ’Šğ’Š)

G

Module

{ğ‘­ï¿½ğ‘­ğ’ğ’‘ğ’ğ’‘, ğ‘­ï¿½ğ‘­ğ’ğ’†ğ’ğ’†}

â„’ğ‘–ğ‘–ğ‘–ğ‘–

ğ‘°ï¿½ğ‘°ğ’ğ’

Figure 2: Overview of MFR-Net. MFR-Net adopts the denoising diffusion model to predict new listener features, which are fed

into the generator along with listener image ğ¼ğ‘™ to produce responsive listening heads. The Feature Aggregation Module is

designed

to

predict

the

noise

used

in

the

denoising

diffusion

model,

given

speaker

clip

ğ‘‰ğ‘ 

,

listener

image

ğ¼ğ‘™

,

listener

attitude

ğ¹ ğ‘ğ‘¡ğ‘¡
ğ‘™

,

intermediate feature ğ‘¥ğ‘¡ and time step ğ‘¡. The brown and black arrows indicate the training and inference process, respectively.

They lack response diversity, which is the key factor in natural faceto-face interaction. To tackle the problem, we build our generation pipeline based on probabilistic denoising diffusion models.
The denoising diffusion model [15] is adopted to denoise a Gaussian noise step-by-step and finally generate the listener head pose and expression features. The form is as follows: ğ‘ğœƒ (x0) := âˆ« ğ‘ğœƒ (x0:ğ‘‡ ) ğ‘‘x1:ğ‘‡ , where ğ‘¥0 is the real data of listener head pose and expression features and ğ‘¥1, ..., ğ‘¥ğ‘‡ are the latent data of the intermediate state. Specifically, the joint distribution ğ‘ğœƒ (x0:ğ‘‡ ) is called the denoising process, which is defined as a Markov chain with learned Gaussian transitions starting at ğ‘ (xğ‘‡ ) = N (xğ‘‡ ; 0, I):

ğ‘‡

ğ‘ğœƒ (x0:ğ‘‡ ) := ğ‘ (xğ‘‡ ) ğ‘ğœƒ (xğ‘¡ âˆ’1 | xğ‘¡ ) ,
ğ‘¡ =1

(1)

ğ‘ğœƒ (xğ‘¡ âˆ’1 | xğ‘¡ ) := N (xğ‘¡ âˆ’1; ğœ‡ğœƒ (xğ‘¡ , ğ‘¡ ) , Î£ğœƒ (xğ‘¡ , ğ‘¡ )) .

Correspondingly, the diffusion process is the approximate poste-

rior ğ‘(ğ‘¥1:ğ‘‡ |ğ‘¥0), which is also designed to fix a Markov chain that gradually adds Gaussian noise to the data according to a variance

schedule ğ›½1, ..., ğ›½ğ‘‡ :

ğ‘‡

ğ‘ (x1:ğ‘‡ | x0) := ğ‘ (xğ‘¡ | xğ‘¡ âˆ’1) ,

ğ‘¡ =1

(2)

ğ‘ (xğ‘¡ | xğ‘¡ âˆ’1) := N

xğ‘¡

;

âˆšï¸ 1

âˆ’

ğ›½ğ‘¡

xğ‘¡

âˆ’1,

ğ›½ğ‘¡

I

.

As shown in the diffusion part in Fig. 2, during training, noise ğœ–

and uniformly sampled time step ğ‘¡ are utilized to generate latent

features. Given that the diffusion process admits sampling ğ‘¥ğ‘¡ at an

arbitrary time step ğ‘¡, instead of repeatedly adding noises to each

intermediate state, tâˆšhe diffusion process can be further denoted as ğ‘ (xğ‘¡ | x0) = N xğ‘¡ ; ğ›¼Â¯ğ‘¡ x0, (1 âˆ’ ğ›¼Â¯ğ‘¡ ) I , where ğ›¼ğ‘¡ := 1 âˆ’ ğ›½ğ‘¡ and ğ›¼Â¯ğ‘¡ :=

ğ‘¡ ğ‘ 

=1

ğ›¼ğ‘ 

.

To

improve

the

performance,

following

iDDPM

[27],

we

choose to predict noise term ğœ– instead of predicting latent feature. To

integrate each input feature and achieve multi-faceted generation,

we design the Feature Aggregation Module (Sec. 3.2) instead of

using traditional U-Net to predict the noise term. Hence, the loss

term used to optimize the model parameters is as follows:

Lnoise = ğ¸ğ‘¡,ğ‘¥0,ğœ– âˆ¥ğœ– âˆ’ ğœ–ğœƒ (ğ‘¥ğ‘¡ , ğ‘¡, ğ‘–ğ‘›ğ‘ğ‘¢ğ‘¡ ) âˆ¥2 .

(3)

Furthermore, as shown in Equation 1, to generate new samples through the denoising process, ğœ‡ğœƒ and Î£ğœƒ are demanded. Following Ho et al. [15], Î£ğœƒ is set to a constant number and ğœ‡ğœƒ is:

ğœ‡ğœƒ

(xğ‘¡ , ğ‘¡, ğ‘–ğ‘›ğ‘ğ‘¢ğ‘¡)

=

1 âˆš

ğ›¼ğ‘¡

xğ‘¡

âˆ’

1 âˆš

âˆ’

ğ›¼ğ‘¡

1 âˆ’ ğ›¼Â¯ğ‘¡

ğœ–ğœƒ

(xğ‘¡ , ğ‘¡, ğ‘–ğ‘›ğ‘ğ‘¢ğ‘¡)

.

(4)

In this way, we manage to denoise the sampled noise step-bystep and finally generate new features, which are conditioned on the given input, as shown in the pink part in Fig. 2.

MFR-Net: Multi-faceted Responsive Listening Head Generation via Denoising Diffusion Model

MM â€™23, October 29-November 3, 2023, Ottawa, ON, Canada

3.2 Feature Aggregation Module
In Sec. 3.1, we show the denoising diffusion model as the listener feature generator, then we will illustrate the neural network for predicting the noise term ğœ–ğœƒ (ğ‘¥ğ‘¡ , ğ‘¡, ğ‘–ğ‘›ğ‘ğ‘¢ğ‘¡). Specifically, the input includes identity features extracted from the face recognition model, pose and expression features reconstructed from 3DMM, audio features, listener attitudes and latent state features. Unlike traditional denoising diffusion models [15, 35, 37, 40] using the U-Net [38] as basic module, to deal with input audio and pose features of variable length, we design our Feature Aggregation Module based on Transformer [42] technique.
Furthermore, previous responsive listener head generation methods directly concatenate listener head features and other driving features, giving the same focus on features of different importance. In this way, responsive listener heads with inaccurate facial contour and the identity mismatch problem are generated. To alleviate the problem, we hope to enhance the speaker information into driving features by aggregating its most compatible identity features.
Firstly, our objective is to enhance the input features and obtain a comprehensive understanding of the existing identity feature, which guides the generation of the noise term. Taking inspiration from the traditional face swapping work FaceShifter [20], we design our approach by incorporating the identity information into the latent feature map using the SPADE-like [29] module. This integration ensures that the explicit identity information is embedded throughout the entire generation process, thereby effectively preserving the relevant information. Then we draw inspiration from the implementation of Transformer [42] and utilize the multi-head self-attention module. Specifically, details of this process can be formulated as follows:
Attention(ğ‘„, ğ¾, ğ‘‰ ) = Softmax ğ‘„ğ‘Šğ‘ âˆš(ğ¾ğ‘Šğ‘˜ )ğ‘‡ ğ‘‰ğ‘Šğ‘£, (5) ğ¶

where ğ‘Šğ‘,ğ‘Šğ‘˜,ğ‘Šğ‘£ are projection parameters and ğ‘„, ğ¾, ğ‘‰ are the

query, key, and value respectively. During feature enhancing, we

employ different projection weights on the combination of the

identity feature and latent feature.

Secondly, to fusion the listener identity information into speaker

driving features of another identity, we adopt the feature fusion

module to build the correlation between input processed identity

features and driving speaker features. The detailed formulation

is the same as Equation 5 except that the key and value are from

features

from

speaker

clip

ğ‘‰ğ‘  ,

listener

attitude

ğ¹ ğ‘ğ‘¡ğ‘¡
ğ‘™

and

time

step

ğ‘¡.

Finally, the feed-forward module is adopted to predict the noise

term. Here we omit the residual connection and LayerNorm [2]

operation in each formula for simplification. The loss term in Equa-

tion 3 is utilized to train the module. Thanks to the Feature Aggre-

gation Module, the listener identity information is enhanced and

injected into each driving feature, thus helping to generate accurate

listener features in the denoising process.

3.3 Generator
To improve the inter-frame coherence, the listener head pose features are obtained clip by clip with a fixed window and stride length through the above module. After obtaining the generated listener features, we adopt the state-of-the-art face reenactment

model PIRenderer [36] to produce new listener heads. To further alleviate the identity mismatch problem, we re-train the model and add another identity restriction loss apart from the original perceptual loss, style loss and GAN loss.

Lğ‘–ğ‘‘ğ‘’ğ‘›ğ‘¡ğ‘–ğ‘¡ ğ‘¦ = ğ‘‰ (ğ¼Ëœğ‘¡ ) âˆ’ ğ‘‰ (ğ¼ğ‘¡ ) 1 ,

(6)

where ğ‘‰ denotes the VGGFace [5] model to extract identity features.

In this way, given generated listener pose and expression feature

{ğ¹Ëœğ‘ ,
ğ‘™

ğ¹Ëœğ‘’
ğ‘™

}

and

listener

head

image

ğ¼ğ‘™

,

new

responsive

listener

head

image ğ¼Ëœğ‘™ is generated.

4 EXPERIMENT
4.1 Experimental Settings
4.1.1 Dataset. Our method is evaluated both quantitatively and qualitatively on the ViCo dataset [52], which is uniquely suited to our task. This dataset features 483 video clips capturing face-to-face interactions between two realistic subjects in a natural environment, with a total of over 0.1 million frames. Specifically, it includes the identities of 76 listeners and 67 speakers, and each response is manually annotated as positive, neutral, or negative attitude. As the only audio-visual dataset of its kind, ViCo provides an ideal benchmark for evaluating our approach.
4.1.2 Comparison Methods. Two responsive listener head generation methods are adopted as comparing methods: ViCo [52] adopts the LSTM-based Sequential Decoder to predict the pose and expression features of the listener subject and renders them into new responsive heads. PCHG [16] post-process the generated videos with a segmentation model to improve the stability of the background. Each method is trained on the training set Dğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘› of ViCo, and further evaluated on the test set Dğ‘¡ğ‘’ğ‘ ğ‘¡ and out-of-domain set Dğ‘œğ‘œğ‘‘ . Specifically, all identities in Dğ‘¡ğ‘’ğ‘ ğ‘¡ have appeared in Dğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘› while identities in Dğ‘œğ‘œğ‘‘ have no overlap with ones in Dğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›.
4.1.3 Implementation Details. The face video frames are cropped to 256 Ã— 256 size at 30 FPS and the audio signals are extracted into 45-dimensional acoustic features, including 14-dim mel-frequency cepstral coefficients (MFCC), 28-dim MFCC-Delta, energy, Zero Crossing Rate and loudness. The listener attitude information is denoted as the one-hot label. The window length of the speaker clip is set to be 40 frames with a sliding window length of 20 frames. As for model details, we utilize multi-head attention with 8 heads and 4 layers. The identity feature is extracted from the face recognition model [5], while the pose and expression features are from the angles and translation coefficients of face 3DMM reconstruction operation [10] on the frames in each video. All experiments use an NVIDIA V100 GPU with 32 GB memory.

4.2 Quantitative Evaluation
4.2.1 Evaluation Metrics. In order to evaluate the precision of the generated speaker pose and expression features, we adopt the evaluation methodology utilized by ViCo [52], which involves measuring the ğ¿1 distance between the generated features and their corresponding ground-truth features (FD). These features are extracted from 3D facial reconstruction data, where the angle and

MM â€™23, October 29-November 3, 2023, Ottawa, ON, Canada
Speaker Video
Ground Truth

Jin Liu et al.

ViCo

PCHG

Ours

Figure 3: Qualitative comparisons with other state-of-the-art methods conditioned by the same listener identity (the second row of each group) and the same attitude (left: positive listener in Dğ‘œğ‘œğ‘‘ , right: positive listener in Dğ‘¡ğ‘’ğ‘ ğ‘¡ ).
Table 1: The Feature Distance (Ã—100) of different listening head generation methods. Each cell in the table represents the feature distance of angle/expression/translation coefficients respectively. Lower is better and the bold denotes the best result.

Method Testset

ViCO [52]

Dtest Dood

PCHG [16]

Dtest Dood

00013O:u0r0s08 00037: 00 01

10 08

13D1t4est 11D1o3od

Positive

Neutral

Negative

Average

angle exp trans angle exp trans angle exp trans angle exp trans

6.79 15.37 6.48 8.79 13.61 6.68 12.45 16.98 6.35 7.79 15.04 6.52 9.72 24.89 9.51 6.33 23.51 8.95 8.54 18.99 5.81 8.23 22.83 8.32

13.73 15.36 6.94 12.32 15.35 5.58 13.48 23.72 8.85 13.37 19.98 7.74 17.93 19.12 8.27 18.32 17.66 9.69 17.47 20.86 17.95 18.00 18.82 8.76

5.36 13.73 5.94 5.35 12.32 4.58 11.78 13.46 5.48 6.82 13.37 6.02 9.03 13.72 6.29 6.27 12.96 4.77 7.77 15.51 5.78 8.12 14.70 6.37

Table 2: Quantitative comparisons on image-level metrics with state-of-the-art methods on ViCo dataset. The bold and underlined notations represents the Top-2 results.

Method SSIM â†‘ CPBD â†‘ PSNRâ†‘ FID â†“ CSIM â†“ Diversity â†‘

ViCO [52] 0.56

0.11 17.36 27.74 0.23

0.16

PCHG [16] 0.58

0.16 18.51 21.35 0.25

0.17

Ours

0.59 0.18 17.82 20.08 0.18

0.28

translation (trans) feature track changes in head pose, while the expression (exp) feature captures variations in facial movements.
To perform a comprehensive evaluation of the video-level performance, we adopt Frâ€™echet Inception Distance (FID) [14], Structural Similarity (SSIM) [46], Peak Signal-to-Noise Ratio (PSNR), and Cumulative Probability of Blur Detection (CPBD) [4]. Additionally, to assess the quality of identity preservation, we utilize cosine similarity (CSIM) between identity features extracted from Vggface2 [5] on generated and ground truth images. Furthermore, in order to validate the diversity of the generated head motions, we compute the standard deviation of head motion feature embeddings extracted from the generated frames using Hopenet [39], which follows the methodology of SadTalker [48].

4.2.2 Evaluation Results. Table 1 presents the quantitative comparison results for both Dğ‘¡ğ‘’ğ‘ ğ‘¡ and Dğ‘œğ‘œğ‘‘ subsets of the ViCo dataset, with evaluations performed on head pose features including angle, expression, and translation. It is worth noting that Dğ‘¡ğ‘’ğ‘ ğ‘¡ shares listener identity overlap with the training set (Dğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›), while Dğ‘œğ‘œğ‘‘ does not. Results are presented for three different attitudes, as well as their average values.
MFR-Net achieves superior performance in head movements and facial expressions across all three attitudes. On average evaluation using the Dğ‘¡ğ‘’ğ‘ ğ‘¡ subset, angle, expression, and translation features show improvements of 0.97, 1.67, and 0.5 respectively. Notably, when generating unseen identities in Dğ‘œğ‘œğ‘‘ , the performance of other methods declines significantly compared to results obtained

MFR-Net: Multi-faceted Responsive Listening Head Generation via Denoising Diffusion Model

MM â€™23, October 29-November 3, 2023, Ottawa, ON, Canada

Table 3: User study results on ViCo dataset.

Method

Overall Motion Identity Attitude Naturalness Diversity Preserving Matching

ViCO [52] PCHG [16]
Ours

18.3% 36.6% 49.0%

11.7% 25.7% 62.7%

13.3% 25.0% 61.7%

29.7% 33.7% 36.7%

Listener Identity

Speaker Video
S Ours w/o
DIFF
L Ours w/o
FAM

on Dğ‘¡ğ‘’ğ‘ ğ‘¡ . In contrast, MFR-Net maintains relatively competitive

performance, demonstrating its exceptional ability to generalize on unseen identities. These outstanding results are attributable to the

Ours w/o ğ“›ğ“›ğ’Šğ’Šğ’Šğ’Šğ’Šğ’Šğ’Šğ’Šğ’Šğ’Šğ’Šğ’Šğ’Šğ’Šğ’Šğ’Š

proposed Feature Aggregation Module, which enhances and fuses

identity information with speaker-related features.

Table 2 presents image-level metrics for both Dğ‘œğ‘œğ‘‘ and Dğ‘¡ğ‘’ğ‘ ğ‘¡ .

Ours

The results demonstrate that MFR-Net outperforms other methods

in terms of CSIM and Diversity scores, indicating superior preser- Figure 4: Qualitative results of ablation study on denoising

vation of identity information and generation of diverse images. diffusion model (DIFF), Feature Aggregation Module (FDM)

Moreover, our method achieves competitive results with respect

and the identity consistency loss Lğ‘–ğ‘‘ğ‘’ğ‘›ğ‘¡ğ‘–ğ‘¡ ğ‘¦.

to image quality. Although PCHG achieves a slightly higher PSNR

score due to its complicated post-processing on background pixels, MFR-Net generates images with accurate head pose features and

Table 4: Ablation study for each proposed component in MFRNet tested on both Dğ‘œğ‘œğ‘‘ and Dğ‘¡ğ‘’ğ‘ ğ‘¡ of ViCo dataset.

natural-looking diverse interaction patterns.

Method angle â†“ exp â†“ trans â†“ CSIM â†“ Diversity â†‘

4.3 Qualitative Evaluation
In this section, we present the qualitative results from the generated responsive listening head frames using each method. The results are depicted in Fig. 3. Our findings reveal that MFR-Net offers a reasonable response that may differ from the ground-truth, yet remains coherent. In comparison, other methods such as ViCo fail to maintain accurate facial contours and generate visible artifacts, while PCHG produces stiff response head videos that lack interactive features with the speaker video. Conversely, our approach ensures the preservation of accurate identity information with no visible artifacts. Additionally, the generated videos appear more visually plausible with natural and synchronized head motions and corresponding attitudes. For a detailed video-format comparison, please refer to the supplementary material.
4.4 User Study
We conduct user studies to evaluate the performance of all methods. We randomly choose 10 speaker videos and 10 listeners. For each cross-combination, three responsive videos of each attitude are generated. This process results in 300 generated videos for each method. To assess the quality, we asked 10 participants to watch the videos and choose the best method based on overall naturalness, motion diversity, identity preservation, and attitude matching quality. The results of the study are presented in Table 3. MFR-Net outperformed all other methods in all aspects, especially with regard to motion diversity and identity preservation. These findings indicate the superiority of our proposed Feature Aggregation Module and the effectiveness of our model.
4.5 Further Analysis
4.5.1 Ablation Study. To assess the effectiveness of the designed components in MFR-Net, we conduct the ablation study on the denoising diffusion model (DIFF), the Feature Aggregation Module

w/o DIFF

7.48 14.89 6.39 0.21

0.15

w/o FAM

7.52 18.27 7.49 0.27

0.26

w/o Lğ‘–ğ‘‘ğ‘’ğ‘›ğ‘¡ğ‘–ğ‘¡ ğ‘¦

-

-

- 0.20

0.28

Ours

7.47 14.04 6.20 0.18

0.28

(FAM) and whether to utilize the identity loss Lğ‘–ğ‘‘ğ‘’ğ‘›ğ‘¡ğ‘–ğ‘¡ ğ‘¦ in the renderer. Specifically, we compare our method without DIFF, which employs a simple LSTM-based model as the backbone for predicting noise terms, and our method without FAM, which concatenates all features directly and inputs them into the diffusion model. The results of both the qualitative and quantitative analyses are presented in Fig. 4 and Table 4, respectively.
The experimental results demonstrate that incorporating the denoising diffusion model into MFR-Net leads to a significant improvement in generation diversity, owing to its probabilistic nature, while also ensuring the accurate synthesis of head features. Moreover, the Feature Aggregation Module and the identity consistency loss effectively preserve the precise listener identity information. The image-level results also attest to the effectiveness of our approach. Notably, when generating previously unseen identities in the out-of-distribution dataset Dğ‘œğ‘œğ‘‘ , the FAM enhances the identity information and fuses it with other speaker-related features, resulting in natural-looking listening head videos. It is worth highlighting that this increased diversity in generated responses does not come at the cost of reduced accuracy in head feature synthesis.
4.5.2 Interaction Patterns. As highlighted in Section 1, generating responsive listening heads that can facilitate interaction is critical. To this end, we present several typical patterns generated by MFR-Net, as illustrated in Fig. 5. In the upper pairs, we observe that when listeners intend to show agreement, they tend to nod their heads or smile. Similarly, the lower pairs demonstrate how MFR-Net can model disagreement by shaking heads or expressing

MM â€™23, October 29-November 3, 2023, Ottawa, ON, Canada

Jin Liu et al.

Speaker Video
Listening Video

Listener Identity

Negative Attitude
Neutral Attitude

Speaker Video

Listening Video

Figure 5: Visual interaction patterns in MFR-Net. The upper

pairs show nodding and smiling for agreement, while the

lower pairs display shaking for disagreement and a neutral

9pUaMtBtistfsuJhdoe_.000022_000032.1 smileâ€”â€”nod
Ew2z_sYABE0_000063_000083.0 smileâ€”â€”smile rSM89vRNq4Y_001469_001473.1 disagreeï¼ˆè§†é¢‘æ•ˆæœè¾ƒ å·®ï¼‰
yTInFHb_o1Q_000516_000551.0 å¹³é™è†å¬ï¼ˆåŸºæœ¬ä¸åŠ¨ï¼‰

Speaker Video

Listener Identity

Listening Results #1

Listening Results #2

Figure 6: Visual results of diverse generation. To convey a negative attitude, frowning or serious emotion can be produced by MFR-Net.

neutrality through stationary head movements. These results indicate that MFR-Net is capable of generating diverse responses that can effectively facilitate interaction.
4.5.3 Generation Diversity. The denoising diffusion model within MFR-Net exhibits a probabilistic property that enables the generation of a diverse range of responsive listening heads, as illustrated in Fig. 6. Instead of producing entirely random output, MFR-Net generates varied results based on the listener identity image, speaker video, and attitude label. Depicted in Fig. 6, MFR-Net produces a frowning expression or a serious demeanor to convey negative attitudes. By leveraging this probabilistic mechanism, MFR-Net achieves greater flexibility and effectiveness in generating responses that accurately reflect the given conditions.
4.5.4 Attitude Analysis. To validate the response on attitude label, we show results generated by MFR-Net conditioned by the same speaker video and listener but different attitude labels, as shown in Fig. 7, where two distinct categories of output are displayed. Our analysis shows that the facial expressions and head motions generated by MFR-Net are highly expressive and distinguishable across various attitude labels, indicating the modelâ€™s ability to produce multi-faceted responses. These findings contribute to a growing body of evidence supporting the efficacy of utilizing MFR-Net for generating natural responses with respect to human attitudes.

Listener Identity

Positive Attitude
Neutral Attitude

Figure 7: Visual results generated by MFR-Net conditioned by same speaker video and listener but different attitudes.

4.6 Limitation
While MFR-Net shows promising results in generating realistic and diverse listening head videos given a speaker video and listener head image, there are still some limitations that need to be addressed. One of the limitations is that 3DMMs do not model the variation of eyes and teeth, which may cause difficulties in synthesizing the finer details of teeth. Additionally, we only consider the speaker frames and audio signal without taking into account the semantic information contained in speech, such as speaker emotions and viewpoints. In future works, we plan to incorporate these factors to build a more realistic and highly interpretable system.
5 ETHICAL CONSIDERATIONS
MFR-Net is designed for modeling face-to-face communication scenarios and can be potentially utilized in world-positive use cases and applications, like digital avatar conversation and virtual online meetings. In case of misuse of the proposed method, we strongly support all relevant safeguarding measures against such malicious applications. We believe the proper usage of this technique will enhance the development of artificial intelligence research and relevant multimedia applications.
6 CONCLUSION
This paper proposes a novel method for generating responsive listening head videos. Our proposed MFR-Net utilizes the probabilistic denoising diffusion model to predict the listener head pose and expression features, thereby generating diverse and natural results. To achieve high-quality outputs with accurate response to speaker video while expressing certain attitude and preserving the listener identity, the Feature Aggregation Module is introduced, which enhances and fuses the multi-faceted responsive features. The proposed method is evaluated both quantitatively and qualitatively, and the experimental results demonstrate its superiority in generating precise and diverse listener head responses.
ACKNOWLEDGEMENT
This research is supported in part by the National Key Research and Development Program of China (2020AAA0140000), and the National Natural Science Foundation of China (No. 61702502).

MFR-Net: Multi-faceted Responsive Listening Head Generation via Denoising Diffusion Model

MM â€™23, October 29-November 3, 2023, Ottawa, ON, Canada

REFERENCES
[1] Mohammed M Alghamdi, He Wang, Andrew J Bulpitt, and David C Hogg. 2022. Talking Head from Speech Audio using a Pre-trained Image Generator. In Proceedings of the 30th ACM International Conference on Multimedia. 5228â€“5236.
[2] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. 2016. Layer normalization. arXiv preprint arXiv:1607.06450 (2016).
[3] Larry L Barker. 1971. Listening Behavior. (1971). [4] P Bohr, Rupali Gargote, Rupali Vhorkate, RU Yawle, and VK Bairagi. 2013. A
no reference image blur detection using cumulative probability blur detection (CPBD) metric. International Journal of Science and Modern Engineering 1, 5 (2013). [5] Qiong Cao, Li Shen, Weidi Xie, Omkar M Parkhi, and Andrew Zisserman. 2018. Vggface2: A dataset for recognising faces across pose and age. In 2018 13th IEEE international conference on automatic face & gesture recognition (FG 2018). IEEE, 67â€“74. [6] Lele Chen, Guofeng Cui, Ziyi Kou, Haitian Zheng, and Chenliang Xu. 2020. What comprises a good talking-head video generation?: A survey and benchmark. arXiv preprint arXiv:2005.03201 (2020). [7] Lele Chen, Guofeng Cui, Celong Liu, Zhong Li, Ziyi Kou, Yi Xu, and Chenliang Xu. 2020. Talking-head generation with rhythmic head motion. In European Conference on Computer Vision. Springer, 35â€“51. [8] Lele Chen, Ross K Maddox, Zhiyao Duan, and Chenliang Xu. 2019. Hierarchical cross-modal talking face generation with dynamic pixel-wise loss. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 7832â€“ 7841. [9] Kun Cheng, Xiaodong Cun, Yong Zhang, Menghan Xia, Fei Yin, Mingrui Zhu, Xuan Wang, Jue Wang, and Nannan Wang. 2022. VideoReTalking: Audio-based Lip Synchronization for Talking Head Video Editing In the Wild. In SIGGRAPH Asia 2022 Conference Papers. 1â€“9. [10] Yu Deng, Jiaolong Yang, Sicheng Xu, Dong Chen, Yunde Jia, and Xin Tong. 2019. Accurate 3D Face Reconstruction with Weakly-Supervised Learning: From Single Image to Image Set. In IEEE Computer Vision and Pattern Recognition Workshops. [11] Emily Drago. 2015. The effect of technology on face-to-face communication. Elon Journal of Undergraduate Research in Communications 6, 1 (2015). [12] Ohad Fried, Ayush Tewari, Michael ZollhÃ¶fer, Adam Finkelstein, Eli Shechtman, Dan B Goldman, Kyle Genova, Zeyu Jin, Christian Theobalt, and Maneesh Agrawala. 2019. Text-based editing of talking-head video. ACM Transactions on Graphics (TOG) 38, 4 (2019), 1â€“14. [13] Yudong Guo, Keyu Chen, Sen Liang, Yong-Jin Liu, Hujun Bao, and Juyong Zhang. 2021. Ad-nerf: Audio driven neural radiance fields for talking head synthesis. In Proceedings of the IEEE/CVF International Conference on Computer Vision. 5784â€“ 5794. [14] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. 2017. Gans trained by a two time-scale update rule converge to a local nash equilibrium. Advances in neural information processing systems 30 (2017). [15] Jonathan Ho, Ajay Jain, and Pieter Abbeel. 2020. Denoising diffusion probabilistic models. Advances in Neural Information Processing Systems 33 (2020), 6840â€“6851. [16] Ailin Huang, Zhewei Huang, and Shuchang Zhou. 2022. Perceptual Conversational Head Generation with Regularized Driver and Enhanced Renderer. In Proceedings of the 30th ACM International Conference on Multimedia. 7050â€“7054. [17] Xinya Ji, Hang Zhou, Kaisiyuan Wang, Qianyi Wu, Wayne Wu, Feng Xu, and Xun Cao. 2022. EAMM: One-Shot Emotional Talking Face via Audio-Based Emotion-Aware Motion Model. arXiv preprint arXiv:2205.15278 (2022). [18] Xinya Ji, Hang Zhou, Kaisiyuan Wang, Wayne Wu, Chen Change Loy, Xun Cao, and Feng Xu. 2021. Audio-driven emotional video portraits. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 14080â€“14089. [19] Avisek Lahiri, Vivek Kwatra, Christian Frueh, John Lewis, and Chris Bregler. 2021. Lipsync3d: Data-efficient learning of personalized 3d talking faces from video using pose and lighting normalization. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2755â€“2764. [20] Lingzhi Li, Jianmin Bao, Hao Yang, Dong Chen, and Fang Wen. 2019. Faceshifter: Towards high fidelity and occlusion aware face swapping. arXiv preprint arXiv:1912.13457 (2019). [21] Borong Liang, Yan Pan, Zhizhi Guo, Hang Zhou, Zhibin Hong, Xiaoguang Han, Junyu Han, Jingtuo Liu, Errui Ding, and Jingdong Wang. 2022. Expressive talking head generation with granular audio-visual control. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 3387â€“3396. [22] Jin Liu, Xi Wang, Xiaomeng Fu, Yesheng Chai, Cai Yu, Jiao Dai, and Jizhong Han. 2023. FONT: Flow-guided One-shot Talking Head Generation with Natural Head Motions. arXiv preprint arXiv:2303.17789 (2023). [23] Jin Liu, Xi Wang, Xiaomeng Fu, Yesheng Chai, Cai Yu, Jiao Dai, and Jizhong Han. 2023. OPT: One-shot Pose-Controllable Talking Head Generation. arXiv preprint arXiv:2302.08197 (2023). [24] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron, Ravi Ramamoorthi, and Ren Ng. 2021. Nerf: Representing scenes as neural radiance fields for view synthesis. Commun. ACM 65, 1 (2021), 99â€“106.

[25] Yisroel Mirsky and Wenke Lee. 2021. The creation and detection of deepfakes: A survey. ACM Computing Surveys (CSUR) 54, 1 (2021), 1â€“41.
[26] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. 2021. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. arXiv preprint arXiv:2112.10741 (2021).
[27] Alexander Quinn Nichol and Prafulla Dhariwal. 2021. Improved denoising diffusion probabilistic models. In International Conference on Machine Learning. PMLR, 8162â€“8171.
[28] Se Jin Park, Minsu Kim, Joanna Hong, Jeongsoo Choi, and Yong Man Ro. 2022. SyncTalkFace: Talking Face Generation with Precise Lip-syncing via Audio-Lip Memory. In 36th AAAI Conference on Artificial Intelligence (AAAI 22). Association for the Advancement of Artificial Intelligence.
[29] Taesung Park, Ming-Yu Liu, Ting-Chun Wang, and Jun-Yan Zhu. 2019. Semantic image synthesis with spatially-adaptive normalization. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2337â€“2346.
[30] Rachel Paul, Jesse Sharrard, and Song Xiong. 2016. The importance of face-to-face communication in the digital world. Journal of Nutrition Education and Behavior 48, 10 (2016), 681.
[31] KR Prajwal, Rudrabha Mukhopadhyay, Vinay P Namboodiri, and CV Jawahar. 2020. A lip sync expert is all you need for speech to lip generation in the wild. In Proceedings of the 28th ACM International Conference on Multimedia. 484â€“492.
[32] Xuebin Qin, Zichen Zhang, Chenyang Huang, Masood Dehghan, Osmar Zaiane, and Martin Jagersand. 2020. U2-Net: Going Deeper with Nested U-Structure for Salient Object Detection. Pattern Recognition 106, 107404.
[33] Xuebin Qin, Zichen Zhang, Chenyang Huang, Masood Dehghan, Osmar R Zaiane, and Martin Jagersand. 2020. U2-Net: Going deeper with nested U-structure for salient object detection. Pattern recognition 106 (2020), 107404.
[34] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. 2021. Learning transferable visual models from natural language supervision. In International conference on machine learning. PMLR, 8748â€“8763.
[35] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. 2022. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125 (2022).
[36] Yurui Ren, Ge Li, Yuanqi Chen, Thomas H Li, and Shan Liu. 2021. Pirenderer: Controllable portrait image generation via semantic neural rendering. In Proceedings of the IEEE/CVF International Conference on Computer Vision. 13759â€“13768.
[37] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and BjÃ¶rn Ommer. 2022. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 10684â€“10695.
[38] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. 2015. U-net: Convolutional networks for biomedical image segmentation. In Medical Image Computing and Computer-Assisted Interventionâ€“MICCAI 2015: 18th International Conference, Munich, Germany, October 5-9, 2015, Proceedings, Part III 18. Springer, 234â€“241.
[39] Nataniel Ruiz, Eunji Chong, and James M Rehg. 2018. Fine-grained head pose estimation without keypoints. In Proceedings of the IEEE conference on computer vision and pattern recognition workshops. 2074â€“2083.
[40] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. 2022. Photorealistic text-to-image diffusion models with deep language understanding. Advances in Neural Information Processing Systems 35 (2022), 36479â€“36494.
[41] Sanjana Sinha, Sandika Biswas, Ravindra Yadav, and Brojeshwar Bhowmick. 2022. Emotion-Controllable Generalized Talking Face Generation. arXiv preprint arXiv:2205.01155 (2022).
[42] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Åukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. Advances in neural information processing systems 30 (2017).
[43] Duomin Wang, Yu Deng, Zixin Yin, Heung-Yeung Shum, and Baoyuan Wang. 2022. Progressive Disentangled Representation Learning for Fine-Grained Controllable Talking Head Synthesis. arXiv preprint arXiv:2211.14506 (2022).
[44] Kaisiyuan Wang, Qianyi Wu, Linsen Song, Zhuoqian Yang, Wayne Wu, Chen Qian, Ran He, Yu Qiao, and Chen Change Loy. 2020. Mead: A large-scale audiovisual dataset for emotional talking-face generation. In European Conference on Computer Vision. Springer, 700â€“717.
[45] Suzhen Wang, Lincheng Li, Yu Ding, and Xin Yu. 2022. One-shot talking face generation from single-speaker audio-visual correlation learning. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 36. 2531â€“2539.
[46] Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P Simoncelli. 2004. Image quality assessment: from error visibility to structural similarity. IEEE transactions on image processing 13, 4 (2004), 600â€“612.
[47] Chenxu Zhang, Yifan Zhao, Yifei Huang, Ming Zeng, Saifeng Ni, Madhukar Budagavi, and Xiaohu Guo. 2021. Facial: Synthesizing dynamic talking face with implicit attribute learning. In Proceedings of the IEEE/CVF international conference on computer vision. 3867â€“3876.
[48] Wenxuan Zhang, Xiaodong Cun, Xuan Wang, Yong Zhang, Xi Shen, Yu Guo, Ying Shan, and Fei Wang. 2022. SadTalker: Learning Realistic 3D Motion Coefficients

MM â€™23, October 29-November 3, 2023, Ottawa, ON, Canada
for Stylized Audio-Driven Single Image Talking Face Animation. arXiv preprint arXiv:2211.12194 (2022). [49] Zhimeng Zhang, Lincheng Li, Yu Ding, and Changjie Fan. 2021. Flow-guided one-shot talking face generation with a high-resolution audio-visual dataset. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 3661â€“3670. [50] Hang Zhou, Yu Liu, Ziwei Liu, Ping Luo, and Xiaogang Wang. 2019. Talking face generation by adversarially disentangled audio-visual representation. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 33. 9299â€“9306.

Jin Liu et al.
[51] Hang Zhou, Yasheng Sun, Wayne Wu, Chen Change Loy, Xiaogang Wang, and Ziwei Liu. 2021. Pose-controllable talking face generation by implicitly modularized audio-visual representation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 4176â€“4186.
[52] Mohan Zhou, Yalong Bai, Wei Zhang, Ting Yao, Tiejun Zhao, and Tao Mei. 2022. Responsive listening head generation: a benchmark dataset and baseline. In Computer Visionâ€“ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23â€“27, 2022, Proceedings, Part XXXVIII. Springer, 124â€“142.

