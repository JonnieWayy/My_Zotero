
Skip to main content
Cornell University
We gratefully acknowledge support from the Simons Foundation, member institutions , and all contributors. Donate
arxiv logo > cs > arXiv:2306.06687

Help | Advanced Search
Search
Computer Science > Computer Vision and Pattern Recognition
(cs)
[Submitted on 11 Jun 2023 ( v1 ), last revised 18 Jun 2023 (this version, v2)]
Title: LAMM: Language-Assisted Multi-Modal Instruction-Tuning Dataset, Framework, and Benchmark
Authors: Zhenfei Yin , Jiong Wang , Jianjian Cao , Zhelun Shi , Dingning Liu , Mukai Li , Lu Sheng , Lei Bai , Xiaoshui Huang , Zhiyong Wang , Jing Shao , Wanli Ouyang
Download a PDF of the paper titled LAMM: Language-Assisted Multi-Modal Instruction-Tuning Dataset, Framework, and Benchmark, by Zhenfei Yin and 11 other authors
Download PDF

    Abstract: Large language models have become a potential pathway toward achieving artificial general intelligence. Recent works on multi-modal large language models have demonstrated their effectiveness in handling visual modalities. In this work, we extend the research of MLLMs to point clouds and present the LAMM-Dataset and LAMM-Benchmark for 2D image and 3D point cloud understanding. We also establish an extensible framework to facilitate the extension of MLLMs to additional modalities. Our main contribution is three-fold: 1) We present the LAMM-Dataset and LAMM-Benchmark, which cover almost all high-level vision tasks for 2D and 3D vision. Extensive experiments validate the effectiveness of our dataset and benchmark. 2) We demonstrate the detailed methods of constructing instruction-tuning datasets and benchmarks for MLLMs, which will enable future research on MLLMs to scale up and extend to other domains, tasks, and modalities faster. 3) We provide a primary but potential MLLM training framework optimized for modalities' extension. We also provide baseline models, comprehensive experimental observations, and analysis to accelerate future research. Codes and datasets are now available at this https URL . 

Comments: 	37 pages, 33 figures. Code available at this https URL ; Project page: this https URL
Subjects: 	Computer Vision and Pattern Recognition (cs.CV)
Cite as: 	arXiv:2306.06687 [cs.CV]
  	(or arXiv:2306.06687v2 [cs.CV] for this version)
  	https://doi.org/10.48550/arXiv.2306.06687
Focus to learn more
arXiv-issued DOI via DataCite
Submission history
From: Zhenfei Yin [ view email ]
[v1] Sun, 11 Jun 2023 14:01:17 UTC (2,189 KB)
[v2] Sun, 18 Jun 2023 13:15:47 UTC (3,699 KB)
Full-text links:
Download:

    Download a PDF of the paper titled LAMM: Language-Assisted Multi-Modal Instruction-Tuning Dataset, Framework, and Benchmark, by Zhenfei Yin and 11 other authors
    PDF
    Other formats 

( license )
Current browse context:
cs.CV
< prev   |   next >
new | recent | 2306
Change to browse by:
cs
References & Citations

    NASA ADS
    Google Scholar
    Semantic Scholar

a export BibTeX citation Loading...
Bookmark
BibSonomy logo Reddit logo
Bibliographic Tools
Bibliographic and Citation Tools
Bibliographic Explorer Toggle
Bibliographic Explorer ( What is the Explorer? )
Litmaps Toggle
Litmaps ( What is Litmaps? )
scite.ai Toggle
scite Smart Citations ( What are Smart Citations? )
Code, Data, Media
Demos
Related Papers
About arXivLabs
Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? )

    About
    Help

    contact arXiv Click here to contact arXiv Contact
    subscribe to arXiv mailings Click here to subscribe Subscribe

    Copyright
    Privacy Policy

    Web Accessibility Assistance

    arXiv Operational Status
    Get status notifications via email or slack

