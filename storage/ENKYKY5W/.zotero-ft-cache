
Skip to main content
Cornell University
We gratefully acknowledge support from the Simons Foundation, member institutions , and all contributors. Donate
arxiv logo > cs > arXiv:2208.03550

Help | Advanced Search
Search
Computer Science > Computer Vision and Pattern Recognition
(cs)
[Submitted on 6 Aug 2022]
Title: Frozen CLIP Models are Efficient Video Learners
Authors: Ziyi Lin , Shijie Geng , Renrui Zhang , Peng Gao , Gerard de Melo , Xiaogang Wang , Jifeng Dai , Yu Qiao , Hongsheng Li
Download a PDF of the paper titled Frozen CLIP Models are Efficient Video Learners, by Ziyi Lin and 8 other authors
Download PDF

    Abstract: Video recognition has been dominated by the end-to-end learning paradigm -- first initializing a video recognition model with weights of a pretrained image model and then conducting end-to-end training on videos. This enables the video network to benefit from the pretrained image model. However, this requires substantial computation and memory resources for finetuning on videos and the alternative of directly using pretrained image features without finetuning the image backbone leads to subpar results. Fortunately, recent advances in Contrastive Vision-Language Pre-training (CLIP) pave the way for a new route for visual recognition tasks. Pretrained on large open-vocabulary image-text pair data, these models learn powerful visual representations with rich semantics. In this paper, we present Efficient Video Learning (EVL) -- an efficient framework for directly training high-quality video recognition models with frozen CLIP features. Specifically, we employ a lightweight Transformer decoder and learn a query token to dynamically collect frame-level spatial features from the CLIP image encoder. Furthermore, we adopt a local temporal module in each decoder layer to discover temporal clues from adjacent frames and their attention maps. We show that despite being efficient to train with a frozen backbone, our models learn high quality video representations on a variety of video recognition datasets. Code is available at this https URL . 

Comments: 	ECCV 2022
Subjects: 	Computer Vision and Pattern Recognition (cs.CV)
Cite as: 	arXiv:2208.03550 [cs.CV]
  	(or arXiv:2208.03550v1 [cs.CV] for this version)
  	https://doi.org/10.48550/arXiv.2208.03550
Focus to learn more
arXiv-issued DOI via DataCite
Submission history
From: Ziyi Lin [ view email ]
[v1] Sat, 6 Aug 2022 17:38:25 UTC (3,936 KB)
Full-text links:
Download:

    Download a PDF of the paper titled Frozen CLIP Models are Efficient Video Learners, by Ziyi Lin and 8 other authors
    PDF
    Other formats 

Current browse context:
cs.CV
< prev   |   next >
new | recent | 2208
Change to browse by:
cs
References & Citations

    NASA ADS
    Google Scholar
    Semantic Scholar

a export BibTeX citation Loading...
Bookmark
BibSonomy logo Reddit logo
Bibliographic Tools
Bibliographic and Citation Tools
Bibliographic Explorer Toggle
Bibliographic Explorer ( What is the Explorer? )
Litmaps Toggle
Litmaps ( What is Litmaps? )
scite.ai Toggle
scite Smart Citations ( What are Smart Citations? )
Code, Data, Media
Demos
Related Papers
About arXivLabs
Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? )

    About
    Help

    contact arXiv Click here to contact arXiv Contact
    subscribe to arXiv mailings Click here to subscribe Subscribe

    Copyright
    Privacy Policy

    Web Accessibility Assistance

    arXiv Operational Status
    Get status notifications via email or slack

